{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture : Graph Convolutional Networks\n",
    "\n",
    "## Lab 01 : ChebNets -- Solution\n",
    "\n",
    "### Xavier Bresson, Nian Liu\n",
    "\n",
    "Defferrard, Bresson, Vandergheynst, Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering, 2016  \n",
    "https://arxiv.org/pdf/1606.09375\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/08_Graph_Convnets'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import time\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, 'lib/')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data torch.Size([1000, 784])\n",
      "train_label torch.Size([1000])\n",
      "test_data torch.Size([100, 784])\n",
      "test_label torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Load small MNIST\n",
    "[train_data, train_label, test_data, test_label] = torch.load('datasets/MNIST_1k.pt')\n",
    "print('train_data',train_data.size())\n",
    "print('train_label',train_label.size())\n",
    "print('test_data',test_data.size())\n",
    "print('test_label',test_label.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute coarsened graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb edges:  6396\n",
      "Heavy Edge Matching coarsening with Xavier version\n",
      "Layer 0: M_0 = |V| = 944 nodes (160 added), |E| = 3198 edges\n",
      "Layer 1: M_1 = |V| = 472 nodes (67 added), |E| = 1619 edges\n",
      "Layer 2: M_2 = |V| = 236 nodes (23 added), |E| = 784 edges\n",
      "Layer 3: M_3 = |V| = 118 nodes (5 added), |E| = 387 edges\n",
      "Layer 4: M_4 = |V| = 59 nodes (0 added), |E| = 190 edges\n",
      "lmax: [1.3857534, 1.3440951, 1.2102374, 1.0000005]\n",
      "torch.Size([1000, 944])\n",
      "torch.Size([100, 944])\n",
      "Execution time: 0.34s\n"
     ]
    }
   ],
   "source": [
    "from lib.grid_graph import grid_graph\n",
    "from lib.coarsening import coarsen\n",
    "from lib.coarsening import lmax_L\n",
    "from lib.coarsening import perm_data\n",
    "from lib.coarsening import rescale_L\n",
    "\n",
    "# Construct grid graph\n",
    "t_start = time.time()\n",
    "grid_side = 28  # Each image is 28 * 28\n",
    "number_edges = 8  # Each pixel has eight neighbors\n",
    "A = grid_graph(grid_side, number_edges, 'euclidean') # create graph of Euclidean grid\n",
    "\n",
    "# Compute coarsened graphs\n",
    "num_coarsening_levels = 4\n",
    "L, perm = coarsen(A, num_coarsening_levels)\n",
    "\n",
    "# Compute largest eigenvalue of graph Laplacians\n",
    "lmax = []\n",
    "for i in range(num_coarsening_levels):\n",
    "    lmax.append(lmax_L(L[i]))\n",
    "print('lmax: ' + str([lmax[i] for i in range(num_coarsening_levels)]))\n",
    "\n",
    "# Reindex nodes to satisfy a binary tree structure\n",
    "train_data = perm_data(train_data, perm)\n",
    "test_data = perm_data(test_data, perm)\n",
    "train_data = torch.tensor(train_data).float()\n",
    "test_data = torch.tensor(test_data).float()\n",
    "print(train_data.size())\n",
    "print(test_data.size())\n",
    "\n",
    "print('Execution time: {:.2f}s'.format(time.time() - t_start))\n",
    "del perm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Implement ChebNet based on the CNN LeNet-5 architecture\n",
    "\n",
    "- First layer : CL with 32 features\n",
    "- Second layer : MaxPooling to reduce graph size by a factor 4\n",
    "- Third layer : CL with 64 features\n",
    "- Fourth layer : MaxPooling to reduce graph size by a factor 4\n",
    "- Fifth layer : Fully connected (or linear) layer with 512 features \n",
    "- Last layer : Fully connected (or linear) layer with 10 output values for 10 classes \n",
    "\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Step 1: Define the model architecture with the constructor `def __init__()`.\n",
    "  - Use [torch.nn.Linear()](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) to linearly transform the feature dimensions. \n",
    "  - Convert the SciPy sparse graph Laplacian and its coarsened versions to PyTorch sparse matrices with [torch.sparse.FloatTensor(indices, data, shape)](https://pytorch.org/docs/stable/sparse.html).\n",
    "  - For a [scipy.sparse.coo_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html) L, `L.row`, `L.col` and `L.data` are the row indices, column indices, and weights for each edge, respectively.\n",
    "\n",
    "Step 2: Define MaxPooling layers in `def graph_max_pool()` using [torch.nn.MaxPool1d()](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html#torch.nn.MaxPool1d).\n",
    "  - Note that the pooling is done along the node dimension (analogous to a 1D sequence) and not the feature dimension.\n",
    "  - Use [torch.permute()](https://pytorch.org/docs/stable/generated/torch.permute.html#torch.permute) to reorder the tensor dimensions to apply `torch.nn.MaxPool1d()`, and [.contiguous()](https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous) to ensure a contiguous memory layout for the target tensor.\n",
    "\n",
    "Step 3: Implement ChebNet convolution in `def graph_conv_cheby()`.\n",
    "  - Compute explicitly the first two Chebyshev terms, then code the recursive formula.\n",
    "  - Use [torch.sparse.mm()](https://pytorch.org/docs/stable/generated/torch.sparse.mm.html#torch.sparse.mm) to perform matrix multiplication between two sparse matrices, which is less memory consuming than [torch.mm()](https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm).\n",
    "  - Node features are integrated into the first two terms, so the following terms automatically contain node features via recursive iterations, see Slides 34 and 35 from the lecture.\n",
    "  - Combine the results from all intermediate Chebyshev orders to get the output, by linearly transforming them to match the input dimension of the next layer.\n",
    "\n",
    "Step 4: Implement the forward pass in `def forward()`.\n",
    "  - For CL1 and CL2, `graph_conv_cheby -> torch.relu (non-linear activation) -> graph_max_pool`\n",
    "  - For FC1 and FC2, `fc1 -> torch.relu (non-linear activation) -> dropout -> fc2`  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class definition\n",
    "class ChebNet_LeNet5(nn.Module):\n",
    "    def __init__(self, net_parameters, Ls, lmax):\n",
    "        super().__init__()\n",
    "        # parameters\n",
    "        # D: dimension of input features\n",
    "        # CL1_F, CL2_F: dimensions of output from the First and Third layers\n",
    "        # CL1_K, CL2_K: orders of Chebyshev terms used in CL1 and CL2\n",
    "        # FC1_F, FC2_F: dimensions of output from the Fifth and Last layers\n",
    "        # FC1Fin: dimensions of input for the Fifth layer\n",
    "        D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F = net_parameters\n",
    "        FC1Fin = CL2_F*(D//16)\n",
    "        \n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        # Step 1.1: Using the input and output dims at CL1, CL2, FC1, FC2 layers, define the corresponding feature transformation functions\n",
    "        ########################################\n",
    "        # graph CL1\n",
    "        self.cl1 = nn.Linear(CL1_K*1, CL1_F) # the dim of input feature is 1\n",
    "        self.CL1_K = CL1_K; self.CL1_F = CL1_F\n",
    "        \n",
    "        # graph CL2\n",
    "        self.cl2 = nn.Linear(CL2_K*CL1_F, CL2_F) \n",
    "        self.CL2_K = CL2_K; self.CL2_F = CL2_F\n",
    "        \n",
    "        # FC1\n",
    "        self.fc1 = nn.Linear(FC1Fin, FC1_F) \n",
    "        self.FC1Fin = FC1Fin\n",
    "        \n",
    "        # FC2\n",
    "        self.fc2 = nn.Linear(FC1_F, FC2_F)\n",
    "        ########################################\n",
    "        # YOUR CODE ENDS\n",
    "        ########################################\n",
    "\n",
    "        # Compute the pytorch Laplacian and its coarsened versions\n",
    "        self.L = []\n",
    "        for i in range(num_coarsening_levels+1):\n",
    "            L = Ls[i] \n",
    "            # rescale Laplacian: shift L into the definition domain of Chebyshev expansion λ ∈ [-1, 1]\n",
    "            lmax = lmax_L(L)\n",
    "            L = rescale_L(L, lmax) \n",
    "            # convert scipy sparse matric L to pytorch\n",
    "            L = L.tocoo()\n",
    "            ########################################\n",
    "            # YOUR CODE STARTS\n",
    "            # Step 1.2: Convert each L into a pytorch sparse tensor\n",
    "            ########################################\n",
    "            indices = np.column_stack((L.row, L.col)).T \n",
    "            indices = indices.astype(np.int64)\n",
    "            indices = torch.from_numpy(indices)\n",
    "            indices = indices.type(torch.LongTensor)\n",
    "            L_data = L.data.astype(np.float32)\n",
    "            L_data = torch.from_numpy(L_data) \n",
    "            L_data = L_data.type(torch.FloatTensor)\n",
    "            L = torch.sparse.FloatTensor(indices, L_data, torch.Size(L.shape))\n",
    "            ########################################\n",
    "            # YOUR CODE ENDS\n",
    "            ########################################\n",
    "            L = Variable( L , requires_grad=False)\n",
    "            self.L.append(L)\n",
    "        \n",
    "    # Max pooling of size p (p must be a power of 2)\n",
    "    def graph_max_pool(self, x, p): \n",
    "        # B, V, F = x.shape\n",
    "        # B = batch size\n",
    "        # V = num vertices\n",
    "        # F = num features\n",
    "        if p > 1: \n",
    "            ########################################\n",
    "            # YOUR CODE STARTS\n",
    "            # Step 2: Exchange the vertex dim and feature dim, do pooling along the last dim.\n",
    "            ########################################\n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x F x V\n",
    "            x = nn.MaxPool1d(p)(x)             # B x F x V/p          \n",
    "            x = x.permute(0,2,1).contiguous()  # x = B x V/p x F\n",
    "            ########################################\n",
    "            # YOUR CODE ENDS\n",
    "            ########################################\n",
    "            return x  \n",
    "        else:\n",
    "            return x   \n",
    "            \n",
    "    # Graph convolution layer\n",
    "    def graph_conv_cheby(self, x, cl, L, Fout, K):\n",
    "        # parameters\n",
    "        # B = batch size\n",
    "        # V = num vertices\n",
    "        # Fin = num input features\n",
    "        # Fout = num output features\n",
    "        # K = Chebyshev order and support size\n",
    "        B, V, Fin = x.size(); B, V, Fin = int(B), int(V), int(Fin) \n",
    "        \n",
    "        # Transform to Chebyshev basis\n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        # Step 3.1: Compute the first two Chebyshev terms, integrate 'x', concatenate them\n",
    "        # The first Chebyshev term: T_0(L)=I\n",
    "        # The second Chebyshev term: T_1(L)=L\n",
    "        ########################################\n",
    "        x0 = x.permute(1,2,0).contiguous()  # V x Fin x B\n",
    "        x0 = x0.view([V, Fin*B])            # V x Fin*B\n",
    "\n",
    "        # x: Concatenate the outputs from each Chebyshev term\n",
    "        x = x0.unsqueeze(0)                 # 1 x V x Fin*B\n",
    "        if K > 1: \n",
    "            x1 = torch.sparse.mm(L,x0)             # V x Fin*B\n",
    "            x = torch.cat((x, x1.unsqueeze(0)),0)  # 2 x V x Fin*B\n",
    "        ########################################\n",
    "        # YOUR CODE ENDS\n",
    "        ########################################\n",
    "        \n",
    "        for k in range(2, K):\n",
    "            ########################################\n",
    "            # YOUR CODE STARTS\n",
    "            # Step 3.2: Apply recursive formula, concatenate results, update x0, x1 as x1, x2\n",
    "            # The following Chebyshev terms: T_2(L)=2*LT_1(L)-T_0(L)\n",
    "            ########################################\n",
    "            x2 = 2 * torch.sparse.mm(L,x1) - x0  \n",
    "            x = torch.cat((x, x2.unsqueeze(0)),0)  # (k+1) x V x Fin*B\n",
    "            x0, x1 = x1, x2  \n",
    "            ########################################\n",
    "            # YOUR CODE ENDS\n",
    "            ########################################\n",
    "        x = x.view([K, V, Fin, B])           # K x V x Fin x B     \n",
    "        x = x.permute(3,1,2,0).contiguous()  # B x V x Fin x K       \n",
    "        x = x.view([B*V, Fin*K])             # B*V x Fin*K\n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        # Step 3.3: Linear transform Fin features to obtain Fout features\n",
    "        ########################################\n",
    "        x = cl(x)                            # B*V x Fout  \n",
    "        ########################################\n",
    "        # YOUR CODE ENDS\n",
    "        ########################################\n",
    "        x = x.view([B, V, Fout])             # B x V x Fout\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # graph CL1\n",
    "        x = x.unsqueeze(2) # B x V x Fin=1, images in MNIST only have one channel/feature\n",
    "        x = self.graph_conv_cheby(x, self.cl1, self.L[0], self.CL1_F, self.CL1_K)\n",
    "        x = torch.relu(x)\n",
    "        x = self.graph_max_pool(x, 4)\n",
    "        # graph CL2\n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        # Step 4.1: Implement CL2\n",
    "        ########################################\n",
    "        x = self.graph_conv_cheby(x, self.cl2, self.L[2], self.CL2_F, self.CL2_K)\n",
    "        x = torch.relu(x)\n",
    "        x = self.graph_max_pool(x, 4)\n",
    "        ########################################\n",
    "        # YOUR CODE ENDS\n",
    "        ########################################\n",
    "        # FC1\n",
    "        x = x.view(-1, self.FC1Fin) # resize the tensor\n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        # Step 4.2: Implement FC1\n",
    "        ########################################\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        ########################################\n",
    "        # YOUR CODE ENDS\n",
    "        ########################################\n",
    "        # FC2\n",
    "        x = self.fc2(x) \n",
    "        return x\n",
    "        \n",
    "    def update_learning_rate(self, optimizer, lr): # Adjust the learning rate based on the number of epochs\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return optimizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Print basic information of ChebNet, and test the forward and backward passes with one batch\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Step 1: Count the number of model parameters.\n",
    "  - For a pytorch model `net`, [net.parameters()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) returns an iterator over module parameters. [torch.numel()](https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel) returns the total number of elements in the input tensor.\n",
    "\n",
    "Step 2: Implement the loss function that includes a standard cross-entropy classfication loss and a L2 regularization loss for the learnable parameters of the network.\n",
    "  - Use [torch.nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) to compute the cross-entropy loss between input logits and target labels.\n",
    "  - Use `for param in net.parameters():` to access to the learnable parameters for the L2 regularization loss defined as L$(w_1,w_2)=\\sum_k\\|w_k\\|^2$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChebNet_LeNet5(\n",
      "  (cl1): Linear(in_features=25, out_features=32, bias=True)\n",
      "  (cl2): Linear(in_features=800, out_features=64, bias=True)\n",
      "  (fc1): Linear(in_features=3776, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Number of parameters: 1991050 (1.99 million)\n",
      "batch_idx:  tensor([946, 145, 856, 427, 730, 484, 168, 637, 341, 521])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/44/qs4c3c1d6ms37t9j12bv14240000gn/T/ipykernel_26777/3455975606.py:56: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1715184444117/work/torch/csrc/utils/tensor_new.cpp:607.)\n",
      "  L = torch.sparse.FloatTensor(indices, L_data, torch.Size(L.shape))\n"
     ]
    }
   ],
   "source": [
    "# network parameters\n",
    "D = train_data.shape[1]\n",
    "CL1_F = 32\n",
    "CL1_K = 25\n",
    "CL2_F = 64\n",
    "CL2_K = 25\n",
    "FC1_F = 512\n",
    "FC2_F = 10\n",
    "net_parameters = [D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F]\n",
    "\n",
    "# instantiate ChebNet\n",
    "net = ChebNet_LeNet5(net_parameters, L, lmax)\n",
    "print(net)\n",
    "\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    ########################################\n",
    "    # YOUR CODE STARTS\n",
    "    # Step 1: Count the number of model parameters\n",
    "    ########################################\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    ########################################\n",
    "    # YOUR CODE ENDS\n",
    "    ########################################\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "display_num_param(net)\n",
    "    \n",
    "# extract one batch\n",
    "batch_size = 10\n",
    "indices = torch.randperm(train_data.shape[0])\n",
    "batch_idx = indices[:batch_size]\n",
    "print('batch_idx: ',batch_idx)\n",
    "train_x, train_y = train_data[batch_idx,:], train_label[batch_idx]\n",
    "\n",
    "# Forward \n",
    "y = net(train_x)\n",
    "\n",
    "# backward\n",
    "def loss_reg(lossCE, net, y, y_target, l2_regularization):\n",
    "    CE_loss = 0.0\n",
    "    l2_loss = 0.0\n",
    "    ########################################\n",
    "    # YOUR CODE STARTS\n",
    "    # Step 2: Compute CE_loss and l2_loss\n",
    "    ########################################\n",
    "    CE_loss = lossCE(y, y_target)\n",
    "    for param in net.parameters():\n",
    "        data = param* param\n",
    "        l2_loss += data.sum()   \n",
    "    ########################################\n",
    "    # YOUR CODE ENDS\n",
    "    ########################################\n",
    "    loss = 0.5* l2_regularization* l2_loss + CE_loss\n",
    "    return loss\n",
    "    \n",
    "lossCE = nn.CrossEntropyLoss()\n",
    "l2_regularization = 1e-3 \n",
    "loss = loss_reg(lossCE, net, y, train_y, l2_regularization)\n",
    "loss.backward()\n",
    "\n",
    "# Update \n",
    "learning_rate = 0.05\n",
    "optimizer = torch.optim.SGD( net.parameters(), lr=learning_rate, momentum=0.9 )\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Training ChebNet\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Step 1: Initialize the model, the optimizer, and the loss function.\n",
    "\n",
    "Step 2: Repeat the training loop `num_epochs` times.\n",
    "- At the beginnig of a new epoch, shuffle the samples, reset the loss value and accuracy to zero, and set the `net` in training mode: [net.train()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train).\n",
    "- During each epoch, the sequence of instructions `batch data -> model -> output logits -> loss -> backward propagation -> parameter update -> evaluation` is used.\n",
    "- Gradient of the loss w.r.t. the net paramaters is automatically computed with [loss.backward()](https://pytorch.org/docs/stable/generated/torch.autograd.backward.html#torch.autograd.backward).\n",
    "- One update step of the parameter values is performed with [optimizer.step()](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step). Do not forget to zero the gradient at each mini-batch with [optimizer.zero_grad()](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad).\n",
    "- After each epoch, update the learning rate, and evaluate the testset accuracy with [torch.no_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html#torch.no_grad) to disable gradient calculation, and [net.eval()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module) to set the `net` in evaluation mode.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChebNet_LeNet5(\n",
      "  (cl1): Linear(in_features=25, out_features=32, bias=True)\n",
      "  (cl2): Linear(in_features=800, out_features=64, bias=True)\n",
      "  (fc1): Linear(in_features=3776, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "Number of parameters: 1991050 (1.99 million)\n",
      "num_epochs= 20 , num_train_data= 1000 , nb_iter= 200\n",
      "epoch= 1, loss(train)= 2.402, accuracy(train)= 10.500, time= 5.352, lr= 0.05000\n",
      "  accuracy(test) = 8.000 %, time= 0.214\n",
      "epoch= 2, loss(train)= 2.365, accuracy(train)= 18.200, time= 3.826, lr= 0.04750\n",
      "  accuracy(test) = 34.000 %, time= 0.187\n",
      "epoch= 3, loss(train)= 2.242, accuracy(train)= 34.900, time= 6.988, lr= 0.04512\n",
      "  accuracy(test) = 46.000 %, time= 0.270\n",
      "epoch= 4, loss(train)= 1.778, accuracy(train)= 49.300, time= 7.281, lr= 0.04287\n",
      "  accuracy(test) = 63.000 %, time= 0.191\n",
      "epoch= 5, loss(train)= 1.034, accuracy(train)= 71.000, time= 5.536, lr= 0.04073\n",
      "  accuracy(test) = 74.000 %, time= 0.206\n",
      "epoch= 6, loss(train)= 0.774, accuracy(train)= 78.900, time= 3.575, lr= 0.03869\n",
      "  accuracy(test) = 74.000 %, time= 0.171\n",
      "epoch= 7, loss(train)= 0.705, accuracy(train)= 81.300, time= 3.407, lr= 0.03675\n",
      "  accuracy(test) = 83.000 %, time= 0.167\n",
      "epoch= 8, loss(train)= 0.522, accuracy(train)= 86.300, time= 4.396, lr= 0.03492\n",
      "  accuracy(test) = 89.000 %, time= 0.188\n",
      "epoch= 9, loss(train)= 0.387, accuracy(train)= 91.500, time= 3.612, lr= 0.03317\n",
      "  accuracy(test) = 89.000 %, time= 0.201\n",
      "epoch= 10, loss(train)= 0.335, accuracy(train)= 93.200, time= 3.410, lr= 0.03151\n",
      "  accuracy(test) = 90.000 %, time= 0.178\n",
      "epoch= 11, loss(train)= 0.286, accuracy(train)= 94.700, time= 5.679, lr= 0.02994\n",
      "  accuracy(test) = 91.000 %, time= 0.209\n",
      "epoch= 12, loss(train)= 0.248, accuracy(train)= 96.600, time= 5.731, lr= 0.02844\n",
      "  accuracy(test) = 93.000 %, time= 0.212\n",
      "epoch= 13, loss(train)= 0.224, accuracy(train)= 97.400, time= 3.590, lr= 0.02702\n",
      "  accuracy(test) = 94.000 %, time= 0.184\n",
      "epoch= 14, loss(train)= 0.208, accuracy(train)= 97.900, time= 3.225, lr= 0.02567\n",
      "  accuracy(test) = 94.000 %, time= 0.196\n",
      "epoch= 15, loss(train)= 0.213, accuracy(train)= 98.100, time= 3.462, lr= 0.02438\n",
      "  accuracy(test) = 94.000 %, time= 0.206\n",
      "epoch= 16, loss(train)= 0.222, accuracy(train)= 97.500, time= 3.562, lr= 0.02316\n",
      "  accuracy(test) = 91.000 %, time= 0.191\n",
      "epoch= 17, loss(train)= 0.214, accuracy(train)= 97.200, time= 3.663, lr= 0.02201\n",
      "  accuracy(test) = 89.000 %, time= 0.207\n",
      "epoch= 18, loss(train)= 0.206, accuracy(train)= 97.400, time= 6.069, lr= 0.02091\n",
      "  accuracy(test) = 95.000 %, time= 0.190\n",
      "epoch= 19, loss(train)= 0.174, accuracy(train)= 98.900, time= 4.264, lr= 0.01986\n",
      "  accuracy(test) = 94.000 %, time= 0.176\n",
      "epoch= 20, loss(train)= 0.163, accuracy(train)= 99.400, time= 3.323, lr= 0.01887\n",
      "  accuracy(test) = 94.000 %, time= 0.187\n"
     ]
    }
   ],
   "source": [
    "# network parameters\n",
    "D = train_data.shape[1]\n",
    "CL1_F = 32\n",
    "CL1_K = 25\n",
    "CL2_F = 64\n",
    "CL2_K = 25\n",
    "FC1_F = 512\n",
    "FC2_F = 10\n",
    "net_parameters = [D, CL1_F, CL1_K, CL2_F, CL2_K, FC1_F, FC2_F]\n",
    "\n",
    "# instantiate ChebNet\n",
    "net = ChebNet_LeNet5(net_parameters, L, lmax)\n",
    "print(net)\n",
    "display_num_param(net)\n",
    "\n",
    "# optimization parameters\n",
    "lr = 0.05 # learning_rate\n",
    "init_lr = lr\n",
    "l2_regularization = 1e-3 \n",
    "batch_size = 100\n",
    "num_epochs = 20\n",
    "num_train_data = train_data.shape[0]\n",
    "nb_iter = int(num_epochs * num_train_data) // batch_size\n",
    "print('num_epochs=',num_epochs,', num_train_data=',num_train_data,', nb_iter=',nb_iter)\n",
    "\n",
    "# Optimizer\n",
    "########################################\n",
    "# YOUR CODE STARTS\n",
    "# Step 1: Initialize loss, optimizer and evaluation\n",
    "########################################\n",
    "lossCE = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD( net.parameters(), lr=lr, momentum=0.9 )\n",
    "########################################\n",
    "# YOUR CODE ENDS\n",
    "########################################\n",
    "\n",
    "def evaluation(y_predicted, y_label):\n",
    "    _, class_predicted = torch.max(y_predicted, 1)\n",
    "    return 100.0* (class_predicted == y_label).sum()/ y_predicted.size(0)\n",
    "    \n",
    "# loop over epochs\n",
    "num_data = 0\n",
    "for epoch in range(num_epochs):  \n",
    "\n",
    "    # reshuffle \n",
    "    indices = torch.randperm(num_train_data)\n",
    "    \n",
    "    # reset time\n",
    "    t_start = time.time()\n",
    "    \n",
    "    # extract batches\n",
    "    running_loss = 0.0\n",
    "    running_accuray = 0\n",
    "    running_total = 0\n",
    "    net.train()\n",
    "    for idx in range(0,num_train_data,batch_size):\n",
    "\n",
    "        # extract batches\n",
    "        train_x, train_y = train_data[idx:idx+batch_size,:], train_label[idx:idx+batch_size]\n",
    "            \n",
    "        # Forward \n",
    "        y = net(train_x)\n",
    "        \n",
    "        # backward\n",
    "        loss = loss_reg(lossCE, net, y, train_y, l2_regularization)\n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        # Step 2: Apply backward propagation and update net parameters\n",
    "        ########################################\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        ########################################\n",
    "        # YOUR CODE ENDS\n",
    "        ########################################\n",
    "        \n",
    "        # Accuracy\n",
    "        acc_train = evaluation(y.detach(), train_y)\n",
    "        \n",
    "        # loss, accuracy\n",
    "        num_data += batch_size \n",
    "        running_loss += loss.detach()\n",
    "        running_accuray += acc_train\n",
    "        running_total += 1\n",
    "      \n",
    "    # print \n",
    "    print('epoch= %d, loss(train)= %.3f, accuracy(train)= %.3f, time= %.3f, lr= %.5f' % \n",
    "          (epoch+1, running_loss/running_total, running_accuray/running_total, time.time()-t_start, lr))\n",
    " \n",
    "    # update learning rate \n",
    "    lr = init_lr * pow( 0.95 , float(num_data// num_train_data) )\n",
    "    optimizer = net.update_learning_rate(optimizer, lr)\n",
    "    \n",
    "    # Test set\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        running_accuray_test = 0\n",
    "        running_total_test = 0\n",
    "        num_test_data = test_data.size(0)\n",
    "        indices_test = torch.arange(num_test_data)\n",
    "        t_start_test = time.time()\n",
    "        for idx in range(0,num_test_data,batch_size):\n",
    "            test_x, test_y = test_data[idx:idx+batch_size,:], test_label[idx:idx+batch_size]\n",
    "            y = net(test_x)\n",
    "            acc_test = evaluation(y.detach(), test_y)\n",
    "            running_accuray_test += acc_test\n",
    "            running_total_test += 1\n",
    "        t_stop_test = time.time() - t_start_test\n",
    "        print('  accuracy(test) = %.3f %%, time= %.3f' % (running_accuray_test / running_total_test, t_stop_test))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
