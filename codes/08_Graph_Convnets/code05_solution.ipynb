{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture : Graph Convolutional Networks\n",
    "\n",
    "## Lab 05 : GatedGCNs for chemical regression -- Solution\n",
    "\n",
    "### Xavier Bresson, Nian Liu\n",
    "\n",
    "Bresson, Laurent, Residual Gated Graph ConvNets, 2017  \n",
    "https://arxiv.org/pdf/1711.07553\n",
    "\n",
    "Bresson, Laurent, A two-step graph convolutional decoder for molecule generation, 2019  \n",
    "https://arxiv.org/pdf/1906.03412\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/08_Graph_Convnets'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl==1.0.0 # Install DGL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "from lib.utils import Dictionary, MoleculeDataset, MoleculeDGL, Molecule\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load molecular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "13\n",
      "4\n",
      "Loading datasets QM9_dgl...\n",
      "train, test, val sizes : 2000 200 200\n",
      "Time: 0.7321s\n",
      "2000\n",
      "200\n",
      "200\n",
      "([Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})], [tensor([-0.2623]), tensor([1.0908])])\n",
      "(Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([0.5063]))\n",
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-4.4348]))\n"
     ]
    }
   ],
   "source": [
    "# Select dataset\n",
    "\n",
    "print('Loading data')\n",
    "data_folder_pytorch = 'datasets/QM9_pytorch/'\n",
    "with open(data_folder_pytorch+\"train_pytorch.pkl\",\"rb\") as f:\n",
    "    dataset=pickle.load(f)\n",
    "\n",
    "# Load the number of atom and bond types \n",
    "with open(data_folder_pytorch + \"atom_dict.pkl\" ,\"rb\") as f: num_atom_type = len(pickle.load(f))\n",
    "with open(data_folder_pytorch + \"bond_dict.pkl\" ,\"rb\") as f: num_bond_type = len(pickle.load(f))\n",
    "print(num_atom_type)\n",
    "print(num_bond_type)\n",
    "\n",
    "# Load the DGL datasets\n",
    "data_folder_dgl = 'datasets/QM9_dgl/'\n",
    "dataset_name = 'QM9'\n",
    "datasets_dgl = MoleculeDataset(dataset_name, data_folder_dgl)\n",
    "trainset, valset, testset = datasets_dgl.train, datasets_dgl.val, datasets_dgl.test\n",
    "print(len(trainset))\n",
    "print(len(valset))\n",
    "print(len(testset))\n",
    "idx = 0\n",
    "print(trainset[:2])\n",
    "print(valset[idx])\n",
    "print(testset[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a batch of graphs and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=88, num_edges=186,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})\n",
      "tensor([[ 0.8187],\n",
      "        [ 0.8730],\n",
      "        [ 4.0595],\n",
      "        [-0.8083],\n",
      "        [-0.5649],\n",
      "        [ 0.1069],\n",
      "        [ 3.0962],\n",
      "        [-0.5172],\n",
      "        [ 1.4457],\n",
      "        [-3.9308]])\n",
      "batch_norm_n: torch.Size([88, 1])\n",
      "batch_norm_e: torch.Size([186, 1])\n",
      "batch_x: torch.Size([88])\n",
      "batch_e: torch.Size([186])\n"
     ]
    }
   ],
   "source": [
    "# collate function prepares a batch of graphs, labels and other graph features\n",
    "def collate(samples):\n",
    "    \n",
    "    # Input sample is a list of pairs (graph, label)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batch_graphs = dgl.batch(graphs)    # batch of graphs\n",
    "    batch_labels = torch.stack(labels)  # batch of labels (here chemical target)\n",
    "    \n",
    "    # Normalization w.r.t. graph sizes\n",
    "    tab_sizes_n = [ graphs[i].number_of_nodes() for i in range(len(graphs))]\n",
    "    tab_norm_n = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_n ]\n",
    "    batch_norm_n = torch.cat(tab_norm_n).sqrt()  \n",
    "    tab_sizes_e = [ graphs[i].number_of_edges() for i in range(len(graphs))]\n",
    "    tab_norm_e = [ torch.FloatTensor(size,1).fill_(1./float(size)) for size in tab_sizes_e ]\n",
    "    batch_norm_e = torch.cat(tab_norm_e).sqrt()\n",
    "    \n",
    "    return batch_graphs, batch_labels, batch_norm_n, batch_norm_e\n",
    "\n",
    "\n",
    "# Generate a batch of graphs\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels, batch_norm_n, batch_norm_e = list(train_loader)[0]\n",
    "print(batch_graphs)\n",
    "print(batch_labels)\n",
    "print('batch_norm_n:',batch_norm_n.size())\n",
    "print('batch_norm_e:',batch_norm_e.size())\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "print('batch_x:',batch_x.size())\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "print('batch_e:',batch_e.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Implement a two-layer MLP for regression on a top of a GatedGCN network\n",
    "\n",
    "Node and edge update equations for GatedGCN layers:  \n",
    "\\begin{eqnarray}\n",
    "h_i^{\\ell+1} &=& h_i^{\\ell} + \\text{ReLU} \\Big( \\text{BN} \\Big( A^\\ell h_i^{\\ell} +  \\sum_{j\\sim i} \\eta(e_{ij}^{\\ell}) \\odot B^\\ell h_j^{\\ell} \\Big) \\Big), \\quad \\eta(e_{ij}^{\\ell}) = \\frac{\\sigma(e_{ij}^{\\ell})}{\\sum_{j'\\sim i} \\sigma(e_{ij'}^{\\ell}) + \\varepsilon} \\\\\n",
    "e_{ij}^{\\ell+1} &=& e^\\ell_{ij} + \\text{ReLU} \\Big( \\text{BN}  \\Big( C^\\ell e_{ij}^{\\ell} + D^\\ell h^{\\ell}_i + E^\\ell h^{\\ell}_j  \\Big) \\Big)\n",
    "\\end{eqnarray}\n",
    "\n",
    "MLP block for scalar regression of a molecular property:\n",
    "\\begin{eqnarray}\n",
    "y &=& \\textrm{MLP}(\\hat{h})\\in\\mathbb{R} \\\\\n",
    "\\hat{h} &=& \\frac{1}{n} \\sum_{i=1}^n h_i^{\\ell=L}\\in\\mathbb{R}^d\\\\\n",
    "\\textrm{MLP}(h) &=& \\textrm{LL}_2\\big( \\textrm{ReLU} \\big( \\textrm{LL}_1 \\big(h\\big) \\big) \\big), h\\in\\mathbb{R}^d\n",
    "\\end{eqnarray}\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a two-layer MLP for regression \n",
    "class MLP_layer(nn.Module): \n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim): \n",
    "        super(MLP_layer, self).__init__()\n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        # Implement a two-layer predictor\n",
    "        ########################################\n",
    "        self.linear1 = nn.Linear( input_dim, hidden_dim, bias=True )\n",
    "        self.linear2 = nn.Linear( hidden_dim, 1, bias=True )   \n",
    "        ########################################\n",
    "        # YOUR CODE ENDS\n",
    "        ########################################\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        # Pass `x` through the two-layer MLP and get prediction y\n",
    "        ########################################\n",
    "        y = self.linear2(torch.relu(self.linear1(x)))\n",
    "        ########################################\n",
    "        # YOUR CODE ENDS\n",
    "        ########################################\n",
    "        return y\n",
    "\n",
    "\n",
    "# class of GatedGCN layer  \n",
    "class GatedGCN_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GatedGCN_layer, self).__init__()\n",
    "        self.A = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.B = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.C = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.D = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.E = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.bn_node_h = nn.BatchNorm1d(output_dim)\n",
    "        self.bn_node_e = nn.BatchNorm1d(output_dim)\n",
    "\n",
    "    # Step 1 of message-passing with DGL: \n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i) \n",
    "    def message_func(self, edges):\n",
    "        Bhj = edges.src['Bh'] # Bhj with j/src\n",
    "        eij = edges.data['Ce'] +  edges.dst['Dh'] + edges.src['Eh'] # Ceij + Dhi + Ehj with dst/i, src/j\n",
    "        edges.data['e'] = eij # update edge feature value\n",
    "        return {'Bhj' : Bhj, 'eij' : eij} # send message={Bhj, eij} to node dst/i\n",
    "\n",
    "    # Step 2 of message-passing with DGL: \n",
    "    #   Reduce function collects all messages={Bhj, eij} sent to node dst/i with Step 1\n",
    "    def reduce_func(self, nodes):\n",
    "        Ahi = nodes.data['Ah']\n",
    "        Bhj = nodes.mailbox['Bhj']\n",
    "        e = nodes.mailbox['eij'] \n",
    "        sigmaij = torch.sigmoid(e) # sigma_ij = sigmoid(e_ij)\n",
    "        h = Ahi + torch.sum( sigmaij * Bhj, dim=1 ) / torch.sum( sigmaij, dim=1 ) # hi = Ahi + sum_j eta_ij * Bhj    \n",
    "        return {'h' : h} # return update node feature hi\n",
    "    \n",
    "    def forward(self, g, h, e, snorm_n, snorm_e):\n",
    "        \n",
    "        h_in = h # residual connection\n",
    "        e_in = e # residual connection\n",
    "        \n",
    "        g.ndata['h']  = h \n",
    "        g.ndata['Ah'] = self.A(h) # linear transformation \n",
    "        g.ndata['Bh'] = self.B(h) # linear transformation \n",
    "        g.ndata['Dh'] = self.D(h) # linear transformation \n",
    "        g.ndata['Eh'] = self.E(h) # linear transformation \n",
    "        g.edata['e']  = e \n",
    "        g.edata['Ce'] = self.C(e) # linear transformation \n",
    "        \n",
    "        g.update_all(self.message_func,self.reduce_func) # update the node and edge features with DGL\n",
    "        \n",
    "        h = g.ndata['h'] # collect the node output of graph convolution\n",
    "        e = g.edata['e'] # collect the edge output of graph convolution\n",
    "        \n",
    "        h = h* snorm_n # normalize activation w.r.t. graph node size\n",
    "        e = e* snorm_e # normalize activation w.r.t. graph edge size\n",
    "        \n",
    "        h = self.bn_node_h(h) # batch normalization  \n",
    "        e = self.bn_node_e(e) # batch normalization  \n",
    "        \n",
    "        h = torch.relu(h) # non-linear activation\n",
    "        e = torch.relu(e) # non-linear activation\n",
    "        \n",
    "        h = h_in + h # residual connection\n",
    "        e = e_in + e # residual connection\n",
    "        \n",
    "        return h, e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Implement the Mean Absolute Error (MAE) loss\n",
    "\n",
    "Hint: You may use [torch.nn.L1Loss()](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html#torch.nn.L1Loss).\n",
    "                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GatedGCN_net(\n",
      "  (embedding_h): Embedding(13, 128)\n",
      "  (embedding_e): Embedding(4, 128)\n",
      "  (GatedGCN_layers): ModuleList(\n",
      "    (0-3): 4 x GatedGCN_layer(\n",
      "      (A): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (B): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (C): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (D): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (E): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (bn_node_h): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn_node_e): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (MLP_layer): MLP_layer(\n",
      "    (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (linear2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class GatedGCN_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        super(GatedGCN_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "        self.embedding_e = nn.Embedding(num_bond_type, hidden_dim)\n",
    "        self.GatedGCN_layers = nn.ModuleList([ GatedGCN_layer(hidden_dim, hidden_dim) for _ in range(L) ]) \n",
    "        self.MLP_layer = MLP_layer(hidden_dim, hidden_dim)\n",
    "        \n",
    "    def forward(self, g, h, e, snorm_n, snorm_e):\n",
    "        \n",
    "        # input embedding\n",
    "        h = self.embedding_h(h)\n",
    "        e = self.embedding_e(e)\n",
    "        \n",
    "        # graph convnet layers\n",
    "        for GGCN_layer in self.GatedGCN_layers:\n",
    "            h,e = GGCN_layer(g,h,e,snorm_n,snorm_e)\n",
    "        \n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        y = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors\n",
    "        y = self.MLP_layer(y)\n",
    "        \n",
    "        return y    \n",
    "    \n",
    "    def loss(self, y_scores, y_labels):\n",
    "        ########################################\n",
    "        # YOUR CODE START\n",
    "        # Define the Mean Absolute Error (MAE) as regression loss \n",
    "        ########################################\n",
    "        loss = nn.L1Loss()(y_scores, y_labels)\n",
    "        ########################################\n",
    "        # YOUR CODE END\n",
    "        ########################################\n",
    "        return loss        \n",
    "    \n",
    "    def update(self, lr):       \n",
    "        update = torch.optim.Adam( self.parameters(), lr=lr )\n",
    "        return update\n",
    "\n",
    "\n",
    "# Instantiate one network (testing)\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['L'] = 4\n",
    "net = GatedGCN_net(net_parameters)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xbresson/miniconda3/envs/gnn_course/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, time 6.5603, train_loss: 1.2752, test_loss: 1.2946, val_loss: 1.1812\n",
      "Epoch 1, time 2.5837, train_loss: 1.1820, test_loss: 1.1751, val_loss: 1.0746\n",
      "Epoch 2, time 2.0501, train_loss: 1.1072, test_loss: 1.0627, val_loss: 0.9810\n",
      "Epoch 3, time 2.0528, train_loss: 1.0613, test_loss: 0.9989, val_loss: 0.9349\n",
      "Epoch 4, time 3.0131, train_loss: 1.0205, test_loss: 0.9602, val_loss: 0.8919\n",
      "Epoch 5, time 4.2675, train_loss: 0.9793, test_loss: 0.9157, val_loss: 0.8583\n",
      "Epoch 6, time 3.5839, train_loss: 0.9437, test_loss: 0.9194, val_loss: 0.8427\n",
      "Epoch 7, time 4.6368, train_loss: 0.9036, test_loss: 0.8688, val_loss: 0.7862\n",
      "Epoch 8, time 5.0549, train_loss: 0.8704, test_loss: 0.8137, val_loss: 0.7314\n",
      "Epoch 9, time 2.4849, train_loss: 0.8398, test_loss: 0.7933, val_loss: 0.7093\n",
      "Epoch 10, time 2.1983, train_loss: 0.8174, test_loss: 0.8576, val_loss: 0.8258\n",
      "Epoch 11, time 2.2391, train_loss: 0.8060, test_loss: 0.7310, val_loss: 0.6537\n",
      "Epoch 12, time 2.5428, train_loss: 0.7860, test_loss: 0.7464, val_loss: 0.6930\n",
      "Epoch 13, time 4.2555, train_loss: 0.7717, test_loss: 0.7110, val_loss: 0.6205\n",
      "Epoch 14, time 6.0920, train_loss: 0.7580, test_loss: 0.6959, val_loss: 0.6171\n",
      "Epoch 15, time 3.9015, train_loss: 0.7346, test_loss: 0.6882, val_loss: 0.6233\n",
      "Epoch 16, time 2.4172, train_loss: 0.7367, test_loss: 0.6688, val_loss: 0.5929\n",
      "Epoch 17, time 3.9918, train_loss: 0.7105, test_loss: 0.6775, val_loss: 0.6178\n",
      "Epoch 18, time 2.5604, train_loss: 0.7291, test_loss: 0.7429, val_loss: 0.6249\n",
      "Epoch 19, time 2.1982, train_loss: 0.7073, test_loss: 0.6472, val_loss: 0.5605\n",
      "Epoch 20, time 2.0421, train_loss: 0.6953, test_loss: 0.7173, val_loss: 0.5979\n",
      "Epoch 21, time 2.1924, train_loss: 0.7050, test_loss: 0.6540, val_loss: 0.5670\n",
      "Epoch 22, time 2.4814, train_loss: 0.6809, test_loss: 0.7195, val_loss: 0.6766\n",
      "Epoch 23, time 2.4237, train_loss: 0.6723, test_loss: 0.6277, val_loss: 0.5530\n",
      "Epoch 24, time 4.6603, train_loss: 0.6797, test_loss: 0.6695, val_loss: 0.6015\n",
      "Epoch 25, time 2.7397, train_loss: 0.6700, test_loss: 0.6828, val_loss: 0.5684\n",
      "Epoch 26, time 3.2319, train_loss: 0.6532, test_loss: 0.6169, val_loss: 0.5546\n",
      "Epoch 27, time 2.3509, train_loss: 0.6547, test_loss: 0.6115, val_loss: 0.5494\n",
      "Epoch 28, time 3.9196, train_loss: 0.6732, test_loss: 0.6321, val_loss: 0.5848\n",
      "Epoch 29, time 2.4924, train_loss: 0.6412, test_loss: 0.6249, val_loss: 0.5596\n",
      "Epoch 30, time 2.1846, train_loss: 0.6375, test_loss: 0.6102, val_loss: 0.5370\n",
      "Epoch 31, time 2.2444, train_loss: 0.6438, test_loss: 0.6153, val_loss: 0.5282\n",
      "Epoch 32, time 2.4044, train_loss: 0.6415, test_loss: 0.7553, val_loss: 0.7262\n",
      "Epoch 33, time 2.9340, train_loss: 0.6289, test_loss: 0.6281, val_loss: 0.5488\n",
      "Epoch 34, time 2.2466, train_loss: 0.6246, test_loss: 0.6152, val_loss: 0.5101\n",
      "Epoch 35, time 2.2463, train_loss: 0.6314, test_loss: 0.6687, val_loss: 0.5552\n",
      "Epoch 36, time 4.3043, train_loss: 0.6368, test_loss: 0.6117, val_loss: 0.5158\n",
      "Epoch 37, time 2.2373, train_loss: 0.6239, test_loss: 0.6417, val_loss: 0.5636\n",
      "Epoch 38, time 2.9718, train_loss: 0.6326, test_loss: 0.6316, val_loss: 0.5265\n",
      "Epoch 39, time 3.7274, train_loss: 0.6302, test_loss: 0.6274, val_loss: 0.5427\n",
      "Epoch 40, time 2.3393, train_loss: 0.6156, test_loss: 0.6106, val_loss: 0.5075\n",
      "Epoch 41, time 2.3114, train_loss: 0.6031, test_loss: 0.6065, val_loss: 0.5034\n",
      "Epoch 42, time 2.2562, train_loss: 0.6058, test_loss: 0.5810, val_loss: 0.5152\n",
      "Epoch 43, time 2.1062, train_loss: 0.6074, test_loss: 0.5974, val_loss: 0.5284\n",
      "Epoch 44, time 3.0436, train_loss: 0.5979, test_loss: 0.6077, val_loss: 0.5041\n",
      "Epoch 45, time 3.0827, train_loss: 0.6091, test_loss: 0.6508, val_loss: 0.5126\n",
      "Epoch 46, time 2.4213, train_loss: 0.6101, test_loss: 0.5947, val_loss: 0.5178\n",
      "Epoch 47, time 3.7588, train_loss: 0.6053, test_loss: 0.5714, val_loss: 0.4962\n",
      "Epoch 48, time 3.0122, train_loss: 0.6055, test_loss: 0.5948, val_loss: 0.4952\n",
      "Epoch 49, time 2.3321, train_loss: 0.5933, test_loss: 0.6023, val_loss: 0.4926\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    gpu_mem = 0\n",
    "    for iter, (batch_graphs, batch_labels, batch_snorm_n, batch_snorm_e) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        batch_e = batch_graphs.edata['feat']\n",
    "        batch_snorm_n = batch_snorm_n\n",
    "        batch_snorm_e = batch_snorm_e\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_e, batch_snorm_n, batch_snorm_e)\n",
    "        loss = net.loss(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        nb_data += batch_labels.size(0)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    return epoch_loss\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=datasets_dgl.collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True, collate_fn=datasets_dgl.collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=True, collate_fn=datasets_dgl.collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['hidden_dim'] = 100\n",
    "net_parameters['L'] = 4\n",
    "net = GatedGCN_net(net_parameters)\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(50):\n",
    "    start = time.time()\n",
    "    epoch_train_loss = run_one_epoch(net, train_loader, True)\n",
    "    with torch.no_grad(): \n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False)\n",
    "        epoch_val_loss = run_one_epoch(net, val_loader, False)  \n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}, val_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss, epoch_val_loss))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
