{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Nzk8feKvlGo2"
   },
   "source": [
    "# Lecture : Weisfeiler-Lehman GNNs\n",
    "\n",
    "## Lab 01 : GINs -- Exercise\n",
    "\n",
    "### Xavier Bresson, Jiaming Wang   \n",
    "\n",
    "Xu, Hu, Leskovec, Jegelka, How powerful are graph neural networks, 2018   \n",
    "https://arxiv.org/pdf/1810.00826.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ora3rIHvlGo4",
    "outputId": "e2005277-64d1-4ee9-e410-65275029c534"
   },
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/09_WL_GNNs'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl==1.0.0 # Install DGL\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MQL0HL1lGo6"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import dgl\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_ncut\n",
    "import time\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iDbelOhMlGo6"
   },
   "source": [
    "# Generate CSL dataset\n",
    "\n",
    "The CSL dataset, introduced in [1], is a dataset to evaluate the graph isomorphism power of GNNs.  \n",
    "This dataset requires a GNN to be at least as powerful as the 1-WL test to separate the classes.  \n",
    "\n",
    "[1] Murphy et-al, Relational pooling for graph representations, 2019, https://arxiv.org/pdf/1903.02541\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 482
    },
    "id": "aNfb_UeBlGo7",
    "outputId": "2fc25f74-d0dd-43b1-820c-da5329def9c9"
   },
   "outputs": [],
   "source": [
    "def generate_CSL_dataset(num_nodes, list_skip_value, num_graph_per_class):\n",
    "    label_class = 0\n",
    "    list_graph = []\n",
    "    # loop over skip values\n",
    "    for skip_value in list_skip_value:\n",
    "        # build CSL graph with node indexing in {0,1,...,num_nodes-1}\n",
    "        list_src = []\n",
    "        list_dst = []\n",
    "        for i in range(num_nodes):\n",
    "            # cycle\n",
    "            list_src.append(i)\n",
    "            list_dst.append((i+1)%num_nodes)\n",
    "            # skip connection\n",
    "            list_src.append(i)\n",
    "            list_dst.append((i+skip_value)%num_nodes)\n",
    "        graph = dgl.graph((list_src, list_dst)) # build DGL graph\n",
    "        graph = dgl.to_bidirected(graph) # symmetrize/undirected graph\n",
    "        list_graph.append([graph, torch.tensor(label_class).long()])\n",
    "        for _ in range(num_graph_per_class-1): # build CSL graph with random node indexing\n",
    "            idx_shuffle = torch.randperm(num_nodes).tolist() # random permutation of node indexing\n",
    "            list_src = [idx_shuffle[i] for i in list_src]\n",
    "            list_dst = [idx_shuffle[i] for i in list_dst]\n",
    "            graph = dgl.graph((list_src, list_dst)) # build DGL graph\n",
    "            graph = dgl.to_bidirected(graph) # symmetrize/undirected graph\n",
    "            list_graph.append([graph, torch.tensor(label_class).long()])\n",
    "        # increment class label\n",
    "        label_class += 1\n",
    "    return list_graph\n",
    "\n",
    "\n",
    "# Generate small CSL graph\n",
    "num_nodes = 7; list_skip_value = [2]; num_graph_per_class = 1\n",
    "small_csl_graph = generate_CSL_dataset(num_nodes, list_skip_value, num_graph_per_class)\n",
    "print(small_csl_graph[0])\n",
    "graph = small_csl_graph[0][0]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "nx.draw(graph.to_networkx(), ax=ax, with_labels=True, font_size=10) # visualise node indexes\n",
    "ax.title.set_text('Visualization with networkx')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPOISG9ZlGo8"
   },
   "source": [
    "# Generate train and test datasets \n",
    "\n",
    "## Add node feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUbuIB-dlGo9",
    "outputId": "964287ee-c118-4056-aca1-f2194b748ff9"
   },
   "outputs": [],
   "source": [
    "# Laplacian eigenvectors as positional encoding\n",
    "def LapEig_positional_encoding(g, pos_enc_dim):\n",
    "    Adj = g.adj().to_dense() # Adjacency matrix\n",
    "    Dn = ( g.in_degrees()** -0.5 ).diag() # Inverse and sqrt of degree matrix\n",
    "    Lap = torch.eye(g.number_of_nodes()) - Dn.matmul(Adj).matmul(Dn) # Laplacian operator\n",
    "    EigVal, EigVec = torch.linalg.eig(Lap) # Compute full EVD\n",
    "    EigVal, EigVec = EigVal.real, EigVec.real # make eig real\n",
    "    EigVec = EigVec[:, EigVal.argsort()] # sort in increasing order of eigenvalues\n",
    "    EigVec = EigVec[:,1:pos_enc_dim+1] # select the first non-trivial \"pos_enc_dim\" eigenvector\n",
    "    return EigVec\n",
    "\n",
    "# Add node and edge features to graphs\n",
    "pos_enc_dim = 6 # dimension of PE\n",
    "def add_node_edge_features(dataset):\n",
    "    for (graph,_) in dataset:\n",
    "        graph.ndata['degree'] = graph.in_degrees().view(-1, 1).float() # node degree as node feature\n",
    "        graph.ndata['random'] = torch.rand((graph.number_of_nodes(), pos_enc_dim)) # random features\n",
    "        graph.ndata['index'] = torch.arange(graph.number_of_nodes()) # node index as node feature\n",
    "        graph.ndata['pos_enc'] = LapEig_positional_encoding(graph, pos_enc_dim) # node positional encoding as node feature \n",
    "    return dataset\n",
    "\n",
    "# Generate CSL datasets\n",
    "num_nodes = 41\n",
    "list_skip_value = [2, 3, 4, 5, 6, 9, 11, 12, 13, 16]\n",
    "num_graph_per_class = 15\n",
    "trainset = generate_CSL_dataset(num_nodes, list_skip_value, num_graph_per_class)\n",
    "testset = generate_CSL_dataset(num_nodes, list_skip_value, num_graph_per_class)\n",
    "trainset = add_node_edge_features(trainset)\n",
    "testset = add_node_edge_features(testset)\n",
    "print(trainset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "360hL9OylGo9"
   },
   "source": [
    "# Visualize positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "id": "8aTtZw67lGo9",
    "outputId": "31b65ec2-82ce-41b9-9e46-4bf475fbfa8a"
   },
   "outputs": [],
   "source": [
    "graph = trainset[0][0]\n",
    "\n",
    "# Visualize graph\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "Adj = graph.adj().to_dense()\n",
    "A_nx = nx.from_numpy_array(Adj.numpy())\n",
    "C = compute_ncut(Adj.long(), 4)\n",
    "nx.draw(A_nx, ax=ax, node_color=C, cmap='jet', with_labels=True, font_size=10) # visualise node indexes\n",
    "ax.title.set_text('Visualization with networkx')\n",
    "plt.show()\n",
    "\n",
    "# Plot 2D coordinates\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = graph.ndata['pos_enc']\n",
    "ax.scatter(x[:,0], x[:,1])\n",
    "idx = list(range(graph.number_of_nodes()))\n",
    "ax.scatter(x[:,0], x[:,1], c=C, cmap='jet')\n",
    "for i, txt in enumerate(idx):\n",
    "    ax.annotate(txt, (x[:,0][i], x[:,1][i]), textcoords=\"offset points\", xytext=(1,5))\n",
    "ax.title.set_text('2D embedding of nodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Djf1jW5DlGo-"
   },
   "source": [
    "## Question 1: Define the collate function to prepare a batch of DGL graphs and test it\n",
    "\n",
    "To speed up training, we typically process training data with mini-batches. \n",
    "\n",
    "PyTorch designed the general class `DataLoader()` to prepare batch of data, including DGL graphs.\n",
    "\n",
    "It requires to re-define the `collate()` function that takes as input a list of training data in the form of tuples of <graph, label>, and outputs a batch of DGL graphs and labels from this list.\n",
    "\n",
    "Hint: you need to use the [dgl.batch()](https://docs.dgl.ai/en/1.1.x/generated/dgl.batch.html) function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3khUbvh6lGo-",
    "outputId": "8304d0be-26e0-4585-d76b-9f0c54f32136"
   },
   "outputs": [],
   "source": [
    "# collate function prepares a batch of graphs, labels and other graph features (if needed)\n",
    "def collate(samples):\n",
    "    # Input sample is a list of pairs (graph, label)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    ########################################\n",
    "    # YOUR CODE STARTS\n",
    "    batch_graphs =  # batch of graphs\n",
    "    batch_labels =  # batch of labels (here class label)\n",
    "    # YOUR CODE ENDS\n",
    "    ########################################\n",
    "    return batch_graphs, batch_labels\n",
    "\n",
    "\n",
    "# Generate a batch of graphs\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "print(batch_graphs)\n",
    "print(batch_labels)\n",
    "batch_x = batch_graphs.ndata['degree']\n",
    "print('batch_x:',batch_x.size())\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "print('batch_pe:',batch_pe.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uqptro8slGo-"
   },
   "source": [
    "## Question 2: Design the class of Graph Isomorphism Networks (GIN) with DGL\n",
    "\n",
    "### Implement the following node update equation with the node degree as input node feature:\n",
    "\\begin{eqnarray}\n",
    "h_i^{\\ell+1} &=& \\textrm{MLP} \\left( (1+\\varepsilon^{\\ell})\\ h_i^{\\ell} + \\sum_{j\\sim i} h_j^{\\ell} \\right) \\\\\n",
    "h_i^{\\ell=0} &=& d_i \\textrm{ (node degree)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Report the training loss and test loss.\n",
    "\n",
    "Hints: You may use \n",
    "- `batch_graphs.ndata['degree']` to access the node degree.\n",
    "- Function [`dgl.sum_nodes()`](https://docs.dgl.ai/en/0.2.x/generated/dgl.sum_nodes.html) to sum over all node features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YNGpXQIXlGo_",
    "outputId": "4fe70815-6b63-4045-e669-092dd9957e3c"
   },
   "outputs": [],
   "source": [
    "# MLP layer for classification\n",
    "class MLP_layer(nn.Module): \n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, L=2): # L = nb of hidden layers\n",
    "        super(MLP_layer, self).__init__()\n",
    "        list_FC_layers = [ nn.Linear( input_dim, input_dim, bias=True ) for l in range(L) ]\n",
    "        list_FC_layers.append(nn.Linear( input_dim, output_dim , bias=True ))\n",
    "        self.FC_layers = nn.ModuleList(list_FC_layers)\n",
    "        self.L = L\n",
    "        \n",
    "    def forward(self, x):   \n",
    "        ########################################\n",
    "        # Process the input using the previously defined layers \n",
    "        # YOUR CODE STARTS \n",
    "        y = \n",
    "        # YOUR CODE ENDS\n",
    "        ########################################\n",
    "        return y\n",
    "\n",
    "        \n",
    "# class of GatedGCN layer  \n",
    "class GIN_layer(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(GIN_layer, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        self.linear2 = nn.Linear(input_dim, output_dim, bias=True)\n",
    "        ########################################\n",
    "        # Define a learnable scalar parameter initialized to 0.001\n",
    "        # You may use \"nn.Parameter()\"\n",
    "        # YOUR CODE STARTS\n",
    "        self.eps =  # scalar, size=()\n",
    "        # YOUR CODE ENDS\n",
    "        ########################################\n",
    "          \n",
    "    # Step 1 of message-passing with DGL: \n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i) \n",
    "    def message_func(self, edges):\n",
    "        hj = edges.src['h'] \n",
    "        return {'hj' : hj}\n",
    "\n",
    "    # Step 2 of message-passing with DGL: \n",
    "    #   Reduce function collects all messages={hj} sent to node dst/i with Step 1\n",
    "    def reduce_func(self, nodes):\n",
    "        hj = nodes.mailbox['hj'] # size=(V,|Nj|,d), |Nj|=num_neighbors\n",
    "        ########################################\n",
    "        # Compute hi = sum_j hj\n",
    "        # YOUR CODE STARTS\n",
    "        sum_hj =   # hi = sum_j hj, size=(V,d)\n",
    "        # YOUR CODE ENDS\n",
    "        ########################################\n",
    "        return {'sum_hj' : sum_hj} \n",
    "    \n",
    "    def forward(self, g, h):\n",
    "        g.ndata['h'] = h \n",
    "        g.update_all(self.message_func,self.reduce_func) # update the node feature with DGL\n",
    "        ########################################\n",
    "        # Compute GIN node update \n",
    "        # You may use \"nn.Linear()\", \"torch.relu()\"\n",
    "        # YOUR CODE STARTS\n",
    "        h =  # size=(V,d)\n",
    "        # YOUR CODE ENDS\n",
    "        ########################################\n",
    "        return h\n",
    "    \n",
    "    \n",
    "class GIN_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        super(GIN_net, self).__init__()\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        output_dim = net_parameters['output_dim']\n",
    "        L = net_parameters['L']\n",
    "        degree_dim = 1\n",
    "        self.embedding_h = nn.Linear(degree_dim, hidden_dim) # node degree as node feature\n",
    "        self.GIN_layers = nn.ModuleList([ GIN_layer(hidden_dim, hidden_dim) for _ in range(L) ]) \n",
    "        self.MLP_layer = MLP_layer(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, g, h):\n",
    "        \n",
    "        # input embedding\n",
    "        h = self.embedding_h(h)\n",
    "        \n",
    "        # GIN layers\n",
    "        for GINlayer in self.GIN_layers:\n",
    "            h = GINlayer(g,h)\n",
    "        \n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        y = dgl.sum_nodes(g,'h')\n",
    "        y = self.MLP_layer(y)\n",
    "        \n",
    "        return y    \n",
    "\n",
    "\n",
    "def accuracy(scores, targets):\n",
    "    scores = scores.detach().argmax(dim=1)\n",
    "    acc = (scores==targets).float().sum().item()\n",
    "    return acc\n",
    "\n",
    "\n",
    "# Instantiate one network (testing)\n",
    "net_parameters = {}\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['output_dim'] = len(list_skip_value) # nb of classes\n",
    "net_parameters['L'] = 4\n",
    "net = GIN_net(net_parameters)\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Jj87XAESlGo_",
    "outputId": "4c5899ee-8540-4394-ffa0-05bafa9c4d2a"
   },
   "outputs": [],
   "source": [
    "def run_one_epoch(net, data_loader, lossCE, train=True, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        batch_x =  # node degree as node feature\n",
    "        # YOUR CODE ENDS\n",
    "        ######################################## \n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x)\n",
    "        loss = lossCE(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        epoch_acc += accuracy(batch_scores,batch_labels)\n",
    "        nb_data += batch_labels.size(0)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_acc /= nb_data\n",
    "    return epoch_loss, epoch_acc, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "train_loader = DataLoader(trainset, batch_size=10, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=10, shuffle=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['hidden_dim'] = 256\n",
    "net_parameters['output_dim'] = len(list_skip_value) # nb of classes\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GIN_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "num_epochs = 51\n",
    "for epoch in range(num_epochs): \n",
    "    epoch_train_loss, epoch_train_acc, optimizer = run_one_epoch(net, train_loader, loss, True, optimizer)\n",
    "    if not epoch%10:\n",
    "        with torch.no_grad(): \n",
    "            epoch_test_loss, epoch_test_acc, _ = run_one_epoch(net, test_loader, loss, False)\n",
    "        print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n",
    "        print('                       train_acc: {:.4f}, test_acc: {:.4f}'.format(epoch_train_acc, epoch_test_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Design GIN with random feature as input node feature\n",
    "\n",
    "\\begin{eqnarray}\n",
    "h_i^{\\ell+1} &=& \\textrm{MLP} \\left( (1+\\varepsilon^{\\ell})\\ h_i^{\\ell} + \\sum_{j\\sim i} h_j^{\\ell} \\right) \\\\\n",
    "h_i^{\\ell=0} &=& \\mathcal{N}_i \\textrm{ (random features)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Report the training loss and test loss.\n",
    "\n",
    "Hint: You may use `batch_graphs.ndata['random']` to access random node feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        super(GIN_net, self).__init__()\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        output_dim = net_parameters['output_dim']\n",
    "        L = net_parameters['L']\n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        self.embedding_h =   # random features\n",
    "        # YOUR CODE ENDS\n",
    "        ########################################   \n",
    "        self.GIN_layers = nn.ModuleList([ GIN_layer(hidden_dim, hidden_dim) for _ in range(L) ]) \n",
    "        self.MLP_layer = MLP_layer(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, g, h):\n",
    "        \n",
    "        # input embedding\n",
    "        h = self.embedding_h(h)\n",
    "        \n",
    "        # GIN layers\n",
    "        for GINlayer in self.GIN_layers:\n",
    "            h = GINlayer(g,h)\n",
    "        \n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        y = dgl.sum_nodes(g,'h')\n",
    "        y = self.MLP_layer(y)\n",
    "        \n",
    "        return y    \n",
    "\n",
    "def run_one_epoch(net, data_loader, lossCE, train=True, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        batch_x =  # random features\n",
    "        # YOUR CODE ENDS\n",
    "        ######################################## \n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x)\n",
    "        loss = lossCE(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        epoch_acc += accuracy(batch_scores,batch_labels)\n",
    "        nb_data += batch_labels.size(0)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_acc /= nb_data\n",
    "    return epoch_loss, epoch_acc, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "train_loader = DataLoader(trainset, batch_size=10, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=10, shuffle=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['hidden_dim'] = 256\n",
    "net_parameters['output_dim'] = len(list_skip_value) # nb of classes\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GIN_net(net_parameters)\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "num_epochs = 101\n",
    "for epoch in range(num_epochs): \n",
    "    epoch_train_loss, epoch_train_acc, optimizer = run_one_epoch(net, train_loader, loss, True, optimizer)\n",
    "    if not epoch%10:\n",
    "        with torch.no_grad(): \n",
    "            epoch_test_loss, epoch_test_acc, _ = run_one_epoch(net, test_loader, loss, False)\n",
    "        print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n",
    "        print('                      train_acc: {:.4f}, test_acc: {:.4f}'.format(epoch_train_acc, epoch_test_acc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Design GIN with node index as input node feature\n",
    "\n",
    "\\begin{eqnarray}\n",
    "h_i^{\\ell+1} &=& \\textrm{MLP} \\left( (1+\\varepsilon^{\\ell})\\ h_i^{\\ell} + \\sum_{j\\sim i} h_j^{\\ell} \\right) \\\\\n",
    "h_i^{\\ell=0} &=& i \\textrm{ (node index)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Report the training loss and test loss.\n",
    "\n",
    "Hint: You may use `batch_graphs.ndata['index']` to access node index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        super(GIN_net, self).__init__()\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        output_dim = net_parameters['output_dim']\n",
    "        L = net_parameters['L']\n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        self.embedding_h =   # node index as node feature\n",
    "        # YOUR CODE ENDS\n",
    "        ########################################       \n",
    "        self.GIN_layers = nn.ModuleList([ GIN_layer(hidden_dim, hidden_dim) for _ in range(L) ]) \n",
    "        self.MLP_layer = MLP_layer(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, g, h):\n",
    "        \n",
    "        # input embedding\n",
    "        h = self.embedding_h(h)\n",
    "        \n",
    "        # GIN layers\n",
    "        for GINlayer in self.GIN_layers:\n",
    "            h = GINlayer(g,h)\n",
    "        \n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        y = dgl.sum_nodes(g,'h')\n",
    "        y = self.MLP_layer(y)\n",
    "        \n",
    "        return y    \n",
    "\n",
    "def run_one_epoch(net, data_loader, lossCE, train=True, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        batch_x =   # node index as node feature\n",
    "        # YOUR CODE ENDS\n",
    "        ######################################## \n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x)\n",
    "        loss = lossCE(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        epoch_acc += accuracy(batch_scores,batch_labels)\n",
    "        nb_data += batch_labels.size(0)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_acc /= nb_data\n",
    "    return epoch_loss, epoch_acc, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "train_loader = DataLoader(trainset, batch_size=10, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=10, shuffle=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['hidden_dim'] = 256\n",
    "net_parameters['output_dim'] = len(list_skip_value) # nb of classes\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GIN_net(net_parameters)\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "num_epochs = 51\n",
    "for epoch in range(num_epochs): \n",
    "    epoch_train_loss, epoch_train_acc, optimizer = run_one_epoch(net, train_loader, loss, True, optimizer)\n",
    "    if not epoch%10:\n",
    "        with torch.no_grad(): \n",
    "            epoch_test_loss, epoch_test_acc, _ = run_one_epoch(net, test_loader, loss, False)\n",
    "        print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n",
    "        print('                      train_acc: {:.4f}, test_acc: {:.4f}'.format(epoch_train_acc, epoch_test_acc))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Design GIN with positional encoding as input node feature\n",
    "\n",
    "\\begin{eqnarray}\n",
    "h_i^{\\ell+1} &=& \\textrm{MLP} \\left( (1+\\varepsilon^{\\ell})\\ h_i^{\\ell} + \\sum_{j\\sim i} h_j^{\\ell} \\right) \\\\\n",
    "h_i^{\\ell=0} &=& i \\textrm{ (node positional encoding, here Laplacian eigenvectors)}\n",
    "\\end{eqnarray}\n",
    "\n",
    "Report the training loss and test loss.\n",
    "\n",
    "Hint: You may use `batch_graphs.ndata['pos_enc']` to access node index.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN_net(nn.Module):\n",
    "    \n",
    "    def __init__(self, net_parameters):\n",
    "        super(GIN_net, self).__init__()\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        output_dim = net_parameters['output_dim']\n",
    "        L = net_parameters['L']\n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        self.embedding_h =   # node positional encoding as node feature \n",
    "        # YOUR CODE ENDS\n",
    "        ########################################   \n",
    "        self.GIN_layers = nn.ModuleList([ GIN_layer(hidden_dim, hidden_dim) for _ in range(L) ]) \n",
    "        self.MLP_layer = MLP_layer(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, g, h):\n",
    "        \n",
    "        # input embedding\n",
    "        h = self.embedding_h(h)\n",
    "        \n",
    "        # GIN layers\n",
    "        for GINlayer in self.GIN_layers:\n",
    "            h = GINlayer(g,h)\n",
    "        \n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        y = dgl.sum_nodes(g,'h')\n",
    "        y = self.MLP_layer(y)\n",
    "        \n",
    "        return y    \n",
    "\n",
    "def run_one_epoch(net, data_loader, lossCE, train=True, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        ########################################\n",
    "        # YOUR CODE STARTS\n",
    "        batch_pe =  # node positional encoding as node feature \n",
    "        # YOUR CODE ENDS\n",
    "        ########################################    \n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_x = batch_pe # node positional encoding as node feature \n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x)\n",
    "        loss = lossCE(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        epoch_acc += accuracy(batch_scores,batch_labels)\n",
    "        nb_data += batch_labels.size(0)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_acc /= nb_data\n",
    "    return epoch_loss, epoch_acc, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "train_loader = DataLoader(trainset, batch_size=10, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=10, shuffle=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['hidden_dim'] = 256\n",
    "net_parameters['output_dim'] = len(list_skip_value) # nb of classes\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GIN_net(net_parameters)\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "num_epochs = 201\n",
    "for epoch in range(num_epochs): \n",
    "    epoch_train_loss, epoch_train_acc, optimizer = run_one_epoch(net, train_loader, loss, True, optimizer)\n",
    "    if not epoch%10:\n",
    "        with torch.no_grad(): \n",
    "            epoch_test_loss, epoch_test_acc, _ = run_one_epoch(net, test_loader, loss, False)\n",
    "        print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n",
    "        print('                      train_acc: {:.4f}, test_acc: {:.4f}'.format(epoch_train_acc, epoch_test_acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GRPE5gllGpA"
   },
   "source": [
    "## Question 6: Understand the results\n",
    "\n",
    "| GIN feature    | train acc | test acc |\n",
    "| -------- | ------- | ------- |\n",
    "| Node degree  |    |      |\n",
    "| Random feature |       |       |\n",
    "| Node index    |      |      |\n",
    "| Node positional encoding    |     |      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
