{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfFWcbbExXcc"
   },
   "source": [
    "# Lecture : Graph Transformers & Graph ViT\n",
    "\n",
    "## Lab 03 : Graph Transformers with edge features and DGL (sparse linear algebra) -- Solution\n",
    "\n",
    "### Xavier Bresson, Guoji Fu\n",
    "\n",
    "Dwivedi, Bresson, A generalization of transformer networks to graphs, 2020   \n",
    "https://arxiv.org/pdf/2012.09699.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7102,
     "status": "ok",
     "timestamp": 1730637248755,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "IZCvd1fTxXce",
    "outputId": "41ceed4f-96e9-4b1a-a395-f72c3621d79d"
   },
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/10_Graph_Transformers'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl==1.0.0 # Install DGL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y9hiy25BxXcf"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import networkx as nx\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_ncut\n",
    "from lib.molecules import Dictionary, MoleculeDataset, MoleculeDGL, Molecule\n",
    "import os, datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3Es6_zDxXcf"
   },
   "source": [
    "# Load molecular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4351,
     "status": "ok",
     "timestamp": 1730637253097,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "fl68-dJTxXcf",
    "outputId": "7f142b87-0e09-4434-d76b-18d370ec5f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "4\n",
      "Loading datasets QM9_1.4k_dgl...\n",
      "train, test, val sizes : 1000 200 200\n",
      "Time: 1.2283s\n",
      "1000\n",
      "200\n",
      "200\n",
      "([Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})], [tensor([-0.2532]), tensor([1.0897])])\n",
      "(Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([0.5060]))\n",
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-4.4048]))\n"
     ]
    }
   ],
   "source": [
    "# Select dataset\n",
    "dataset_name = 'QM9_1.4k'; data_folder_pytorch = 'dataset/QM9_1.4k_pytorch/'; data_folder_dgl = 'dataset/QM9_1.4k_dgl/'\n",
    "\n",
    "# Load the number of atom and bond types\n",
    "with open(data_folder_pytorch + \"atom_dict.pkl\" ,\"rb\") as f: num_atom_type = len(pickle.load(f))\n",
    "with open(data_folder_pytorch + \"bond_dict.pkl\" ,\"rb\") as f: num_bond_type = len(pickle.load(f))\n",
    "print(num_atom_type)\n",
    "print(num_bond_type)\n",
    "\n",
    "# Load the DGL datasets\n",
    "datasets_dgl = MoleculeDataset(dataset_name, data_folder_dgl)\n",
    "trainset, valset, testset = datasets_dgl.train, datasets_dgl.val, datasets_dgl.test\n",
    "print(len(trainset))\n",
    "print(len(valset))\n",
    "print(len(testset))\n",
    "idx = 0\n",
    "print(trainset[:2])\n",
    "print(valset[idx])\n",
    "print(testset[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTYjHxtUxXcg"
   },
   "source": [
    "# Add positional encoding feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1998,
     "status": "ok",
     "timestamp": 1730637255093,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "TQp3RdpAxXcg",
    "outputId": "19126433-8031-4aa7-d982-1cd2d516a142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'pos_enc': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-0.2532]))\n"
     ]
    }
   ],
   "source": [
    "# Positional encoding as Laplacian eigenvectors\n",
    "def LapEig_positional_encoding(g, pos_enc_dim):\n",
    "    Adj = g.adj().to_dense() # Adjacency matrix\n",
    "    Dn = ( g.in_degrees()** -0.5 ).diag() # Inverse and sqrt of degree matrix\n",
    "    Lap = torch.eye(g.number_of_nodes()) - Dn.matmul(Adj).matmul(Dn) # Laplacian operator\n",
    "    EigVal, EigVec = torch.linalg.eig(Lap) # Compute full EVD\n",
    "    EigVal, EigVec = EigVal.real, EigVec.real # make eig real\n",
    "    EigVec = EigVec[:, EigVal.argsort()] # sort in increasing order of eigenvalues\n",
    "    EigVec = EigVec[:,1:pos_enc_dim+1] # select the first non-trivial \"pos_enc_dim\" eigenvector\n",
    "    return EigVec\n",
    "\n",
    "# Add node positional encoding features to graphs\n",
    "pos_enc_dim = 3 # dimension of PE, QM9\n",
    "def add_node_edge_features(dataset):\n",
    "    for (graph,_) in dataset:\n",
    "        graph.ndata['pos_enc'] = LapEig_positional_encoding(graph, pos_enc_dim) # node positional encoding feature\n",
    "    return dataset\n",
    "\n",
    "# Generate graph datasets\n",
    "trainset = add_node_edge_features(trainset)\n",
    "testset = add_node_edge_features(testset)\n",
    "valset = add_node_edge_features(valset)\n",
    "print(trainset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukrFXYNexXcg"
   },
   "source": [
    "# Visualize positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1730637255907,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "i77JNkdBxXcg",
    "outputId": "a2e5b1cf-7e62-4fe9-8653-d1fb55bab668"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = trainset[0][0]\n",
    "\n",
    "# Visualize graph\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "Adj = graph.adj().to_dense()\n",
    "A_nx = nx.from_numpy_array(Adj.numpy())\n",
    "C = compute_ncut(Adj.long(), 4)\n",
    "nx.draw(A_nx, ax=ax, node_color=C, cmap='jet', with_labels=True, font_size=10) # visualise node indexes\n",
    "ax.title.set_text('Visualization with networkx')\n",
    "plt.show()\n",
    "\n",
    "# plot 2D coordinates\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = graph.ndata['pos_enc']\n",
    "ax.scatter(x[:,0], x[:,1])\n",
    "idx = list(range(graph.number_of_nodes()))\n",
    "ax.scatter(x[:,0], x[:,1], c=C, cmap='jet')\n",
    "for i, txt in enumerate(idx):\n",
    "    ax.annotate(txt, (x[:,0][i], x[:,1][i]), textcoords=\"offset points\", xytext=(1,5))\n",
    "ax.title.set_text('2D embdding of nodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R14LMFIRxXch"
   },
   "source": [
    "# Define the collate function to prepare a batch of DGL graphs and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 689,
     "status": "ok",
     "timestamp": 1730637256594,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "d7ZXaxwKxXch",
    "outputId": "af206596-e7cd-41ce-9b51-20b2b9e4167e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=87, num_edges=178,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'pos_enc': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})\n",
      "tensor([[ 0.3179],\n",
      "        [-0.7268],\n",
      "        [-0.4836],\n",
      "        [ 1.5437],\n",
      "        [-2.2482],\n",
      "        [-0.7211],\n",
      "        [ 0.2331],\n",
      "        [-2.4061],\n",
      "        [ 0.0602],\n",
      "        [-1.3969]])\n",
      "batch_x: torch.Size([87])\n",
      "batch_pe: torch.Size([87, 3])\n",
      "batch_e: torch.Size([178])\n"
     ]
    }
   ],
   "source": [
    "# collate function prepares a batch of graphs, labels and other graph features (if needed)\n",
    "def collate(samples):\n",
    "    # Input sample is a list of pairs (graph, label)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batch_graphs = dgl.batch(graphs)    # batch of graphs\n",
    "    batch_labels = torch.stack(labels)  # batch of labels (here chemical target)\n",
    "    return batch_graphs, batch_labels\n",
    "\n",
    "\n",
    "# Generate a batch of graphs\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "print(batch_graphs)\n",
    "print(batch_labels)\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "print('batch_x:',batch_x.size())\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "print('batch_pe:',batch_pe.size())\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "print('batch_e:',batch_e.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VBKKjxWxXch"
   },
   "source": [
    "# Exercise 1: Design the class of GraphTransformer networks with edge features\n",
    "\n",
    "Node update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=& h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell}),\\textrm{LN}(e^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h,e)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k,e_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{E\\times d'}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h,e)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T \\textrm{diag}(e_{ij}) k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T \\textrm{diag}(e_{ij'}) k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score w/ edge feature}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, E=e_k W_E\\in \\mathbb{R}^{E\\times d'=d/H}, W_Q, W_K, W_V, W_E\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Edge update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{e}^{\\ell} &=& e^{\\ell} + \\textrm{gMHE} (\\textrm{LN}(e^{\\ell}),\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{E\\times d}\\\\\n",
    "e^{\\ell+1} &=& \\bar{e}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{e}^{\\ell})) \\in \\mathbb{R}^{E\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHE}(e,h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHE}(e_k,h_k) \\right) W_O^e \\in \\mathbb{R}^{E\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{E\\times d'}, W_O^e\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\textrm{with } \\textrm{gHE}(e,h)_{ij}=q_i \\odot e_{ij} \\odot k_j/\\sqrt{d'} \\in \\mathbb{R}^{d'} \\textrm{ (point-wise equation)}\\\\\n",
    "e^{\\ell=0} &=& \\textrm{LL}(e_0) \\in \\mathbb{R}^{E\\times d}\\ \\textrm{(input edge feature)}\\\\\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqI7VXd9gchf"
   },
   "source": [
    "### Question 1.1: Implement a Graph Multi-Head Attention (MHA) Layer with edge features\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Step 1 of message-passing with DGL:* Pass node feature and edge features along edges (src/j => dst/i) by:\n",
    "    - *Step 1.1:* Compute bi-linear products with edge feature: $q_i^T * diag(e_{ij}) * k_j$. You may use ```edges.dst[]``` for ```i, edges.src[]``` for ```j, edges.data[]``` form ```ij```\".  \n",
    "\n",
    "    - *Step 1.2* Compute $\\textrm{exp}_{ij} = \\exp( q_i^T * k_j / \\sqrt{d'} )$, ```size=(E,K,1)```.\n",
    "\n",
    "    - *Step 1.3:* Obtain ```V```.\n",
    "\n",
    "    - *Step 1.4:* Compute edge feature: $q_i^T * diag(e_{ij}) * k_j$.\n",
    "\n",
    "    - *Step 1.5:* Update edge feature.\n",
    "\n",
    "- *Step 2 of message-passing with DGL:* Define a reduce function that\n",
    "    - *Step 2.1:* Use ```nodes.mailbox[]``` to collects all messages ```= {vj, eij}``` sent to node dst/i with *Step 1*.\n",
    "    \n",
    "    - *Step 2.2:* Sum/mean over the graph neigbors ```j``` in ```Ni```.\n",
    "\n",
    "- Assign ```Q, K, V, E, F, G```  to graphs by storing them in the ndata dictionary with the keys ```'Q', 'K', 'V', 'E', 'F', 'G'``` for ```g.ndata[]``` and reshape them using ```.view(-1, num_heads, head_hidden_dim)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQ-OgcDJ7rC4"
   },
   "outputs": [],
   "source": [
    "# class graph multi head attention layer\n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "\n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x W matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WE = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "\n",
    "    # Step 1 of message-passing with DGL:\n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i)\n",
    "    def message_func(self, edges):\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Step 1.1: Compute bi-linear products with edge feature\n",
    "        # src = j, dst = i, data=ij. edges.data['E'] is already a diag\n",
    "        # qi * eij * kj -> Sum based on j which is dim=2\n",
    "        # gHA(h, e)_ij\n",
    "        qikj = (edges.src['K'] * edges.data['E'] * edges.dst['Q']).sum(dim=2).unsqueeze(2) # size=(E,K,1), edges.src/dst/data[].size=(E,K,d')\n",
    "\n",
    "        # Step 1.2: Compute exp_ij = exp( q_i^T * k_j / sqrt(d') ), size=(E,K,1)\n",
    "        # Compute the numerator\n",
    "        expij = torch.exp( qikj / torch.sqrt(torch.tensor(self.head_hidden_dim)) )\n",
    "\n",
    "        # Step 1.3: Obtain vj\n",
    "        vj = edges.src['V'] # size=(E,K,d')\n",
    "\n",
    "        # Step 1.4: Compute edge feature: e_ij = q_i^T * diag(E_ij) * k_j / sqrt(d'), size=(E,K,d')\n",
    "        # gHE(e, h)_ij\n",
    "        eij = edges.src['K'] * edges.data['E'] * edges.dst['Q'] / torch.sqrt(torch.tensor(self.head_hidden_dim))\n",
    "\n",
    "        # Step 1.5: Update edge feature\n",
    "        edges.data['e'] = eij\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return {'expij' : expij, 'vj' : vj}\n",
    "\n",
    "    # Step 2 of message-passing with DGL:\n",
    "    #   Reduce function collects all messages={hj, eij} sent to node dst/i with Step 1\n",
    "    #                   and sum/mean over the graph neigbors j in Ni\n",
    "    # Compute gMHA(h, e)\n",
    "    def reduce_func(self, nodes):\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Step 2.1: Collects all messages= eij\n",
    "        # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        expij = nodes.mailbox['expij']\n",
    "\n",
    "        # Step 2.1: Collects all messages= vj\n",
    "        # size=(N,|Nj|,K,d')\n",
    "        vj = nodes.mailbox['vj']\n",
    "\n",
    "        # Step 2.2: Sum/mean over the graph neigbors j in Ni\n",
    "        # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        numerator = torch.sum( expij * vj, dim=1 )\n",
    "\n",
    "        # sum_j' exp_ij', size=(N,K,1)\n",
    "        denominator = torch.sum( expij, dim=1 )\n",
    "\n",
    "        # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        h = numerator / denominator\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return {'h' : h}\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "        E = self.WE(e) # size=(E, d)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        g.ndata['Q'] = Q.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = K.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = V.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.edata['E'] = E.view(-1, self.num_heads, self.head_hidden_dim) # size=(E, K, d'=d/K)\n",
    "        g.update_all(self.message_func, self.reduce_func) # compute with DGL the graph MHA\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        gMHE = g.edata['e'] # size=(E, K, d'=d/K)\n",
    "        return gMHA, gMHE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJix5TqbrFhR"
   },
   "source": [
    "### Question 1.2: Implement a Graph Transformer layer (with edge feature)\n",
    "\n",
    "- Implement dropout, layer normalization, and residual connection layers for edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Roe4rKU371mH"
   },
   "outputs": [],
   "source": [
    "# class GraphTransformer layer\n",
    "class GraphTransformer_layer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_h_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_h_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim) # layer normalization\n",
    "        self.layer_norm1e = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Dropout layers for edge features\n",
    "        self.dropout_e_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_e_mlp = nn.Dropout(dropout) # dropout value\n",
    "\n",
    "        # MLP layers for edge features\n",
    "        self.WOe = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "\n",
    "        # Layer normalization for edge features\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2e = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # MLP layers for edge features\n",
    "        self.linear1e = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2e = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "\n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        e_rc = e\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # layer normalization for edge features, size=(N, d)\n",
    "        e = self.layer_norm1e(e)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        h_MHA, e_MHE = self.gMHA(g, h, e) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_h_mha(h_MHA) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Update for edge features\n",
    "        e_MHE = e_MHE.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        e_MHE = self.dropout_e_mha(e_MHE) # dropout, size=(N, d)\n",
    "        e_MHE = self.WOe(e_MHE) # LL, size=(N, d)\n",
    "        e = e_rc + e_MHE # residual connection, size=(N, d)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        e_rc = e # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N, d)\n",
    "        e = self.layer_norm2e(e) # layer normalization, size=(N, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(H, d)\n",
    "        e_MLP = self.linear1e(e) # LL, size=(H, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        e_MLP = torch.relu(e_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_h_mlp(h_MLP) # dropout, size=(N, d)\n",
    "        e_MLP = self.dropout_e_mlp(e_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        e_MLP = self.linear2e(e_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "        e = e_rc + e_MLP # residual connection, size=(N, d)\n",
    "\n",
    "        return h, e\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rST8IpB2rYqK"
   },
   "source": [
    "### Question 1.3: Combine all previous defined MLP Layer, GraphTransformer layer to construct the Graph Transformer network (with edge feature)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Adding a input edge embedding layer:* Initialize a linear layer ```nn.Linear()``` to convert input edge features into edge embeddings.\n",
    "\n",
    "- *Graph transformer layer (with edge feature):* Initialize a ModuleList ```nn.ModuleList()``` containing ```L``` instances of ```GraphTransformer_layer()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2DQRl2bYxXch"
   },
   "outputs": [],
   "source": [
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        pos_enc_dim = net_parameters['pos_enc_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Initialize a edge embedding layer\n",
    "        self.embedding_e = nn.Embedding(num_bond_type, hidden_dim)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        self.embedding_pe = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ])\n",
    "        self.ln_h_final = nn.LayerNorm(hidden_dim)\n",
    "        self.linear_h_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, g, h, pe, e):\n",
    "\n",
    "        # input node embedding\n",
    "        h = self.embedding_h(h) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Implement teh edge embedding layer\n",
    "        # size=(num_edges, hidden_dim)\n",
    "        e = self.embedding_e(e)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h, e = GT_layer(g, h, e) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        mol_token = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)\n",
    "        y = self.ln_h_final(mol_token)\n",
    "        y = self.linear_h_final(y) # size=(num_graphs, num_classes)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730637256594,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "qzWVgqKW7vSU",
    "outputId": "3703d3bb-d48e-43d6-ed06-8723e11cb904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformer_net(\n",
      "  (embedding_h): Embedding(9, 128)\n",
      "  (embedding_e): Embedding(4, 128)\n",
      "  (embedding_pe): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (GraphTransformer_layers): ModuleList(\n",
      "    (0-3): 4 x GraphTransformer_layer(\n",
      "      (dropout_h_mha): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_h_mlp): Dropout(p=0.0, inplace=False)\n",
      "      (gMHA): graph_MHA_layer(\n",
      "        (WQ): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WK): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WV): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WE): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (WO): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm1e): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout_e_mha): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_e_mlp): Dropout(p=0.0, inplace=False)\n",
      "      (WOe): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2e): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1e): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2e): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_h_final): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_h_final): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate one network (testing)\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "print(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x, batch_pe, batch_e)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0gZzixzxXci"
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291348,
     "status": "ok",
     "timestamp": 1730637547939,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "Hw7LEG5exXci",
    "outputId": "ead6b872-7372-4f0c-cffa-845c8147baff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 667137 (0.67 million)\n",
      "Epoch 0, time 9.0502, train_loss: 1.3324, test_loss: 1.2443\n",
      "Epoch 1, time 17.8112, train_loss: 1.1559, test_loss: 1.0881\n",
      "Epoch 2, time 26.7555, train_loss: 1.0400, test_loss: 0.9704\n",
      "Epoch 3, time 35.6100, train_loss: 0.9704, test_loss: 0.9118\n",
      "Epoch 4, time 44.3237, train_loss: 1.0107, test_loss: 0.9752\n",
      "Epoch 5, time 52.6552, train_loss: 0.9685, test_loss: 0.9623\n",
      "Epoch 6, time 61.9425, train_loss: 0.9264, test_loss: 0.9242\n",
      "Epoch 7, time 70.9184, train_loss: 0.9074, test_loss: 0.8804\n",
      "Epoch 8, time 79.7594, train_loss: 0.8714, test_loss: 0.8484\n",
      "Epoch 9, time 88.7605, train_loss: 0.8982, test_loss: 0.9049\n",
      "Epoch 10, time 97.6549, train_loss: 0.8868, test_loss: 0.8364\n",
      "Epoch 11, time 106.1548, train_loss: 0.8553, test_loss: 0.8336\n",
      "Epoch 12, time 114.4364, train_loss: 0.8315, test_loss: 0.8141\n",
      "Epoch 13, time 122.8150, train_loss: 0.8217, test_loss: 0.8078\n",
      "Epoch 14, time 131.4215, train_loss: 0.8140, test_loss: 0.8103\n",
      "Epoch 15, time 140.0274, train_loss: 0.7906, test_loss: 0.7852\n",
      "Epoch 16, time 148.8601, train_loss: 0.7741, test_loss: 0.7893\n",
      "Epoch 17, time 157.8151, train_loss: 0.8024, test_loss: 0.8308\n",
      "Epoch 18, time 166.5377, train_loss: 0.7679, test_loss: 0.7765\n",
      "Epoch 19, time 175.4943, train_loss: 0.7518, test_loss: 0.7704\n",
      "Epoch 20, time 183.7578, train_loss: 0.7294, test_loss: 0.7976\n",
      "Epoch 21, time 192.1112, train_loss: 0.7308, test_loss: 0.7889\n",
      "Epoch 22, time 200.3581, train_loss: 0.7736, test_loss: 0.7742\n",
      "Epoch 23, time 208.5710, train_loss: 0.7195, test_loss: 0.7638\n",
      "Epoch 24, time 216.7313, train_loss: 0.7224, test_loss: 0.7812\n",
      "Epoch 25, time 225.1280, train_loss: 0.7138, test_loss: 0.7781\n",
      "Epoch 26, time 233.4546, train_loss: 0.7195, test_loss: 0.7723\n",
      "Epoch 27, time 241.7371, train_loss: 0.7003, test_loss: 0.7462\n",
      "Epoch 28, time 249.9482, train_loss: 0.7175, test_loss: 0.7705\n",
      "Epoch 29, time 257.9061, train_loss: 0.6747, test_loss: 0.7810\n",
      "Epoch 30, time 265.7278, train_loss: 0.6686, test_loss: 0.7594\n",
      "Epoch 31, time 274.1824, train_loss: 0.6894, test_loss: 0.8283\n",
      "Epoch 32, time 282.4863, train_loss: 0.6928, test_loss: 0.7845\n",
      "Epoch 33, time 290.8057, train_loss: 0.6881, test_loss: 0.7600\n",
      "Epoch 34, time 299.0464, train_loss: 0.6447, test_loss: 0.7219\n",
      "Epoch 35, time 307.3488, train_loss: 0.6309, test_loss: 0.7442\n",
      "Epoch 36, time 315.6931, train_loss: 0.6500, test_loss: 0.7642\n",
      "Epoch 37, time 323.9306, train_loss: 0.6329, test_loss: 0.7606\n",
      "Epoch 38, time 332.2689, train_loss: 0.6284, test_loss: 0.7831\n",
      "Epoch 39, time 340.5385, train_loss: 0.6430, test_loss: 0.7614\n",
      "Epoch 40, time 348.8184, train_loss: 0.6306, test_loss: 0.7583\n",
      "Epoch 41, time 357.1268, train_loss: 0.6299, test_loss: 0.7520\n",
      "Epoch 42, time 365.6018, train_loss: 0.6229, test_loss: 0.7589\n",
      "Epoch 43, time 374.7649, train_loss: 0.6168, test_loss: 0.7338\n",
      "Epoch 44, time 383.8353, train_loss: 0.6105, test_loss: 0.7456\n",
      "Epoch 45, time 392.0805, train_loss: 0.6072, test_loss: 0.7385\n",
      "Epoch 46, time 400.2162, train_loss: 0.6117, test_loss: 0.7554\n",
      "Epoch 47, time 408.6412, train_loss: 0.6310, test_loss: 0.7493\n",
      "Epoch 48, time 416.9082, train_loss: 0.5845, test_loss: 0.7292\n",
      "Epoch 49, time 425.3009, train_loss: 0.5836, test_loss: 0.7177\n",
      "Epoch 50, time 433.4260, train_loss: 0.5828, test_loss: 0.7195\n",
      "Epoch 51, time 441.3342, train_loss: 0.5898, test_loss: 0.7314\n",
      "Epoch 52, time 449.1814, train_loss: 0.5873, test_loss: 0.7153\n",
      "Epoch 53, time 457.3348, train_loss: 0.5868, test_loss: 0.7400\n",
      "Epoch 54, time 465.4227, train_loss: 0.6009, test_loss: 0.7376\n",
      "Epoch 55, time 473.6560, train_loss: 0.5626, test_loss: 0.7124\n",
      "Epoch 56, time 481.9323, train_loss: 0.5571, test_loss: 0.7376\n",
      "Epoch 57, time 490.0985, train_loss: 0.5514, test_loss: 0.7339\n",
      "Epoch 58, time 498.2905, train_loss: 0.5495, test_loss: 0.7017\n",
      "Epoch 59, time 506.5198, train_loss: 0.5647, test_loss: 0.7172\n",
      "Epoch 60, time 514.9660, train_loss: 0.5787, test_loss: 0.7076\n",
      "Epoch 61, time 523.1783, train_loss: 0.5440, test_loss: 0.7411\n",
      "Epoch 62, time 531.6079, train_loss: 0.5257, test_loss: 0.7163\n",
      "Epoch 63, time 539.7631, train_loss: 0.5343, test_loss: 0.7549\n",
      "Epoch 64, time 548.0209, train_loss: 0.5494, test_loss: 0.7074\n",
      "Epoch 65, time 556.1993, train_loss: 0.5258, test_loss: 0.7229\n",
      "Epoch 66, time 564.3974, train_loss: 0.5191, test_loss: 0.7330\n",
      "Epoch 67, time 572.5418, train_loss: 0.5225, test_loss: 0.7093\n",
      "Epoch 68, time 580.9699, train_loss: 0.5146, test_loss: 0.7172\n",
      "Epoch 69, time 589.2618, train_loss: 0.5141, test_loss: 0.7125\n",
      "Epoch 70, time 597.2934, train_loss: 0.5177, test_loss: 0.7047\n",
      "Epoch 71, time 605.2950, train_loss: 0.5356, test_loss: 0.7499\n",
      "Epoch 72, time 613.2405, train_loss: 0.5179, test_loss: 0.7233\n",
      "Epoch 73, time 621.4069, train_loss: 0.5072, test_loss: 0.7063\n",
      "Epoch 74, time 629.7036, train_loss: 0.5085, test_loss: 0.7428\n",
      "Epoch 75, time 637.7022, train_loss: 0.5205, test_loss: 0.7319\n",
      "Epoch 76, time 645.6248, train_loss: 0.5188, test_loss: 0.7282\n",
      "Epoch 77, time 653.8837, train_loss: 0.4852, test_loss: 0.7028\n",
      "Epoch 78, time 662.0734, train_loss: 0.5054, test_loss: 0.7064\n",
      "Epoch 79, time 670.2661, train_loss: 0.4939, test_loss: 0.7126\n",
      "Epoch 80, time 678.5327, train_loss: 0.5212, test_loss: 0.7699\n",
      "Epoch 81, time 686.8720, train_loss: 0.4895, test_loss: 0.7113\n",
      "Epoch 82, time 695.1367, train_loss: 0.4837, test_loss: 0.7065\n",
      "Epoch 83, time 703.3309, train_loss: 0.4846, test_loss: 0.7030\n",
      "Epoch 84, time 711.6361, train_loss: 0.4714, test_loss: 0.7294\n",
      "Epoch 85, time 719.7286, train_loss: 0.5029, test_loss: 0.7228\n",
      "Epoch 86, time 727.8222, train_loss: 0.4867, test_loss: 0.7229\n",
      "Epoch 87, time 736.4937, train_loss: 0.4755, test_loss: 0.7174\n",
      "Epoch 88, time 745.8665, train_loss: 0.4728, test_loss: 0.7282\n",
      "Epoch 89, time 755.8208, train_loss: 0.4594, test_loss: 0.7240\n",
      "Epoch 90, time 765.5708, train_loss: 0.4623, test_loss: 0.7165\n",
      "Epoch 91, time 774.3950, train_loss: 0.4507, test_loss: 0.7372\n",
      "Epoch 92, time 783.1617, train_loss: 0.4628, test_loss: 0.7329\n",
      "Epoch 93, time 791.9438, train_loss: 0.4588, test_loss: 0.7593\n",
      "Epoch 94, time 800.7288, train_loss: 0.4867, test_loss: 0.7231\n",
      "Epoch 95, time 809.6506, train_loss: 0.4991, test_loss: 0.7269\n",
      "Epoch 96, time 818.3691, train_loss: 0.5088, test_loss: 0.7453\n",
      "Epoch 97, time 827.2714, train_loss: 0.4620, test_loss: 0.7444\n",
      "Epoch 98, time 836.2346, train_loss: 0.4376, test_loss: 0.7160\n",
      "Epoch 99, time 845.2786, train_loss: 0.4385, test_loss: 0.7199\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        bs2 = batch_labels.size(0)\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_e = batch_graphs.edata['feat']\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_pe, batch_e)\n",
    "        lossMAE = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            lossMAE.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += bs2 * lossMAE.detach().item()\n",
    "        nb_data += bs2\n",
    "    epoch_loss /= nb_data\n",
    "    return epoch_loss, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "lossMAE = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    epoch_train_loss, optimizer = run_one_epoch(net, train_loader, True, lossMAE, optimizer)\n",
    "    with torch.no_grad():\n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False, lossMAE)[0]\n",
    "        # epoch_val_loss = run_one_epoch(net, val_loader, False, lossMAE)[0]\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehCjHW1YxXci"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Amo_4YiKxXci"
   },
   "source": [
    "# GT without edge features\n",
    "\n",
    "Node update equation:\n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=& h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h)=\\textrm{Softmax}\\left( A_G \\odot \\frac{QK^T}{\\sqrt{d'}} \\right) V \\in \\mathbb{R}^{N\\times d'=d/H}, A_G\\in \\mathbb{R}^{N\\times N} \\textrm{ (graph adjacency matrix)}\\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, W_Q, W_K, W_V\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BKceFO_W8HBf"
   },
   "outputs": [],
   "source": [
    "# class graph multi head attention layer\n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "\n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x WQ matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "\n",
    "    # Step 1 of message-passing with DGL:\n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i)\n",
    "    def message_func(self, edges):\n",
    "        # Compute the dot products q_i^T * k_j\n",
    "        # You may use \"edges.dst[] for i, edges.src[] for j\"\n",
    "        qikj = (edges.dst['Q'] * edges.src['K']).sum(dim=2).unsqueeze(2) # all dot products q_i^T * k_j, size=(E,K,1), edges.src/dst[].size=(E,K,d')\n",
    "        expij = torch.exp( qikj / torch.sqrt(torch.tensor(self.head_hidden_dim)) ) # exp_ij = exp( clamp(q_i^T * k_j / sqrt(d')) ), size=(E,K,1)\n",
    "        vj = edges.src['V'] # size=(E,K,d')\n",
    "        return {'expij' : expij, 'vj' : vj}\n",
    "\n",
    "    # Step 2 of message-passing with DGL:\n",
    "    #   Reduce function collects all messages={hj, eij} sent to node dst/i with Step 1\n",
    "    #                   and sum/mean over the graph neigbors j in Ni\n",
    "    def reduce_func(self, nodes):\n",
    "        expij = nodes.mailbox['expij'] # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        vj = nodes.mailbox['vj'] # size=(N,|Nj|,K,d')\n",
    "        # Compute h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij'\n",
    "        numerator = torch.sum( expij * vj, dim=1 ) # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        denominator = torch.sum( expij, dim=1 ) # sum_j' exp_ij', size=(N,K,1)\n",
    "        h = numerator / denominator # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        return {'h' : h}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "        g.ndata['Q'] = Q.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = K.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = V.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.update_all(self.message_func, self.reduce_func) # compute with DGL the graph MHA\n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        return gMHA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lWOIc6W_8JY4"
   },
   "outputs": [],
   "source": [
    "# class GraphTransformer layer\n",
    "class GraphTransformer_layer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim) # layer normalization\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "\n",
    "    def forward(self, g, h):\n",
    "\n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        h_MHA = self.gMHA(g, h) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_mha(h_MHA) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "\n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(H, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_mlp(h_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o_Gaf_Vi8MQt"
   },
   "outputs": [],
   "source": [
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        pos_enc_dim = net_parameters['pos_enc_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "        self.embedding_pe = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        self.embedding_e = nn.Linear(1, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ])\n",
    "        self.ln_h_final = nn.LayerNorm(hidden_dim)\n",
    "        self.linear_h_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, g, h, pe):\n",
    "\n",
    "        # input node embedding = node in-degree feature\n",
    "        h = self.embedding_h(h) # in-degree feature, size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h = GT_layer(g,h) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        mol_token = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)\n",
    "        y = self.ln_h_final(mol_token)\n",
    "        y = self.linear_h_final(y) # size=(num_graphs, 1)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730637547939,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "rPtk6EjKxXci",
    "outputId": "8db1c7ef-bbc3-4f67-df10-3ce74c4fac93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 400641 (0.40 million)\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "_ = display_num_param(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x, batch_pe)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jt8BzHJxXci"
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 168398,
     "status": "ok",
     "timestamp": 1730637716334,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "xAdlIhxXxXci",
    "outputId": "ec39dc6a-0677-4b9e-93a0-d79426d21d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 400641 (0.40 million)\n",
      "Epoch 0, time 8.4898, train_loss: 1.3367, test_loss: 1.2856\n",
      "Epoch 1, time 16.9636, train_loss: 1.2364, test_loss: 1.1967\n",
      "Epoch 2, time 25.5298, train_loss: 1.1725, test_loss: 1.1648\n",
      "Epoch 3, time 33.9296, train_loss: 1.1198, test_loss: 1.1598\n",
      "Epoch 4, time 42.3989, train_loss: 1.1184, test_loss: 1.1182\n",
      "Epoch 5, time 50.8900, train_loss: 1.0994, test_loss: 1.2465\n",
      "Epoch 6, time 59.0548, train_loss: 1.0752, test_loss: 1.0763\n",
      "Epoch 7, time 67.2227, train_loss: 1.0457, test_loss: 1.0393\n",
      "Epoch 8, time 75.4204, train_loss: 1.0410, test_loss: 1.0896\n",
      "Epoch 9, time 83.6352, train_loss: 1.0123, test_loss: 1.0494\n",
      "Epoch 10, time 92.0175, train_loss: 0.9821, test_loss: 0.9574\n",
      "Epoch 11, time 100.6100, train_loss: 0.9682, test_loss: 0.9723\n",
      "Epoch 12, time 109.0475, train_loss: 0.9532, test_loss: 0.9749\n",
      "Epoch 13, time 117.5556, train_loss: 0.9405, test_loss: 0.9918\n",
      "Epoch 14, time 126.0614, train_loss: 0.9609, test_loss: 0.9503\n",
      "Epoch 15, time 134.3755, train_loss: 0.9694, test_loss: 0.9630\n",
      "Epoch 16, time 142.8571, train_loss: 0.9204, test_loss: 0.9695\n",
      "Epoch 17, time 150.6552, train_loss: 0.8921, test_loss: 0.9446\n",
      "Epoch 18, time 157.7684, train_loss: 0.9115, test_loss: 0.9908\n",
      "Epoch 19, time 164.8463, train_loss: 0.9311, test_loss: 0.9417\n",
      "Epoch 20, time 172.0333, train_loss: 0.9263, test_loss: 1.0340\n",
      "Epoch 21, time 179.1913, train_loss: 0.9294, test_loss: 0.9776\n",
      "Epoch 22, time 186.3684, train_loss: 0.9217, test_loss: 0.9185\n",
      "Epoch 23, time 193.4921, train_loss: 0.8746, test_loss: 0.9471\n",
      "Epoch 24, time 200.6739, train_loss: 0.8857, test_loss: 0.9241\n",
      "Epoch 25, time 207.7886, train_loss: 0.8808, test_loss: 0.9080\n",
      "Epoch 26, time 214.9202, train_loss: 0.9082, test_loss: 0.9261\n",
      "Epoch 27, time 222.0377, train_loss: 0.9053, test_loss: 0.9104\n",
      "Epoch 28, time 229.1919, train_loss: 0.8768, test_loss: 1.0151\n",
      "Epoch 29, time 236.3382, train_loss: 0.8920, test_loss: 0.9251\n",
      "Epoch 30, time 243.4808, train_loss: 0.8801, test_loss: 0.9473\n",
      "Epoch 31, time 250.6284, train_loss: 0.8653, test_loss: 0.9366\n",
      "Epoch 32, time 257.7782, train_loss: 0.8833, test_loss: 0.9677\n",
      "Epoch 33, time 264.9036, train_loss: 0.8972, test_loss: 0.9329\n",
      "Epoch 34, time 272.0524, train_loss: 0.8664, test_loss: 0.9221\n",
      "Epoch 35, time 279.2004, train_loss: 0.8617, test_loss: 0.9035\n",
      "Epoch 36, time 286.3644, train_loss: 0.8525, test_loss: 0.8932\n",
      "Epoch 37, time 293.4742, train_loss: 0.8408, test_loss: 0.9227\n",
      "Epoch 38, time 300.6355, train_loss: 0.8457, test_loss: 0.9159\n",
      "Epoch 39, time 307.7755, train_loss: 0.8682, test_loss: 0.9329\n",
      "Epoch 40, time 314.8097, train_loss: 0.8759, test_loss: 0.9094\n",
      "Epoch 41, time 321.9024, train_loss: 0.8425, test_loss: 0.9302\n",
      "Epoch 42, time 328.9523, train_loss: 0.8668, test_loss: 0.9161\n",
      "Epoch 43, time 336.0387, train_loss: 0.8308, test_loss: 0.8963\n",
      "Epoch 44, time 343.1315, train_loss: 0.8422, test_loss: 0.9162\n",
      "Epoch 45, time 350.2461, train_loss: 0.8374, test_loss: 0.9164\n",
      "Epoch 46, time 357.3148, train_loss: 0.8355, test_loss: 0.9270\n",
      "Epoch 47, time 364.4446, train_loss: 0.8184, test_loss: 0.9361\n",
      "Epoch 48, time 371.5291, train_loss: 0.8255, test_loss: 0.9205\n",
      "Epoch 49, time 378.6393, train_loss: 0.8200, test_loss: 0.8932\n",
      "Epoch 50, time 385.7812, train_loss: 0.8110, test_loss: 0.9190\n",
      "Epoch 51, time 392.8606, train_loss: 0.8204, test_loss: 0.9272\n",
      "Epoch 52, time 399.9997, train_loss: 0.8176, test_loss: 0.9537\n",
      "Epoch 53, time 407.1903, train_loss: 0.8227, test_loss: 0.8994\n",
      "Epoch 54, time 414.5294, train_loss: 0.7882, test_loss: 0.8993\n",
      "Epoch 55, time 422.0162, train_loss: 0.8185, test_loss: 0.9593\n",
      "Epoch 56, time 429.2032, train_loss: 0.8235, test_loss: 0.9461\n",
      "Epoch 57, time 436.3614, train_loss: 0.8200, test_loss: 0.9130\n",
      "Epoch 58, time 443.4307, train_loss: 0.8028, test_loss: 0.9160\n",
      "Epoch 59, time 450.4789, train_loss: 0.8036, test_loss: 0.9150\n",
      "Epoch 60, time 457.6351, train_loss: 0.8117, test_loss: 0.9574\n",
      "Epoch 61, time 464.7895, train_loss: 0.7882, test_loss: 0.8911\n",
      "Epoch 62, time 471.9503, train_loss: 0.7913, test_loss: 0.9207\n",
      "Epoch 63, time 479.0896, train_loss: 0.7825, test_loss: 0.8952\n",
      "Epoch 64, time 486.2321, train_loss: 0.7704, test_loss: 0.8879\n",
      "Epoch 65, time 493.3837, train_loss: 0.7661, test_loss: 0.9114\n",
      "Epoch 66, time 500.5665, train_loss: 0.7993, test_loss: 0.9300\n",
      "Epoch 67, time 507.7025, train_loss: 0.7764, test_loss: 0.9440\n",
      "Epoch 68, time 514.8399, train_loss: 0.7664, test_loss: 0.9130\n",
      "Epoch 69, time 521.9886, train_loss: 0.7438, test_loss: 0.9112\n",
      "Epoch 70, time 529.1543, train_loss: 0.7763, test_loss: 0.8960\n",
      "Epoch 71, time 536.2557, train_loss: 0.7531, test_loss: 0.8939\n",
      "Epoch 72, time 543.3204, train_loss: 0.7371, test_loss: 0.8778\n",
      "Epoch 73, time 550.4872, train_loss: 0.7273, test_loss: 0.9043\n",
      "Epoch 74, time 557.5577, train_loss: 0.7278, test_loss: 0.8732\n",
      "Epoch 75, time 564.6094, train_loss: 0.7235, test_loss: 0.9311\n",
      "Epoch 76, time 571.7442, train_loss: 0.7384, test_loss: 0.9263\n",
      "Epoch 77, time 578.9453, train_loss: 0.7398, test_loss: 0.9133\n",
      "Epoch 78, time 586.1379, train_loss: 0.7530, test_loss: 0.9043\n",
      "Epoch 79, time 593.3087, train_loss: 0.7463, test_loss: 0.9029\n",
      "Epoch 80, time 600.5315, train_loss: 0.7241, test_loss: 0.9132\n",
      "Epoch 81, time 607.6422, train_loss: 0.7131, test_loss: 0.8878\n",
      "Epoch 82, time 614.7918, train_loss: 0.7144, test_loss: 0.9110\n",
      "Epoch 83, time 621.9337, train_loss: 0.6864, test_loss: 0.9044\n",
      "Epoch 84, time 629.1133, train_loss: 0.6896, test_loss: 0.9191\n",
      "Epoch 85, time 636.2783, train_loss: 0.6891, test_loss: 0.9556\n",
      "Epoch 86, time 643.4115, train_loss: 0.7113, test_loss: 0.8987\n",
      "Epoch 87, time 650.5480, train_loss: 0.6921, test_loss: 0.9383\n",
      "Epoch 88, time 657.7048, train_loss: 0.7153, test_loss: 0.9392\n",
      "Epoch 89, time 664.8551, train_loss: 0.6949, test_loss: 0.9181\n",
      "Epoch 90, time 672.0542, train_loss: 0.6997, test_loss: 0.9095\n",
      "Epoch 91, time 679.2282, train_loss: 0.6829, test_loss: 0.9136\n",
      "Epoch 92, time 686.3587, train_loss: 0.6900, test_loss: 0.9214\n",
      "Epoch 93, time 693.4418, train_loss: 0.6729, test_loss: 0.9199\n",
      "Epoch 94, time 700.6109, train_loss: 0.6705, test_loss: 0.8886\n",
      "Epoch 95, time 707.7536, train_loss: 0.6813, test_loss: 0.9129\n",
      "Epoch 96, time 714.8378, train_loss: 0.6991, test_loss: 0.8970\n",
      "Epoch 97, time 721.9729, train_loss: 0.6727, test_loss: 0.9223\n",
      "Epoch 98, time 729.0403, train_loss: 0.6661, test_loss: 0.9357\n",
      "Epoch 99, time 736.1562, train_loss: 0.6463, test_loss: 0.9026\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        bs2 = batch_labels.size(0)\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_pe)\n",
    "        lossMAE = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            lossMAE.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += bs2 * lossMAE.detach().item()\n",
    "        nb_data += bs2\n",
    "    epoch_loss /= nb_data\n",
    "    return epoch_loss, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "lossMAE = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    epoch_train_loss, optimizer = run_one_epoch(net, train_loader, True, lossMAE, optimizer)\n",
    "    with torch.no_grad():\n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False, lossMAE)[0]\n",
    "        # epoch_val_loss = run_one_epoch(net, val_loader, False, lossMAE)[0]\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8uBSgqTxXcj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWLI02VPxXcj"
   },
   "source": [
    "## Compare results\n",
    "\n",
    "| GNN    | train MAE | test MAE |\n",
    "| -------- | ------- | ------- |\n",
    "| GT w/ edge features (bond type)    | 0.4483    | 0.7327    |\n",
    "| GT without edge features (only atom type)    | 0.6583   | 0.9095    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d26mlnkvxXcj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j73ydTFQxXcj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gnn_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
