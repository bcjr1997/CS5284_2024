{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6JtO7-nxVZz"
   },
   "source": [
    "# Lecture : Graph Transformers & Graph ViT\n",
    "\n",
    "## Lab 04 : Graph Transformers with edge features and PyTorch (dense linear algebra) -- Exercise\n",
    "\n",
    "### Xavier Bresson, Guoji Fu\n",
    "\n",
    "Dwivedi, Bresson, A generalization of transformer networks to graphs, 2020   \n",
    "https://arxiv.org/pdf/2012.09699.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10358,
     "status": "ok",
     "timestamp": 1730638731947,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "SlHbOw3NxVZ0",
    "outputId": "a1054b9d-6b96-4525-f856-7404ac72733c"
   },
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/10_Graph_Transformers'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl==1.0.0 # Install DGL\n",
    "    !pip install rdkit==2023.09.6 # Install RDKit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1666,
     "status": "ok",
     "timestamp": 1730638733609,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "t4gOKvLQxVZ1",
    "outputId": "811e75af-6f4e-47e2-be7e-b3ffe5d8f54d"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import networkx as nx\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_ncut\n",
    "from lib.molecules import Dictionary, MoleculeDataset, MoleculeDGL, Molecule\n",
    "import os, datetime\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdmolops\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1730638733610,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "nsdrLo3pxVZ1",
    "outputId": "2da8f161-3ea8-4c4d-fa9f-f5e331d22cc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n",
      "NVIDIA GeForce RTX 4090 Laptop GPU\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch version and GPU\n",
    "print(torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "  device= torch.device(\"cuda\") # use GPU\n",
    "else:\n",
    "  device= torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4-MeJfhxVZ2"
   },
   "source": [
    "# Load molecular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 4699,
     "status": "ok",
     "timestamp": 1730638738305,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "Jl7rS9Z6xVZ2",
    "outputId": "f6b04892-0fba-4c88-88f0-aed5d6add3ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "dataset/QM9_1.4k_pytorch/\n",
      "Time: 0.33646655082702637\n",
      "num train data : 1000\n",
      "atom_dict.idx2word : ['N', 'C', 'O', 'F', 'N H3 +', 'O -', 'C H1 -', 'N +', 'N -']\n",
      "atom_dict.word2idx : {'N': 0, 'C': 1, 'O': 2, 'F': 3, 'N H3 +': 4, 'O -': 5, 'C H1 -': 6, 'N +': 7, 'N -': 8}\n",
      "bond_dict.idx2word : ['NONE', 'SINGLE', 'DOUBLE', 'TRIPLE']\n",
      "bond_dict.word2idx : {'NONE': 0, 'SINGLE': 1, 'DOUBLE': 2, 'TRIPLE': 3}\n",
      "9 4\n",
      "train[idx].atom_type : tensor([1, 1, 2, 1, 1, 1, 1])\n",
      "train[idx].atom_type_pe : tensor([0, 1, 0, 2, 3, 4, 5])\n",
      "train[idx].bond_type : tensor([[0, 1, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 1, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 1, 1, 0],\n",
      "        [0, 0, 0, 1, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 1, 0, 1],\n",
      "        [0, 0, 0, 0, 0, 1, 0]])\n",
      "train[idx].bag_of_atoms : tensor([0, 6, 1, 0, 0, 0, 0, 0, 0])\n",
      "train[idx].smile:  CC(O)C1CC1C\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x16fd7ccf660>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "start = time.time()\n",
    "\n",
    "data_folder_pytorch = 'dataset/QM9_1.4k_pytorch/'\n",
    "print(data_folder_pytorch)\n",
    "\n",
    "with open(data_folder_pytorch+\"atom_dict.pkl\",\"rb\") as f:\n",
    "    atom_dict=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"bond_dict.pkl\",\"rb\") as f:\n",
    "    bond_dict=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"test_pytorch.pkl\",\"rb\") as f:\n",
    "    test=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"val_pytorch.pkl\",\"rb\") as f:\n",
    "    val=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"train_pytorch.pkl\",\"rb\") as f:\n",
    "    train=pickle.load(f)\n",
    "print('Time:',time.time()-start)\n",
    "\n",
    "print('num train data :',len(train))\n",
    "\n",
    "print('atom_dict.idx2word :',atom_dict.idx2word)\n",
    "print('atom_dict.word2idx :',atom_dict.word2idx)\n",
    "print('bond_dict.idx2word :',bond_dict.idx2word)\n",
    "print('bond_dict.word2idx :',bond_dict.word2idx)\n",
    "\n",
    "num_atom_type = len(atom_dict.idx2word)\n",
    "num_bond_type = len(bond_dict.idx2word)\n",
    "print(num_atom_type, num_bond_type)\n",
    "\n",
    "idx = 45\n",
    "print('train[idx].atom_type :',train[idx].atom_type)\n",
    "print('train[idx].atom_type_pe :',train[idx].atom_type_pe)\n",
    "print('train[idx].bond_type :',train[idx].bond_type)\n",
    "print('train[idx].bag_of_atoms :',train[idx].bag_of_atoms)\n",
    "print('train[idx].smile: ',train[idx].smile)\n",
    "mol = Chem.MolFromSmiles(train[idx].smile)\n",
    "mol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9HcvHgMxVZ2"
   },
   "source": [
    "# Print dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1730638738306,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "FXjnSa2vxVZ2",
    "outputId": "f8280897-c066-44ef-cbc6-25cf527819c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n",
      "Max num atoms =  9\n",
      "Train\n",
      "number of molecule of size 4: \t 1\n",
      "number of molecule of size 5: \t 1\n",
      "number of molecule of size 6: \t 7\n",
      "number of molecule of size 7: \t 24\n",
      "number of molecule of size 8: \t 136\n",
      "number of molecule of size 9: \t 831\n",
      "Val\n",
      "number of molecule of size 7: \t 6\n",
      "number of molecule of size 8: \t 28\n",
      "number of molecule of size 9: \t 166\n",
      "Test\n",
      "number of molecule of size 6: \t 1\n",
      "number of molecule of size 7: \t 3\n",
      "number of molecule of size 8: \t 37\n",
      "number of molecule of size 9: \t 159\n"
     ]
    }
   ],
   "source": [
    "# Organize data into group of of molecules of fixed sized\n",
    "# Example: train[22] is a list containing all the molecules of size 22\n",
    "def group_molecules_per_size(dataset):\n",
    "    mydict={}\n",
    "    for mol in dataset:\n",
    "        if len(mol) not in mydict:\n",
    "            mydict[len(mol)]=[]\n",
    "        mydict[len(mol)].append(mol)\n",
    "    return mydict\n",
    "test_group  = group_molecules_per_size(test)\n",
    "val_group   = group_molecules_per_size(val)\n",
    "train_group = group_molecules_per_size(train)\n",
    "print(len(train_group[8])) # QM9\n",
    "# print(len(train_group[28])) # ZINC\n",
    "\n",
    "# what is the biggest molecule in the train set\n",
    "max_mol_sz= max(list( train_group.keys()))\n",
    "print('Max num atoms = ', max_mol_sz)\n",
    "\n",
    "# print distribution w.r.t. molecule size\n",
    "def print_distribution(data):\n",
    "    for nb_atom in range(max_mol_sz+1):\n",
    "        try:\n",
    "            print('number of molecule of size {}: \\t {}'.format(nb_atom, len(data[nb_atom])))\n",
    "        except:\n",
    "            pass\n",
    "print('Train'); print_distribution(train_group)\n",
    "print('Val'); print_distribution(val_group)\n",
    "print('Test'); print_distribution(test_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhrS0_D7xVZ3"
   },
   "source": [
    "# Generate batch of pytorch molecules of same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsAGj6cB8i0A"
   },
   "source": [
    "### Implement the molecule sampler class for batch sampling of molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AJLO8Iby8ZkO"
   },
   "outputs": [],
   "source": [
    "# A class to help drawing batches of molecules having the same size\n",
    "class MoleculeSampler:\n",
    "    def __init__(self, organized_dataset, bs , shuffle=True):\n",
    "        self.bs = bs\n",
    "        self.num_mol =  { sz: len(list_of_mol)  for sz , list_of_mol in organized_dataset.items() }\n",
    "        self.counter = { sz: 0   for sz in organized_dataset }\n",
    "        if shuffle:\n",
    "            self.order = { sz: np.random.permutation(num)  for sz , num in self.num_mol.items() }\n",
    "        else:\n",
    "            self.order = { sz: np.arange(num)  for sz , num in self.num_mol.items() }\n",
    "\n",
    "    def compute_num_batches_remaining(self):\n",
    "        #return {sz:  ( self.num_mol[sz] - self.counter[sz] ) // self.bs  for sz in self.num_mol}\n",
    "        return {sz:  math.ceil(((self.num_mol[sz] - self.counter[sz])/self.bs))  for sz in self.num_mol}\n",
    "\n",
    "    def choose_molecule_size(self):\n",
    "        num_batches = self.compute_num_batches_remaining()\n",
    "        possible_sizes =  np.array( list( num_batches.keys()) )\n",
    "        prob           =  np.array( list( num_batches.values() )   )\n",
    "        prob =  prob / prob.sum()\n",
    "        sz   = np.random.choice(  possible_sizes , p=prob )\n",
    "        return sz\n",
    "\n",
    "    def is_empty(self):\n",
    "        num_batches= self.compute_num_batches_remaining()\n",
    "        return sum( num_batches.values() ) == 0\n",
    "\n",
    "    def draw_batch_of_molecules(self, sz):\n",
    "        num_batches = self.compute_num_batches_remaining()\n",
    "        if (self.num_mol[sz] - self.counter[sz])/self.bs >= 1.0:\n",
    "            bs = self.bs\n",
    "        else:\n",
    "            bs = self.num_mol[sz] - (self.num_mol[sz]//self.bs) * self.bs\n",
    "        #print('sz, bs',sz, bs)\n",
    "        indices = self.order[sz][ self.counter[sz] : self.counter[sz] + bs]\n",
    "        self.counter[sz] += bs\n",
    "        return indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1Xnq_v29VC6"
   },
   "source": [
    "### Extract one mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1730638738306,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "uHjZ9bskxVZ3",
    "outputId": "da30f87b-e3b4-4b5f-8493-2f5ea37470c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampler.num_mol : {9: 831, 8: 136, 7: 24, 6: 7, 5: 1, 4: 1}\n",
      "num_batches_remaining : {9: 17, 8: 3, 7: 1, 6: 1, 5: 1, 4: 1}\n",
      "sz : 9\n",
      "indices : 50 [214 398 429 779 732  65 574 444 498 346 339 132 809 356  29 817 370 271\n",
      " 446 297  70 485 298 624 390 675 335 541  44 410 112 253 146 517 745 224\n",
      " 168 644  81 460 494 601 501  23  36 792 546 292 496 699]\n",
      "minibatch_node : torch.Size([50, 9])\n",
      "minibatch_pe : torch.Size([50, 9])\n",
      "minibatch_edge : torch.Size([50, 9, 9])\n",
      "minibatch_boa : torch.Size([50, 9])\n"
     ]
    }
   ],
   "source": [
    "# extract one mini-batch\n",
    "bs = 50\n",
    "sampler = MoleculeSampler(train_group, bs)\n",
    "print('sampler.num_mol :',sampler.num_mol)\n",
    "while(not sampler.is_empty()):\n",
    "    num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "    print('num_batches_remaining :',num_batches_remaining)\n",
    "    sz = sampler.choose_molecule_size()\n",
    "    print('sz :',sz)\n",
    "    indices = sampler.draw_batch_of_molecules(sz)\n",
    "    print('indices :',len(indices),indices)\n",
    "    minibatch_node = torch.stack( [ train_group[sz][i].atom_type for i in indices] )\n",
    "    print('minibatch_node :',minibatch_node.size())\n",
    "    minibatch_pe  = torch.stack( [ train_group[sz][i].atom_type_pe  for i in indices] )\n",
    "    print('minibatch_pe :',minibatch_pe.size())\n",
    "    minibatch_edge = torch.stack( [ train_group[sz][i].bond_type for i in indices] )\n",
    "    print('minibatch_edge :',minibatch_edge.size())\n",
    "    minibatch_boa = torch.stack( [ train_group[sz][i].bag_of_atoms for i in indices] )\n",
    "    print('minibatch_boa :',minibatch_boa.size())\n",
    "    break\n",
    "    print('---------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kIrah_OxVZ4"
   },
   "source": [
    "### Compute valid molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1730638738306,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "fccZnK6xxVZ4",
    "outputId": "09f8135d-d007-4336-ee5a-72c0aa5efad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampler_size.num_mol : {9: 831, 8: 136, 7: 24, 6: 7, 5: 1, 4: 1}\n",
      "sz : 9\n"
     ]
    }
   ],
   "source": [
    "# A class to sample a molecule size w.r.t. the train distribution\n",
    "class sample_molecule_size:\n",
    "    def __init__(self, organized_dataset):\n",
    "        self.num_mol =  { sz: len(list_of_mol)  for sz , list_of_mol in organized_dataset.items() }\n",
    "        self.num_batches_remaining = { sz:  self.num_mol[sz]  for sz in self.num_mol }\n",
    "    def choose_molecule_size(self):\n",
    "        num_batches = self.num_batches_remaining\n",
    "        possible_sizes =  np.array( list( num_batches.keys()) )\n",
    "        prob           =  np.array( list( num_batches.values() )   )\n",
    "        prob =  prob / prob.sum()\n",
    "        sz   = np.random.choice(  possible_sizes , p=prob )\n",
    "        return sz\n",
    "\n",
    "sampler_size = sample_molecule_size(train_group)\n",
    "print('sampler_size.num_mol :',sampler_size.num_mol)\n",
    "sz = sampler_size.choose_molecule_size()\n",
    "print('sz :',sz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj4jYiPZxVZ4"
   },
   "source": [
    "# Exercise 1: Design the class of dense GraphTransformer networks with edge features\n",
    "\n",
    "Node update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=&  h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell}),\\textrm{LN}(e^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell}))  \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h,e)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k,e_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{N\\times N\\times d'}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h,e)_i= \\sum_{j\\in V} \\underbrace{\\frac{\\exp(q_i^T \\textrm{diag}(e_{ij}) k_j/\\sqrt{d'})}{ \\sum_{j'\\in V} \\exp(q_i^T \\textrm{diag}(e_{ij'}) k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score w/ edge feature}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, E=e_k W_E\\in \\mathbb{R}^{E\\times d'=d/H}, W_Q, W_K, W_V, W_E\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Edge update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{e}^{\\ell} &=&  e^{\\ell} + \\textrm{gMHE} (\\textrm{LN}(e^{\\ell}),\\textrm{LN}(h^{\\ell}))  \\in \\mathbb{R}^{N\\times N\\times d}\\\\\n",
    "e^{\\ell+1} &=& \\bar{e}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{e}^{\\ell})) \\in \\mathbb{R}^{N\\times N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHE}(e,h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHE}(e_k,h_k) \\right) W_O^e \\in \\mathbb{R}^{N\\times N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{N\\times N\\times d'}, W_O^e\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\textrm{with } \\textrm{gHE}(e,h)_{ij}=q_i \\odot e_{ij} \\odot k_j/\\sqrt{d'} \\in \\mathbb{R}^{d'} \\textrm{ (point-wise equation)}\\\\\n",
    "e^{\\ell=0} &=& \\textrm{LL}(e_0) \\in \\mathbb{R}^{N\\times N\\times d}\\ \\textrm{(input edge feature)}\\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Differences between sparse GT and dense GT:\n",
    "- The edge features are now dense : Sparse $e\\in \\mathbb{R}^{E\\times d}$ to $e\\in \\mathbb{R}^{N\\times N\\times d}$.\n",
    "- The absence of edges in the graph is now considered as a bond type e.g. `None` type with integer value `0`.\n",
    "- The attention function is now connected to all nodes in the graph : Sparse attention with $\\sum_{j\\in \\mathcal{N}_i}$ to dense attention $\\sum_{j\\in V}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730638738306,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "xY2kMS3wxVZ4",
    "outputId": "09968627-713a-4756-e363-8c8682c3dd8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d, num_heads, num_layers, drop :  128 8 4 0.0\n",
      "num_warmup : 40\n"
     ]
    }
   ],
   "source": [
    "# Global constants\n",
    "num_heads = 8; d = 16*num_heads; num_layers = 4; drop = 0.0; bs = 50\n",
    "print('d, num_heads, num_layers, drop : ', d, num_heads, num_layers, drop)\n",
    "\n",
    "# Warmup\n",
    "num_mol_size = 20\n",
    "num_warmup = 2 * max( num_mol_size, len(train) // bs ) # 4 epochs * max( num_mol_size=20, num_mol/batch_size)\n",
    "print('num_warmup :',num_warmup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QVFNOMstF_H"
   },
   "source": [
    "### Implement the Graph Transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "g_Zx9Ohs9rOl"
   },
   "outputs": [],
   "source": [
    "# Define Graph Transformer architecture\n",
    "class head_attention(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d, d_head)\n",
    "        self.K = nn.Linear(d, d_head)\n",
    "        self.E = nn.Linear(d, d_head)\n",
    "        self.V = nn.Linear(d, d_head)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(d_head))\n",
    "        self.drop_att = nn.Dropout(drop)\n",
    "        self.Ni = nn.Linear(d, d_head)\n",
    "        self.Nj = nn.Linear(d, d_head)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        Q = self.Q(x) # [bs, n, d_head]\n",
    "        K = self.K(x) # [bs, n, d_head]\n",
    "        V = self.V(x) # [bs, n, d_head]\n",
    "        Q = Q.unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        K = K.unsqueeze(1) # [bs, 1, n, d_head]\n",
    "        E = self.E(e) # [bs, n, n, d_head]\n",
    "        Ni = self.Ni(x).unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        Nj = self.Nj(x).unsqueeze(1) # [bs, 1, n, d_head]\n",
    "        e = Ni + Nj + E\n",
    "        Att = (Q * e * K).sum(dim=3) / self.sqrt_d # [bs, n, n]\n",
    "        Att = torch.softmax(Att, dim=1) # [bs, n, n]\n",
    "        Att = self.drop_att(Att)\n",
    "        x = Att @ V  # [bs, n, d_head]\n",
    "        return x, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omwQ4KHoz01e"
   },
   "source": [
    "### Question 1.1: Implement a **dense** Graph Multi-Head Attention (MHA) Layer with Pytorch\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Step 1:* Pass node feature and edge features along edges.\n",
    "\n",
    "- *Step 2:* Update node feature and edge features. You may use ```torch.cat()``` for concatenating vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "rVOm71w09uVv"
   },
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d, num_heads):\n",
    "        super().__init__()\n",
    "        d_head = d // num_heads\n",
    "        self.heads = nn.ModuleList( [head_attention(d, d_head) for _ in range(num_heads)] )\n",
    "        self.WOx = nn.Linear(d, d)\n",
    "        self.WOe = nn.Linear(d, d)\n",
    "        self.drop_x = nn.Dropout(drop)\n",
    "        self.drop_e = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        x_MHA = []\n",
    "        e_MHA = []\n",
    "        for head in self.heads:\n",
    "            ###############################################\n",
    "            # YOUR CODE STARTS\n",
    "            ###############################################\n",
    "            # Step 1: Pass node feature and edge features along edges.\n",
    "            x_HA, e_HA = head(x, e)\n",
    "            ###############################################\n",
    "            # YOUR CODE ENDS\n",
    "            ###############################################\n",
    "\n",
    "            x_MHA.append(x_HA)\n",
    "            e_MHA.append(e_HA)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Step 2: Update node features\n",
    "        x = self.WOx(torch.cat(x_MHA, dim=2)) # [bs, n, d]\n",
    "        x = self.drop_x(x)\n",
    "\n",
    "        # Step 2: Update edge features\n",
    "        e = self.WOe(torch.cat(e_MHA, dim=3)) # [bs, n, n, d]\n",
    "        e = self.drop_e(e)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return x, e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrDJPufg0bxe"
   },
   "source": [
    "### Question 1.2: Implement a **dense** GraphTransformer layer\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Layer normalization:* Intialize two layer normalization ```nn.LayerNorm(input_dim)``` for  edge features.\n",
    "\n",
    "- *Input embedding layer:* Initialize a linear layer ```nn.Linear()``` to convert input edge features into edge embeddings.\n",
    "\n",
    "- *Graph transformer layer:* Initialize a graph Transformer layer using the defined ```MHA()``` class.\n",
    "\n",
    "- *MLP layer:* Initialize a MLP layer ```nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, output_dim))``` for edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjPRzP9A91qg"
   },
   "outputs": [],
   "source": [
    "class BlockGT(nn.Module):\n",
    "    def __init__(self, d, num_heads):\n",
    "        super().__init__()\n",
    "        # Intialize two layer normalization for node features\n",
    "        self.LNx = nn.LayerNorm(d)\n",
    "        self.LNx2 = nn.LayerNorm(d)\n",
    "\n",
    "        # Intialize a MLP layer for node features\n",
    "        self.MLPx = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Linear(4*d, d))\n",
    "\n",
    "        # Intialize the graph Transformer layer\n",
    "        self.MHA = MHA(d, num_heads)\n",
    "\n",
    "        # Intialize a dropout layer for node features\n",
    "        self.drop_x_mlp = nn.Dropout(drop)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Intialize two layer normalization for edge features\n",
    "        self.LNe =  nn.LayerNorm(d)\n",
    "        self.LNe2 =  nn.LayerNorm(d)\n",
    "\n",
    "        # Intialize a MLP layer for edge features\n",
    "        self.MLPe = nn.Sequential(\n",
    "            nn.Linear(d, 4 * d),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * d, d)\n",
    "        )\n",
    "\n",
    "        # Intialize a dropout layer for edge features\n",
    "        self.drop_e_mlp = nn.Dropout(drop)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        # Implement layer normalization for node and edge features, respectively\n",
    "        x = self.LNx(x)\n",
    "        e = self.LNe(e)\n",
    "\n",
    "        # Implement graph Transformer\n",
    "        x_MHA, e_MHA = self.MHA(x, e) # [bs, n, d], [bs, n, n, d]\n",
    "\n",
    "        # Implement residual connection layers for node features\n",
    "        x = x + x_MHA # [bs, n, d]\n",
    "        x = x + self.MLPx(self.LNx2(x)) # [bs, n, d]\n",
    "\n",
    "        x = self.drop_x_mlp(x)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Implement residual connection and dropout for edge features like the above operators for node features\n",
    "        e = e + e_MHA\n",
    "        e = e + self.MLPe(self.LNe2(e))\n",
    "        e = self.drop_e_mlp(e) # [bs, n, n, d]\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return x, e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1730638738306,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "n7GwPHMx9-J2",
    "outputId": "692e68c6-31f5-4d67-a031-0e3174fb5f3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First run\n"
     ]
    }
   ],
   "source": [
    "def sym_tensor(x):\n",
    "    x = x.permute(0,3,1,2)\n",
    "    triu = torch.triu(x,diagonal=1).transpose(3,2)\n",
    "    mask = (triu.abs()>0).float()\n",
    "    x =  x * (1 - mask ) + mask * triu\n",
    "    x = x.permute(0,2,3,1)\n",
    "    return x\n",
    "\n",
    "class GT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.atom_emb = nn.Embedding(num_atom_type, d)\n",
    "        self.bond_emb = nn.Embedding(num_bond_type, d)\n",
    "        num_layers_encoder = 4\n",
    "        self.BlockGT_encoder_layers = nn.ModuleList( [BlockGT(d, num_heads) for _ in range(num_layers_encoder)] )\n",
    "        self.ln_x_final = nn.LayerNorm(d)\n",
    "        self.linear_x_final = nn.Linear(d, 1, bias=True)\n",
    "        self.drop_x_emb = nn.Dropout(drop)\n",
    "        self.drop_e_emb = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "\n",
    "        # input layer\n",
    "        x = self.atom_emb(x)                   # [bs, n, d]\n",
    "        e = self.bond_emb(e)                   # [bs, n, n, d]\n",
    "        e = sym_tensor(e)                      # [bs, n, n, d]\n",
    "        x = self.drop_x_emb(x)\n",
    "        e = self.drop_e_emb(e)\n",
    "\n",
    "        # encoder\n",
    "        for gt_layer in self.BlockGT_encoder_layers:\n",
    "            x, e = gt_layer(x, e)  # [bs, n, d],  [bs, n, n, d]\n",
    "            e = sym_tensor(e)\n",
    "\n",
    "        # class token\n",
    "        mol_token = x.mean(1) # [bs, d]\n",
    "\n",
    "        # regressor\n",
    "        x = self.ln_x_final(mol_token)\n",
    "        x = self.linear_x_final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "try:\n",
    "    del net\n",
    "except:\n",
    "  print(\"First run\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KZMCbP40ySu"
   },
   "source": [
    "### Instantiate a **dense** graph Transformer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1730638738856,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "N8hNaugJ-BP6",
    "outputId": "5ac56198-3433-4297-c73c-028766df5269"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1588225 (1.59 million)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the network\n",
    "net = GT()\n",
    "net = net.to(device)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMcqJOpk0wHd"
   },
   "source": [
    "### Test the forward pass, backward pass and gradient update with a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14235,
     "status": "ok",
     "timestamp": 1730638753090,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "WDCEa4_bxVZ4",
    "outputId": "4c397748-53cb-45c6-bc28-e884b952d626"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampler.num_mol : {9: 831, 8: 136, 7: 24, 6: 7, 5: 1, 4: 1}\n",
      "num_batches_remaining : {9: 17, 8: 3, 7: 1, 6: 1, 5: 1, 4: 1}\n",
      "sz : 8\n",
      "indices : 50 [ 75  17  71  55 114  34  91 126  46  19  42  79  74 123  31 118  47  10\n",
      "  72  92  59  41  16  62  54  13 117  80  89  95 109  32  44 112  30  24\n",
      "  15  85  39  96  37  63   2  70  58  76  93  98 111  64]\n",
      "minibatch_node : torch.Size([50, 8])\n",
      "minibatch_edge : torch.Size([50, 8, 8])\n",
      "batch_target : torch.Size([50, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (8) must match the size of tensor b (50) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m batch_target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack( [ train_group[sz][i]\u001b[38;5;241m.\u001b[39mlogP_SA_cycle_normalized \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices] )\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# [bs, 1]\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_target :\u001b[39m\u001b[38;5;124m'\u001b[39m,batch_target\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m---> 22\u001b[0m batch_x_pred \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_e0\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# [bs, 1]\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_x_pred\u001b[39m\u001b[38;5;124m'\u001b[39m,batch_x_pred\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mL1Loss()(batch_x_pred, batch_target)\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\gnn_course\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\gnn_course\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 31\u001b[0m, in \u001b[0;36mGT.forward\u001b[1;34m(self, x, e)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# encoder\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m gt_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBlockGT_encoder_layers:\n\u001b[1;32m---> 31\u001b[0m     x, e \u001b[38;5;241m=\u001b[39m \u001b[43mgt_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [bs, n, d],  [bs, n, n, d]\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     e \u001b[38;5;241m=\u001b[39m sym_tensor(e)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# class token\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\gnn_course\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\envs\\gnn_course\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 56\u001b[0m, in \u001b[0;36mBlockGT.forward\u001b[1;34m(self, x, e)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m###############################################\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# YOUR CODE STARTS\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m###############################################\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Implement residual connection and dropout for edge features like the above operators for node features\u001b[39;00m\n\u001b[0;32m     55\u001b[0m e \u001b[38;5;241m=\u001b[39m e \u001b[38;5;241m+\u001b[39m e_MHA\n\u001b[1;32m---> 56\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMLPe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLNe2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_e_mlp(e) \u001b[38;5;66;03m# [bs, n, n, d]\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m###############################################\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# YOUR CODE ENDS\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m###############################################\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (8) must match the size of tensor b (50) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Test the forward pass, backward pass and gradient update with a single batch\n",
    "init_lr = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, verbose=True)\n",
    "\n",
    "bs = 50\n",
    "sampler = MoleculeSampler(train_group, bs)\n",
    "print('sampler.num_mol :',sampler.num_mol)\n",
    "num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "print('num_batches_remaining :',num_batches_remaining)\n",
    "sz = sampler.choose_molecule_size()\n",
    "print('sz :',sz)\n",
    "indices = sampler.draw_batch_of_molecules(sz)\n",
    "print('indices :',len(indices),indices)\n",
    "batch_x0 = minibatch_node = torch.stack( [ train_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "print('minibatch_node :',minibatch_node.size())\n",
    "batch_e0 = minibatch_edge = torch.stack( [ train_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "print('minibatch_edge :',minibatch_edge.size())\n",
    "batch_target = torch.stack( [ train_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "print('batch_target :',batch_target.size())\n",
    "\n",
    "batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "print('batch_x_pred',batch_x_pred.size())\n",
    "\n",
    "loss = nn.L1Loss()(batch_x_pred, batch_target)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4K_C4Fl06_o"
   },
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1363189,
     "status": "ok",
     "timestamp": 1730640116276,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "B7pTUYKmxVZ4",
    "outputId": "aa7cf91b-c7ce-4653-a1da-7d2e01aaa3c8"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "del net\n",
    "net = GT()\n",
    "net = net.to(device)\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# Optimizer\n",
    "init_lr = 0.0003\n",
    "init_lr = 0.0001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: min((t+1)/num_warmup, 1.0) ) # warmup scheduler\n",
    "scheduler_tracker = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, verbose=True) # tracker scheduler\n",
    "\n",
    "num_warmup_batch = 0\n",
    "\n",
    "# Number of mini-batches per epoch\n",
    "nb_epochs = 100\n",
    "\n",
    "lossMAE = nn.L1Loss()\n",
    "\n",
    "print('num batch(before scheduler_tracker), num epoch(before scheduler_tracker), num_warmup_batch(current):', \\\n",
    "      num_warmup, num_warmup//(len(train)//bs), num_warmup_batch)\n",
    "\n",
    "total_loss = moving_loss = -1\n",
    "list_loss = []\n",
    "start=time.time()\n",
    "for epoch in range(nb_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    num_data = 0\n",
    "    net.train()\n",
    "\n",
    "    bs = 50\n",
    "    sampler = MoleculeSampler(train_group, bs)\n",
    "    #print('sampler.num_mol :',sampler.num_mol)\n",
    "    while(not sampler.is_empty()):\n",
    "        num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "        #print('num_batches_remaining :',num_batches_remaining)\n",
    "        sz = sampler.choose_molecule_size()\n",
    "        #print('sz :',sz)\n",
    "        indices = sampler.draw_batch_of_molecules(sz)\n",
    "        bs2 = len(indices)\n",
    "        #print('indices :',len(indices),indices)\n",
    "        batch_x0 = minibatch_node = torch.stack( [ train_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "        #print('minibatch_node :',minibatch_node.size())\n",
    "        batch_e0 = minibatch_edge = torch.stack( [ train_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "        #print('minibatch_edge :',minibatch_edge.size())\n",
    "        batch_target = torch.stack( [ train_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "\n",
    "        batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "\n",
    "        loss = lossMAE(batch_x_pred, batch_target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if num_warmup_batch < num_warmup:\n",
    "            scheduler_warmup.step() # warmup scheduler\n",
    "        num_warmup_batch += 1\n",
    "\n",
    "        # COMPUTE STATS\n",
    "        running_loss += bs2 * loss.detach().item()\n",
    "        num_batches += 1\n",
    "        num_data += bs2\n",
    "\n",
    "\n",
    "    # TEST SET\n",
    "    bs = 50\n",
    "    sampler = MoleculeSampler(test_group, bs)\n",
    "    running_test_loss = 0\n",
    "    num_test_data = 0\n",
    "    with torch.no_grad():\n",
    "        while(not sampler.is_empty()):\n",
    "            num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "            sz = sampler.choose_molecule_size()\n",
    "            indices = sampler.draw_batch_of_molecules(sz)\n",
    "            bs2 = len(indices)\n",
    "            batch_x0 = minibatch_node = torch.stack( [ test_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "            batch_e0 = minibatch_edge = torch.stack( [ test_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "            batch_target = torch.stack( [ test_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "            batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "            running_test_loss += bs2 * lossMAE(batch_x_pred, batch_target).detach().item()\n",
    "            num_test_data += bs2\n",
    "\n",
    "\n",
    "    # AVERAGE STATS THEN DISPLAY\n",
    "    mean_train_loss = running_loss/num_data\n",
    "    mean_test_loss = running_test_loss/num_test_data\n",
    "    if num_warmup_batch >= num_warmup:\n",
    "        scheduler_tracker.step(mean_train_loss) # tracker scheduler defined w.r.t. loss value\n",
    "        num_warmup_batch += 1\n",
    "    elapsed = (time.time()-start)/60\n",
    "    if not epoch%1:\n",
    "        line = 'epoch= ' + str(epoch) + '\\t time= ' + str(elapsed)[:6] + ' min' + '\\t lr= ' + \\\n",
    "        '{:.7f}'.format(optimizer.param_groups[0]['lr']) + '\\t train_loss= ' + str(mean_train_loss)[:6] + \\\n",
    "        '\\t test_loss= ' + str(mean_test_loss)[:6]\n",
    "        print(line)\n",
    "\n",
    "    # Check lr value\n",
    "    if optimizer.param_groups[0]['lr'] < 10**-6: # 2*10**-4: quick, # 10**-6: slow\n",
    "      print(\"\\n lr is equal to min lr -- training stopped\\n\")\n",
    "      break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27B-nWpxxVZ5"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgoipxpwxVZ5"
   },
   "source": [
    "## Compare results\n",
    "\n",
    "| GNN    | train MAE | test MAE |\n",
    "| -------- | ------- | ------- |\n",
    "| Sparse GT (DGL)   | 0.4483    | 0.7327    |\n",
    "| Dense GT (PyTroch)    |     |     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cT0lRlYMxVZ5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_OCOsMTxVZ5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gnn_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
