{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6JtO7-nxVZz"
   },
   "source": [
    "# Lecture : Graph Transformers & Graph ViT\n",
    "\n",
    "## Lab 04 : Graph Transformers with edge features and PyTorch (dense linear algebra) -- Exercise\n",
    "\n",
    "### Xavier Bresson, Guoji Fu\n",
    "\n",
    "Dwivedi, Bresson, A generalization of transformer networks to graphs, 2020   \n",
    "https://arxiv.org/pdf/2012.09699.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10358,
     "status": "ok",
     "timestamp": 1730638731947,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "SlHbOw3NxVZ0",
    "outputId": "a1054b9d-6b96-4525-f856-7404ac72733c"
   },
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/10_Graph_Transformers'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl==1.0.0 # Install DGL\n",
    "    !pip install rdkit==2023.09.6 # Install RDKit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1666,
     "status": "ok",
     "timestamp": 1730638733609,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "t4gOKvLQxVZ1",
    "outputId": "811e75af-6f4e-47e2-be7e-b3ffe5d8f54d"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import networkx as nx\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_ncut\n",
    "from lib.molecules import Dictionary, MoleculeDataset, MoleculeDGL, Molecule\n",
    "import os, datetime\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import rdmolops\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1730638733610,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "nsdrLo3pxVZ1",
    "outputId": "2da8f161-3ea8-4c4d-fa9f-f5e331d22cc2"
   },
   "outputs": [],
   "source": [
    "# PyTorch version and GPU\n",
    "print(torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "  print(torch.cuda.get_device_name(0))\n",
    "  device= torch.device(\"cuda\") # use GPU\n",
    "else:\n",
    "  device= torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4-MeJfhxVZ2"
   },
   "source": [
    "# Load molecular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 4699,
     "status": "ok",
     "timestamp": 1730638738305,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "Jl7rS9Z6xVZ2",
    "outputId": "f6b04892-0fba-4c88-88f0-aed5d6add3ab"
   },
   "outputs": [],
   "source": [
    "print('Loading data...')\n",
    "start = time.time()\n",
    "\n",
    "data_folder_pytorch = 'dataset/QM9_1.4k_pytorch/'\n",
    "print(data_folder_pytorch)\n",
    "\n",
    "with open(data_folder_pytorch+\"atom_dict.pkl\",\"rb\") as f:\n",
    "    atom_dict=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"bond_dict.pkl\",\"rb\") as f:\n",
    "    bond_dict=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"test_pytorch.pkl\",\"rb\") as f:\n",
    "    test=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"val_pytorch.pkl\",\"rb\") as f:\n",
    "    val=pickle.load(f)\n",
    "with open(data_folder_pytorch+\"train_pytorch.pkl\",\"rb\") as f:\n",
    "    train=pickle.load(f)\n",
    "print('Time:',time.time()-start)\n",
    "\n",
    "print('num train data :',len(train))\n",
    "\n",
    "print('atom_dict.idx2word :',atom_dict.idx2word)\n",
    "print('atom_dict.word2idx :',atom_dict.word2idx)\n",
    "print('bond_dict.idx2word :',bond_dict.idx2word)\n",
    "print('bond_dict.word2idx :',bond_dict.word2idx)\n",
    "\n",
    "num_atom_type = len(atom_dict.idx2word)\n",
    "num_bond_type = len(bond_dict.idx2word)\n",
    "print(num_atom_type, num_bond_type)\n",
    "\n",
    "idx = 45\n",
    "print('train[idx].atom_type :',train[idx].atom_type)\n",
    "print('train[idx].atom_type_pe :',train[idx].atom_type_pe)\n",
    "print('train[idx].bond_type :',train[idx].bond_type)\n",
    "print('train[idx].bag_of_atoms :',train[idx].bag_of_atoms)\n",
    "print('train[idx].smile: ',train[idx].smile)\n",
    "mol = Chem.MolFromSmiles(train[idx].smile)\n",
    "mol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9HcvHgMxVZ2"
   },
   "source": [
    "# Print dataset statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1730638738306,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "FXjnSa2vxVZ2",
    "outputId": "f8280897-c066-44ef-cbc6-25cf527819c8"
   },
   "outputs": [],
   "source": [
    "# Organize data into group of of molecules of fixed sized\n",
    "# Example: train[22] is a list containing all the molecules of size 22\n",
    "def group_molecules_per_size(dataset):\n",
    "    mydict={}\n",
    "    for mol in dataset:\n",
    "        if len(mol) not in mydict:\n",
    "            mydict[len(mol)]=[]\n",
    "        mydict[len(mol)].append(mol)\n",
    "    return mydict\n",
    "test_group  = group_molecules_per_size(test)\n",
    "val_group   = group_molecules_per_size(val)\n",
    "train_group = group_molecules_per_size(train)\n",
    "print(len(train_group[8])) # QM9\n",
    "# print(len(train_group[28])) # ZINC\n",
    "\n",
    "# what is the biggest molecule in the train set\n",
    "max_mol_sz= max(list( train_group.keys()))\n",
    "print('Max num atoms = ', max_mol_sz)\n",
    "\n",
    "# print distribution w.r.t. molecule size\n",
    "def print_distribution(data):\n",
    "    for nb_atom in range(max_mol_sz+1):\n",
    "        try:\n",
    "            print('number of molecule of size {}: \\t {}'.format(nb_atom, len(data[nb_atom])))\n",
    "        except:\n",
    "            pass\n",
    "print('Train'); print_distribution(train_group)\n",
    "print('Val'); print_distribution(val_group)\n",
    "print('Test'); print_distribution(test_group)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uhrS0_D7xVZ3"
   },
   "source": [
    "# Generate batch of pytorch molecules of same size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsAGj6cB8i0A"
   },
   "source": [
    "### Implement the molecule sampler class for batch sampling of molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJLO8Iby8ZkO"
   },
   "outputs": [],
   "source": [
    "# A class to help drawing batches of molecules having the same size\n",
    "class MoleculeSampler:\n",
    "    def __init__(self, organized_dataset, bs , shuffle=True):\n",
    "        self.bs = bs\n",
    "        self.num_mol =  { sz: len(list_of_mol)  for sz , list_of_mol in organized_dataset.items() }\n",
    "        self.counter = { sz: 0   for sz in organized_dataset }\n",
    "        if shuffle:\n",
    "            self.order = { sz: np.random.permutation(num)  for sz , num in self.num_mol.items() }\n",
    "        else:\n",
    "            self.order = { sz: np.arange(num)  for sz , num in self.num_mol.items() }\n",
    "\n",
    "    def compute_num_batches_remaining(self):\n",
    "        #return {sz:  ( self.num_mol[sz] - self.counter[sz] ) // self.bs  for sz in self.num_mol}\n",
    "        return {sz:  math.ceil(((self.num_mol[sz] - self.counter[sz])/self.bs))  for sz in self.num_mol}\n",
    "\n",
    "    def choose_molecule_size(self):\n",
    "        num_batches = self.compute_num_batches_remaining()\n",
    "        possible_sizes =  np.array( list( num_batches.keys()) )\n",
    "        prob           =  np.array( list( num_batches.values() )   )\n",
    "        prob =  prob / prob.sum()\n",
    "        sz   = np.random.choice(  possible_sizes , p=prob )\n",
    "        return sz\n",
    "\n",
    "    def is_empty(self):\n",
    "        num_batches= self.compute_num_batches_remaining()\n",
    "        return sum( num_batches.values() ) == 0\n",
    "\n",
    "    def draw_batch_of_molecules(self, sz):\n",
    "        num_batches = self.compute_num_batches_remaining()\n",
    "        if (self.num_mol[sz] - self.counter[sz])/self.bs >= 1.0:\n",
    "            bs = self.bs\n",
    "        else:\n",
    "            bs = self.num_mol[sz] - (self.num_mol[sz]//self.bs) * self.bs\n",
    "        #print('sz, bs',sz, bs)\n",
    "        indices = self.order[sz][ self.counter[sz] : self.counter[sz] + bs]\n",
    "        self.counter[sz] += bs\n",
    "        return indices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1Xnq_v29VC6"
   },
   "source": [
    "### Extract one mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1730638738306,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "uHjZ9bskxVZ3",
    "outputId": "da30f87b-e3b4-4b5f-8493-2f5ea37470c8"
   },
   "outputs": [],
   "source": [
    "# extract one mini-batch\n",
    "bs = 50\n",
    "sampler = MoleculeSampler(train_group, bs)\n",
    "print('sampler.num_mol :',sampler.num_mol)\n",
    "while(not sampler.is_empty()):\n",
    "    num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "    print('num_batches_remaining :',num_batches_remaining)\n",
    "    sz = sampler.choose_molecule_size()\n",
    "    print('sz :',sz)\n",
    "    indices = sampler.draw_batch_of_molecules(sz)\n",
    "    print('indices :',len(indices),indices)\n",
    "    minibatch_node = torch.stack( [ train_group[sz][i].atom_type for i in indices] )\n",
    "    print('minibatch_node :',minibatch_node.size())\n",
    "    minibatch_pe  = torch.stack( [ train_group[sz][i].atom_type_pe  for i in indices] )\n",
    "    print('minibatch_pe :',minibatch_pe.size())\n",
    "    minibatch_edge = torch.stack( [ train_group[sz][i].bond_type for i in indices] )\n",
    "    print('minibatch_edge :',minibatch_edge.size())\n",
    "    minibatch_boa = torch.stack( [ train_group[sz][i].bag_of_atoms for i in indices] )\n",
    "    print('minibatch_boa :',minibatch_boa.size())\n",
    "    break\n",
    "    print('---------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kIrah_OxVZ4"
   },
   "source": [
    "### Compute valid molecules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1730638738306,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "fccZnK6xxVZ4",
    "outputId": "09f8135d-d007-4336-ee5a-72c0aa5efad6"
   },
   "outputs": [],
   "source": [
    "# A class to sample a molecule size w.r.t. the train distribution\n",
    "class sample_molecule_size:\n",
    "    def __init__(self, organized_dataset):\n",
    "        self.num_mol =  { sz: len(list_of_mol)  for sz , list_of_mol in organized_dataset.items() }\n",
    "        self.num_batches_remaining = { sz:  self.num_mol[sz]  for sz in self.num_mol }\n",
    "    def choose_molecule_size(self):\n",
    "        num_batches = self.num_batches_remaining\n",
    "        possible_sizes =  np.array( list( num_batches.keys()) )\n",
    "        prob           =  np.array( list( num_batches.values() )   )\n",
    "        prob =  prob / prob.sum()\n",
    "        sz   = np.random.choice(  possible_sizes , p=prob )\n",
    "        return sz\n",
    "\n",
    "sampler_size = sample_molecule_size(train_group)\n",
    "print('sampler_size.num_mol :',sampler_size.num_mol)\n",
    "sz = sampler_size.choose_molecule_size()\n",
    "print('sz :',sz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zj4jYiPZxVZ4"
   },
   "source": [
    "# Exercise 1: Design the class of dense GraphTransformer networks with edge features\n",
    "\n",
    "Node update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=&  h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell}),\\textrm{LN}(e^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell}))  \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h,e)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k,e_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{N\\times N\\times d'}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h,e)_i= \\sum_{j\\in V} \\underbrace{\\frac{\\exp(q_i^T \\textrm{diag}(e_{ij}) k_j/\\sqrt{d'})}{ \\sum_{j'\\in V} \\exp(q_i^T \\textrm{diag}(e_{ij'}) k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score w/ edge feature}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, E=e_k W_E\\in \\mathbb{R}^{E\\times d'=d/H}, W_Q, W_K, W_V, W_E\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Edge update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{e}^{\\ell} &=&  e^{\\ell} + \\textrm{gMHE} (\\textrm{LN}(e^{\\ell}),\\textrm{LN}(h^{\\ell}))  \\in \\mathbb{R}^{N\\times N\\times d}\\\\\n",
    "e^{\\ell+1} &=& \\bar{e}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{e}^{\\ell})) \\in \\mathbb{R}^{N\\times N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHE}(e,h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHE}(e_k,h_k) \\right) W_O^e \\in \\mathbb{R}^{N\\times N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{N\\times N\\times d'}, W_O^e\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\textrm{with } \\textrm{gHE}(e,h)_{ij}=q_i \\odot e_{ij} \\odot k_j/\\sqrt{d'} \\in \\mathbb{R}^{d'} \\textrm{ (point-wise equation)}\\\\\n",
    "e^{\\ell=0} &=& \\textrm{LL}(e_0) \\in \\mathbb{R}^{N\\times N\\times d}\\ \\textrm{(input edge feature)}\\\\\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Differences between sparse GT and dense GT:\n",
    "- The edge features are now dense : Sparse $e\\in \\mathbb{R}^{E\\times d}$ to $e\\in \\mathbb{R}^{N\\times N\\times d}$.\n",
    "- The absence of edges in the graph is now considered as a bond type e.g. `None` type with integer value `0`.\n",
    "- The attention function is now connected to all nodes in the graph : Sparse attention with $\\sum_{j\\in \\mathcal{N}_i}$ to dense attention $\\sum_{j\\in V}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730638738306,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "xY2kMS3wxVZ4",
    "outputId": "09968627-713a-4756-e363-8c8682c3dd8b"
   },
   "outputs": [],
   "source": [
    "# Global constants\n",
    "num_heads = 8; d = 16*num_heads; num_layers = 4; drop = 0.0; bs = 50\n",
    "print('d, num_heads, num_layers, drop : ', d, num_heads, num_layers, drop)\n",
    "\n",
    "# Warmup\n",
    "num_mol_size = 20\n",
    "num_warmup = 2 * max( num_mol_size, len(train) // bs ) # 4 epochs * max( num_mol_size=20, num_mol/batch_size)\n",
    "print('num_warmup :',num_warmup)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QVFNOMstF_H"
   },
   "source": [
    "### Implement the Graph Transformer architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_Zx9Ohs9rOl"
   },
   "outputs": [],
   "source": [
    "# Define Graph Transformer architecture\n",
    "class head_attention(nn.Module):\n",
    "    def __init__(self, d, d_head):\n",
    "        super().__init__()\n",
    "        self.Q = nn.Linear(d, d_head)\n",
    "        self.K = nn.Linear(d, d_head)\n",
    "        self.E = nn.Linear(d, d_head)\n",
    "        self.V = nn.Linear(d, d_head)\n",
    "        self.sqrt_d = torch.sqrt(torch.tensor(d_head))\n",
    "        self.drop_att = nn.Dropout(drop)\n",
    "        self.Ni = nn.Linear(d, d_head)\n",
    "        self.Nj = nn.Linear(d, d_head)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        Q = self.Q(x) # [bs, n, d_head]\n",
    "        K = self.K(x) # [bs, n, d_head]\n",
    "        V = self.V(x) # [bs, n, d_head]\n",
    "        Q = Q.unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        K = K.unsqueeze(1) # [bs, 1, n, d_head]\n",
    "        E = self.E(e) # [bs, n, n, d_head]\n",
    "        Ni = self.Ni(x).unsqueeze(2) # [bs, n, 1, d_head]\n",
    "        Nj = self.Nj(x).unsqueeze(1) # [bs, 1, n, d_head]\n",
    "        e = Ni + Nj + E\n",
    "        Att = (Q * e * K).sum(dim=3) / self.sqrt_d # [bs, n, n]\n",
    "        Att = torch.softmax(Att, dim=1) # [bs, n, n]\n",
    "        Att = self.drop_att(Att)\n",
    "        x = Att @ V  # [bs, n, d_head]\n",
    "        return x, e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omwQ4KHoz01e"
   },
   "source": [
    "### Question 1.1: Implement a **dense** Graph Multi-Head Attention (MHA) Layer with Pytorch\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Step 1:* Pass node feature and edge features along edges.\n",
    "\n",
    "- *Step 2:* Update node feature and edge features. You may use ```torch.cat()``` for concatenating vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVOm71w09uVv"
   },
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d, num_heads):\n",
    "        super().__init__()\n",
    "        d_head = d // num_heads\n",
    "        self.heads = nn.ModuleList( [head_attention(d, d_head) for _ in range(num_heads)] )\n",
    "        self.WOx = nn.Linear(d, d)\n",
    "        self.WOe = nn.Linear(d, d)\n",
    "        self.drop_x = nn.Dropout(drop)\n",
    "        self.drop_e = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        x_MHA = []\n",
    "        e_MHA = []\n",
    "        for head in self.heads:\n",
    "            ###############################################\n",
    "            # YOUR CODE STARTS\n",
    "            ###############################################\n",
    "            # Step 1: Pass node feature and edge features along edges.\n",
    "            x_HA, e_HA = \n",
    "            ###############################################\n",
    "            # YOUR CODE ENDS\n",
    "            ###############################################\n",
    "\n",
    "            x_MHA.append(x_HA)\n",
    "            e_MHA.append(e_HA)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Step 2: Update node features\n",
    "        x =  # [bs, n, d]\n",
    "        x = self.drop_x(x)\n",
    "\n",
    "        # Step 2: Update edge features\n",
    "        e =  # [bs, n, n, d]\n",
    "        e = self.drop_e(e)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return x, e\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrDJPufg0bxe"
   },
   "source": [
    "### Question 1.2: Implement a **dense** GraphTransformer layer\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Layer normalization:* Intialize two layer normalization ```nn.LayerNorm(input_dim)``` for  edge features.\n",
    "\n",
    "- *Input embedding layer:* Initialize a linear layer ```nn.Linear()``` to convert input edge features into edge embeddings.\n",
    "\n",
    "- *Graph transformer layer:* Initialize a graph Transformer layer using the defined ```MHA()``` class.\n",
    "\n",
    "- *MLP layer:* Initialize a MLP layer ```nn.Sequential(nn.Linear(input_dim, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, output_dim))``` for edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjPRzP9A91qg"
   },
   "outputs": [],
   "source": [
    "class BlockGT(nn.Module):\n",
    "    def __init__(self, d, num_heads):\n",
    "        super().__init__()\n",
    "        # Intialize two layer normalization for node features\n",
    "        self.LNx = nn.LayerNorm(d)\n",
    "        self.LNx2 = nn.LayerNorm(d)\n",
    "\n",
    "        # Intialize a MLP layer for node features\n",
    "        self.MLPx = nn.Sequential(nn.Linear(d, 4*d), nn.ReLU(), nn.Linear(4*d, d))\n",
    "\n",
    "        # Intialize the graph Transformer layer\n",
    "        self.MHA = MHA(d, num_heads)\n",
    "\n",
    "        # Intialize a dropout layer for node features\n",
    "        self.drop_x_mlp = nn.Dropout(drop)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Intialize two layer normalization for edge features\n",
    "        self.LNe =  \n",
    "        self.LNe2 =  \n",
    "\n",
    "        # Intialize a MLP layer for edge features\n",
    "        self.MLPe =  \n",
    "\n",
    "        # Intialize a dropout layer for edge features\n",
    "        self.drop_x_mlp =  \n",
    "        self.drop_e_mlp =  \n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "    def forward(self, x, e):\n",
    "        # Implement layer normalization for node and edge features, respectively\n",
    "        x = self.LNx(x)\n",
    "        e = self.LNe(e)\n",
    "\n",
    "        # Implement graph Transformer\n",
    "        x_MHA, e_MHA = self.MHA(x, e) # [bs, n, d], [bs, n, n, d]\n",
    "\n",
    "        # Implement residual connection layers for node features\n",
    "        x = x + x_MHA # [bs, n, d]\n",
    "        x = x + self.MLPx(self.LNx2(x)) # [bs, n, d]\n",
    "\n",
    "        x = self.drop_x_mlp(x)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Implement residual connection and dropout for edge features like the above operators for node features\n",
    "        e =  # [bs, n, n, d]\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return x, e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1730638738306,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "n7GwPHMx9-J2",
    "outputId": "692e68c6-31f5-4d67-a031-0e3174fb5f3f"
   },
   "outputs": [],
   "source": [
    "def sym_tensor(x):\n",
    "    x = x.permute(0,3,1,2)\n",
    "    triu = torch.triu(x,diagonal=1).transpose(3,2)\n",
    "    mask = (triu.abs()>0).float()\n",
    "    x =  x * (1 - mask ) + mask * triu\n",
    "    x = x.permute(0,2,3,1)\n",
    "    return x\n",
    "\n",
    "class GT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.atom_emb = nn.Embedding(num_atom_type, d)\n",
    "        self.bond_emb = nn.Embedding(num_bond_type, d)\n",
    "        num_layers_encoder = 4\n",
    "        self.BlockGT_encoder_layers = nn.ModuleList( [BlockGT(d, num_heads) for _ in range(num_layers_encoder)] )\n",
    "        self.ln_x_final = nn.LayerNorm(d)\n",
    "        self.linear_x_final = nn.Linear(d, 1, bias=True)\n",
    "        self.drop_x_emb = nn.Dropout(drop)\n",
    "        self.drop_e_emb = nn.Dropout(drop)\n",
    "    def forward(self, x, e):\n",
    "\n",
    "        # input layer\n",
    "        x = self.atom_emb(x)                   # [bs, n, d]\n",
    "        e = self.bond_emb(e)                   # [bs, n, n, d]\n",
    "        e = sym_tensor(e)                      # [bs, n, n, d]\n",
    "        x = self.drop_x_emb(x)\n",
    "        e = self.drop_e_emb(e)\n",
    "\n",
    "        # encoder\n",
    "        for gt_layer in self.BlockGT_encoder_layers:\n",
    "            x, e = gt_layer(x, e)  # [bs, n, d],  [bs, n, n, d]\n",
    "            e = sym_tensor(e)\n",
    "\n",
    "        # class token\n",
    "        mol_token = x.mean(1) # [bs, d]\n",
    "\n",
    "        # regressor\n",
    "        x = self.ln_x_final(mol_token)\n",
    "        x = self.linear_x_final(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "try:\n",
    "    del net\n",
    "except:\n",
    "  print(\"First run\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KZMCbP40ySu"
   },
   "source": [
    "### Instantiate a **dense** graph Transformer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 555,
     "status": "ok",
     "timestamp": 1730638738856,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "N8hNaugJ-BP6",
    "outputId": "5ac56198-3433-4297-c73c-028766df5269"
   },
   "outputs": [],
   "source": [
    "# Instantiate the network\n",
    "net = GT()\n",
    "net = net.to(device)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMcqJOpk0wHd"
   },
   "source": [
    "### Test the forward pass, backward pass and gradient update with a single batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14235,
     "status": "ok",
     "timestamp": 1730638753090,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "WDCEa4_bxVZ4",
    "outputId": "4c397748-53cb-45c6-bc28-e884b952d626"
   },
   "outputs": [],
   "source": [
    "# Test the forward pass, backward pass and gradient update with a single batch\n",
    "init_lr = 0.001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, verbose=True)\n",
    "\n",
    "bs = 50\n",
    "sampler = MoleculeSampler(train_group, bs)\n",
    "print('sampler.num_mol :',sampler.num_mol)\n",
    "num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "print('num_batches_remaining :',num_batches_remaining)\n",
    "sz = sampler.choose_molecule_size()\n",
    "print('sz :',sz)\n",
    "indices = sampler.draw_batch_of_molecules(sz)\n",
    "print('indices :',len(indices),indices)\n",
    "batch_x0 = minibatch_node = torch.stack( [ train_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "print('minibatch_node :',minibatch_node.size())\n",
    "batch_e0 = minibatch_edge = torch.stack( [ train_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "print('minibatch_edge :',minibatch_edge.size())\n",
    "batch_target = torch.stack( [ train_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "print('batch_target :',batch_target.size())\n",
    "\n",
    "batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "print('batch_x_pred',batch_x_pred.size())\n",
    "\n",
    "loss = nn.L1Loss()(batch_x_pred, batch_target)\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4K_C4Fl06_o"
   },
   "source": [
    "### Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1363189,
     "status": "ok",
     "timestamp": 1730640116276,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "B7pTUYKmxVZ4",
    "outputId": "aa7cf91b-c7ce-4653-a1da-7d2e01aaa3c8"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "del net\n",
    "net = GT()\n",
    "net = net.to(device)\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# Optimizer\n",
    "init_lr = 0.0003\n",
    "init_lr = 0.0001\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=init_lr)\n",
    "scheduler_warmup = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda t: min((t+1)/num_warmup, 1.0) ) # warmup scheduler\n",
    "scheduler_tracker = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.95, patience=1, verbose=True) # tracker scheduler\n",
    "\n",
    "num_warmup_batch = 0\n",
    "\n",
    "# Number of mini-batches per epoch\n",
    "nb_epochs = 100\n",
    "\n",
    "lossMAE = nn.L1Loss()\n",
    "\n",
    "print('num batch(before scheduler_tracker), num epoch(before scheduler_tracker), num_warmup_batch(current):', \\\n",
    "      num_warmup, num_warmup//(len(train)//bs), num_warmup_batch)\n",
    "\n",
    "total_loss = moving_loss = -1\n",
    "list_loss = []\n",
    "start=time.time()\n",
    "for epoch in range(nb_epochs):\n",
    "\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    num_data = 0\n",
    "    net.train()\n",
    "\n",
    "    bs = 50\n",
    "    sampler = MoleculeSampler(train_group, bs)\n",
    "    #print('sampler.num_mol :',sampler.num_mol)\n",
    "    while(not sampler.is_empty()):\n",
    "        num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "        #print('num_batches_remaining :',num_batches_remaining)\n",
    "        sz = sampler.choose_molecule_size()\n",
    "        #print('sz :',sz)\n",
    "        indices = sampler.draw_batch_of_molecules(sz)\n",
    "        bs2 = len(indices)\n",
    "        #print('indices :',len(indices),indices)\n",
    "        batch_x0 = minibatch_node = torch.stack( [ train_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "        #print('minibatch_node :',minibatch_node.size())\n",
    "        batch_e0 = minibatch_edge = torch.stack( [ train_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "        #print('minibatch_edge :',minibatch_edge.size())\n",
    "        batch_target = torch.stack( [ train_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "\n",
    "        batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "\n",
    "        loss = lossMAE(batch_x_pred, batch_target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if num_warmup_batch < num_warmup:\n",
    "            scheduler_warmup.step() # warmup scheduler\n",
    "        num_warmup_batch += 1\n",
    "\n",
    "        # COMPUTE STATS\n",
    "        running_loss += bs2 * loss.detach().item()\n",
    "        num_batches += 1\n",
    "        num_data += bs2\n",
    "\n",
    "\n",
    "    # TEST SET\n",
    "    bs = 50\n",
    "    sampler = MoleculeSampler(test_group, bs)\n",
    "    running_test_loss = 0\n",
    "    num_test_data = 0\n",
    "    with torch.no_grad():\n",
    "        while(not sampler.is_empty()):\n",
    "            num_batches_remaining = sampler.compute_num_batches_remaining()\n",
    "            sz = sampler.choose_molecule_size()\n",
    "            indices = sampler.draw_batch_of_molecules(sz)\n",
    "            bs2 = len(indices)\n",
    "            batch_x0 = minibatch_node = torch.stack( [ test_group[sz][i].atom_type for i in indices] ).long().to(device) # [bs, n]\n",
    "            batch_e0 = minibatch_edge = torch.stack( [ test_group[sz][i].bond_type for i in indices] ).long().to(device) # [bs, n, n]\n",
    "            batch_target = torch.stack( [ test_group[sz][i].logP_SA_cycle_normalized for i in indices] ).float().to(device) # [bs, 1]\n",
    "            batch_x_pred = net(batch_x0, batch_e0) # [bs, 1]\n",
    "            running_test_loss += bs2 * lossMAE(batch_x_pred, batch_target).detach().item()\n",
    "            num_test_data += bs2\n",
    "\n",
    "\n",
    "    # AVERAGE STATS THEN DISPLAY\n",
    "    mean_train_loss = running_loss/num_data\n",
    "    mean_test_loss = running_test_loss/num_test_data\n",
    "    if num_warmup_batch >= num_warmup:\n",
    "        scheduler_tracker.step(mean_train_loss) # tracker scheduler defined w.r.t. loss value\n",
    "        num_warmup_batch += 1\n",
    "    elapsed = (time.time()-start)/60\n",
    "    if not epoch%1:\n",
    "        line = 'epoch= ' + str(epoch) + '\\t time= ' + str(elapsed)[:6] + ' min' + '\\t lr= ' + \\\n",
    "        '{:.7f}'.format(optimizer.param_groups[0]['lr']) + '\\t train_loss= ' + str(mean_train_loss)[:6] + \\\n",
    "        '\\t test_loss= ' + str(mean_test_loss)[:6]\n",
    "        print(line)\n",
    "\n",
    "    # Check lr value\n",
    "    if optimizer.param_groups[0]['lr'] < 10**-6: # 2*10**-4: quick, # 10**-6: slow\n",
    "      print(\"\\n lr is equal to min lr -- training stopped\\n\")\n",
    "      break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27B-nWpxxVZ5"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgoipxpwxVZ5"
   },
   "source": [
    "## Compare results\n",
    "\n",
    "| GNN    | train MAE | test MAE |\n",
    "| -------- | ------- | ------- |\n",
    "| Sparse GT (DGL)   | 0.4483    | 0.7327    |\n",
    "| Dense GT (PyTroch)    |     |     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cT0lRlYMxVZ5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_OCOsMTxVZ5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
