{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvFBzAxixZB2"
   },
   "source": [
    "# Lecture : Graph Transformers & Graph ViT\n",
    "\n",
    "## Lab 01 : Vanilla Graph Transformers (GT without Positional Encoding) -- Exercise\n",
    "\n",
    "### Xavier Bresson, Guoji Fu\n",
    "\n",
    "Dwivedi, Bresson, A generalization of transformer networks to graphs, 2020   \n",
    "https://arxiv.org/pdf/2012.09699.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14980,
     "status": "ok",
     "timestamp": 1730638619599,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "H5s9FwqjxZB5",
    "outputId": "803118e2-d94b-478e-8aef-2c9d056d9576"
   },
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/10_Graph_Transformers'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl==1.0.0 # Install DGL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10226,
     "status": "ok",
     "timestamp": 1730638629822,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "lL9od7gGxZB6",
    "outputId": "c42a5d58-90cd-42cd-d8c5-eabad2329af9"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import dgl\n",
    "from dgl.data import MiniGCDataset\n",
    "import dgl.function as fn\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0ErPFvexZB6"
   },
   "source": [
    "# Visualize the artifical graph dataset used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 8758,
     "status": "ok",
     "timestamp": 1730638638577,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "qdt9GsRRxZB6",
    "outputId": "0d1607a6-1f77-4c54-bc06-b46f571808c0"
   },
   "outputs": [],
   "source": [
    "dataset = MiniGCDataset(8, 10, 20) # DGL artificial dataset\n",
    "\n",
    "# visualise the 8 classes of graphs\n",
    "for c in range(8):\n",
    "    graph, label = dataset[c]\n",
    "    #fig, ax = plt.subplots()\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    nx.draw(graph.to_networkx(), ax=ax)\n",
    "    ax.set_title('Class: {:d}'.format(label))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVvB_OYtxZB7"
   },
   "source": [
    "# Exercise 1: Generate train, val and test datasets\n",
    "\n",
    "### Question 1.1: Add node features\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "- Compute the in-degree for each node using ```g.in_degrees()```.\n",
    "\n",
    "- Reshape and convert the in-degrees to a floating-point tensor with shape ```(number_of_nodes, 1)``` using ```.view(-1, 1).float()```.\n",
    "\n",
    "- Assign node features to graphs by storing these features in the ```ndata``` dictionary with the key ```'feat'```: ```g.ndata['feat']```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sg_xm1CpxZB7"
   },
   "outputs": [],
   "source": [
    "# Add node features to graphs\n",
    "def add_node_edge_features(dataset):\n",
    "    for (graph,_) in dataset:\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "\n",
    "        graph.ndata['feat'] = \n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2642,
     "status": "ok",
     "timestamp": 1730638641215,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "vBBlQuY0-Vj2",
    "outputId": "775c03b6-c8f4-4f0a-c82e-a4776a228f1d"
   },
   "outputs": [],
   "source": [
    "# Generate graph datasets\n",
    "trainset = MiniGCDataset(350, 10, 20)\n",
    "testset = MiniGCDataset(100, 10, 20)\n",
    "valset = MiniGCDataset(100, 10, 20)\n",
    "trainset = add_node_edge_features(trainset)\n",
    "testset = add_node_edge_features(testset)\n",
    "valset = add_node_edge_features(valset)\n",
    "print(trainset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtYz7UwWxZB8"
   },
   "source": [
    "### Question 1.2: Define the collate function to prepare a batch of DGL graphs and test it\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- Use ```map()``` and ```zip()``` to unpack the samples into separate graphs and labels lists.\n",
    "\n",
    "- Use DGL's batch function ```dgl.batch()``` to combine the list of individual graphs into a single batched graph.\n",
    "\n",
    "- Transform the list of labels into a PyTorch tensor using ```torch.tensor()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xP_UEoe2xZB8"
   },
   "outputs": [],
   "source": [
    "# collate function prepares a batch of graphs, labels and other graph features (if needed)\n",
    "def collate(samples):\n",
    "    # Input sample is a list of pairs (graph, label)\n",
    "    ###############################################\n",
    "    # YOUR CODE STARTS\n",
    "    ###############################################\n",
    "    graphs, labels = \n",
    "    batch_graphs =   # batch of graphs\n",
    "    batch_labels =   # batch of labels (here class label)\n",
    "    ###############################################\n",
    "    # YOUR CODE ENDS\n",
    "    ###############################################\n",
    "\n",
    "    return batch_graphs, batch_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1730638641216,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "38Q4uK8a6MOZ",
    "outputId": "8f917a6f-68d0-422b-99a8-83bf2211f6ca"
   },
   "outputs": [],
   "source": [
    "# Generate a batch of graphs\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "print(batch_graphs)\n",
    "print(batch_labels)\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "print('batch_x:',batch_x.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0B6GMQRFxZB8"
   },
   "source": [
    "# Exercise 2: Design the class of vanilla GraphTransformer networks with DGL\n",
    "\n",
    "Node update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=&Â  h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h)=\\textrm{Softmax}\\left( A_G \\odot \\frac{QK^T}{\\sqrt{d'}} \\right) V \\in \\mathbb{R}^{N\\times d'=d/H}, A_G\\in \\mathbb{R}^{N\\times N} \\textrm{ (graph adjacency matrix)}\\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, W_Q, W_K, W_V\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}(h_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature)}\\\\\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GuTIR6ikDa53"
   },
   "source": [
    "### Implement a MLP layer for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1C9gZV_O5Xtw"
   },
   "outputs": [],
   "source": [
    "# class MLP layer for classification\n",
    "class MLP_layer(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, L=2): # L = nb of hidden layers\n",
    "        super(MLP_layer, self).__init__()\n",
    "        list_FC_layers = [ nn.Linear( input_dim, input_dim, bias=True ) for l in range(L) ]\n",
    "        list_FC_layers.append(nn.Linear( input_dim, output_dim , bias=True ))\n",
    "        self.FC_layers = nn.ModuleList(list_FC_layers)\n",
    "        self.L = L\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = x\n",
    "        for l in range(self.L):\n",
    "            y = self.FC_layers[l](y)\n",
    "            y = torch.relu(y)\n",
    "        y = self.FC_layers[self.L](y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SATQGOegD7tk"
   },
   "source": [
    "### Question 2.1: Implement a Graph Multi-Head Attention (MHA) Layer with DGL\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Step 1 of message-passing with DGL:* Pass node feature and edge features along edges (src/j => dst/i) by:\n",
    "    - *Step 1.1:* Compute $q_i^T * k_j$. You may use ```edges.dst[]``` for ```i, edges.src[]``` for ```j\n",
    "\n",
    "    - *Step 1.2:* Compute $\\textrm{exp}_{ij} = \\exp( q_i^T * k_j / \\sqrt{d'} )$, ```size=(E,K,1)```.\n",
    "\n",
    "    - *Step 1.3:* Obtain ```V``` by ```edges.src['V'], size=(E,K,d')```.\n",
    "\n",
    "- *Step 2 of message-passing with DGL:* Define a reduce function that\n",
    "    - *Step 2.1:* Use ```nodes.mailbox[]``` to collects all messages ```= {vj, eij}``` sent to node dst/i with *Step 1*.\n",
    "    \n",
    "    - *Step 2.2:* Sum/mean over the graph neigbors ```j``` in ```Ni```.\n",
    "\n",
    "- Assign ```Q, K, V```  to graphs by storing them in the ndata dictionary with the keys ```'Q', 'K', 'V'```: ```g.ndata['Q'], g.ndata['K'], g.ndata['V']``` and reshape them using ```.view(-1, num_heads, head_hidden_dim)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MfmphxRh5YvX"
   },
   "outputs": [],
   "source": [
    "# class graph multi head attention layer\n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "\n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x WQ matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "\n",
    "    # Step 1 of message-passing with DGL:\n",
    "    def message_func(self, edges):\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Step 1.1: Compute q_i^T * k_j, size=(E,K,1), edges.src/dst[].size=(E,K,d')\n",
    "        qikj = ().sum(dim=2).unsqueeze(2)\n",
    "\n",
    "        # Step 1.2: Compute exp_ij = exp( q_i^T * k_j / sqrt(d') ), size=(E,K,1)\n",
    "        expij = \n",
    "\n",
    "        # Step 1.3: Obtain V, size=(E,K,d')\n",
    "        vj = \n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "        return {'expij' : expij, 'vj' : vj}\n",
    "\n",
    "    # Step 2 of message-passing with DGL:\n",
    "    def reduce_func(self, nodes):\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Step 2.1: Collects all messages= eij\n",
    "        # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        expij = \n",
    "\n",
    "        # Step 2.1: Collects all messages= vj\n",
    "        # size=(N,|Nj|,K,d')\n",
    "        vj = \n",
    "\n",
    "        # Step 2.2: Sum/mean over the graph neigbors j in Ni\n",
    "        # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        numerator = \n",
    "\n",
    "        # sum_j' exp_ij', size=(N,K,1)\n",
    "        denominator = \n",
    "\n",
    "        # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        h = numerator / denominator\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return {'h' : h}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        g.ndata['Q'] = # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = # size=(N, K, d'=d/K)\n",
    "\n",
    "        # compute with DGL the graph MHA\n",
    "        g.update_all(self.message_func, self.reduce_func)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        return gMHA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CuJC50bIOKvN"
   },
   "source": [
    "### Implement a GraphTransformer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1rvYQ2z5jje"
   },
   "outputs": [],
   "source": [
    "# class GraphTransformer layer\n",
    "class GraphTransformer_layer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "\n",
    "    def forward(self, g, h):\n",
    "\n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        h_MHA = self.gMHA(g, h) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_mha(h_MHA) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "\n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N f, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(N, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_mlp(h_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e22k33hbOgA7"
   },
   "source": [
    "### Question 2.2: Combine all previous defined MLP Layer, GraphTransformer layer to construct the Graph Transformer network\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Input embedding layer:* Initialize a linear layer ```nn.Linear()``` to convert input features into node embeddings.\n",
    "\n",
    "- *Graph transformer layer:* Initialize a ModuleList ```nn.ModuleList()``` containing ```L``` instances of ```GraphTransformer_layer()```.\n",
    "\n",
    "- *MLP layer:* Initialize a MLP layer ```MLP_layer()``` for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a9X1ec6l5oVq"
   },
   "outputs": [],
   "source": [
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        output_dim = net_parameters['output_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        self.embedding_h = nn.Linear(input_dim, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ])\n",
    "        self.MLP_layer = MLP_layer(hidden_dim, output_dim)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # input embedding\n",
    "        h =  # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h = GT_layer(g,h) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        y = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)\n",
    "        y =  # size=(num_graphs, num_classes)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tesE1rtOV0x4"
   },
   "source": [
    "### Instantiate a graph Transformer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 415,
     "status": "ok",
     "timestamp": 1730638641624,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "foEBVTqtxZB8",
    "outputId": "20e151df-ebd6-4989-ba98-a1898a180386"
   },
   "outputs": [],
   "source": [
    "# Instantiate one network (testing)\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['output_dim'] = 8 # nb of classes\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "print(net)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBZ_dJJNxZB9"
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SkDPX_lVPuwo"
   },
   "source": [
    "### Define the accuracy function to compute the prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Ns4Zq6K5-hl"
   },
   "outputs": [],
   "source": [
    "def accuracy(scores, targets):\n",
    "    scores = scores.detach().argmax(dim=1)\n",
    "    acc = (scores==targets).float().sum().item()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7fVyFMgP4mN"
   },
   "source": [
    "### Implement the ```run_one_epoch``` function for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrG1FjnZ6A5b"
   },
   "outputs": [],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    nb_data = 0\n",
    "    gpu_mem = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x)\n",
    "        loss = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        epoch_acc += accuracy(batch_scores, batch_labels)\n",
    "        nb_data += batch_labels.size(0)\n",
    "    epoch_loss /= (iter + 1)\n",
    "    epoch_acc /= nb_data\n",
    "    return epoch_loss, epoch_acc, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6mimbRdPs_M"
   },
   "source": [
    "### Set up data loaders, initialize the model, and running the training loop for the graph Transformer network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84986,
     "status": "ok",
     "timestamp": 1730638726604,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "YWp4qgvoxZB9",
    "outputId": "ff7036a5-d116-4eb8-c172-c60e234f8da4"
   },
   "outputs": [],
   "source": [
    "# dataset loaders\n",
    "train_loader = DataLoader(trainset, batch_size=50, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=50, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=50, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['output_dim'] = 8 # nb of classes\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# optimizer\n",
    "loss_fc = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(50):\n",
    "    start = time.time()\n",
    "    epoch_train_loss, epoch_train_acc, optimizer = run_one_epoch(net, train_loader, True, loss_fc, optimizer)\n",
    "    with torch.no_grad():\n",
    "        epoch_test_loss, epoch_test_acc, _ = run_one_epoch(net, test_loader, False, loss_fc)\n",
    "        epoch_val_loss, epoch_val_acc, _ = run_one_epoch(net, val_loader, False, loss_fc)\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}, val_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss, epoch_val_loss))\n",
    "    print('                       train_acc: {:.4f}, test_acc: {:.4f}, val_acc: {:.4f}'.format(epoch_train_acc, epoch_test_acc, epoch_val_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ifXzMpwpxZB-"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Npx7M6gzxZB-"
   },
   "source": [
    "## Compare results\n",
    "\n",
    "| GNN    | train acc | test acc |\n",
    "| -------- | ------- | ------- |\n",
    "| GCN  | 0.7829   | 0.7900    |\n",
    "| GIN | 0.0800     | 0.1000     |\n",
    "| GAT    | 0.9229    | 0.9400    |\n",
    "| Vanilla GT    |     |     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ct3_yGIcxZB-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STZltpAwxZB-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
