{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfFWcbbExXcc"
   },
   "source": [
    "# Lecture : Graph Transformers & Graph ViT\n",
    "\n",
    "## Lab 03 : Graph Transformers with edge features and DGL (sparse linear algebra) -- Exercise\n",
    "\n",
    "### Xavier Bresson, Guoji Fu\n",
    "\n",
    "Dwivedi, Bresson, A generalization of transformer networks to graphs, 2020   \n",
    "https://arxiv.org/pdf/2012.09699.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7102,
     "status": "ok",
     "timestamp": 1730637248755,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "IZCvd1fTxXce",
    "outputId": "41ceed4f-96e9-4b1a-a395-f72c3621d79d"
   },
   "outputs": [],
   "source": [
    "# For Google Colaboratory\n",
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    # mount google drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    path_to_file = '/content/gdrive/My Drive/CS5284_2024_codes/codes/10_Graph_Transformers'\n",
    "    print(path_to_file)\n",
    "    # change current path to the folder containing \"path_to_file\"\n",
    "    os.chdir(path_to_file)\n",
    "    !pwd\n",
    "    !pip install dgl==1.0.0 # Install DGL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Y9hiy25BxXcf"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pickle\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import networkx as nx\n",
    "import sys; sys.path.insert(0, 'lib/')\n",
    "from lib.utils import compute_ncut\n",
    "from lib.molecules import Dictionary, MoleculeDataset, MoleculeDGL, Molecule\n",
    "import os, datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3Es6_zDxXcf"
   },
   "source": [
    "# Load molecular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4351,
     "status": "ok",
     "timestamp": 1730637253097,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "fl68-dJTxXcf",
    "outputId": "7f142b87-0e09-4434-d76b-18d370ec5f8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "4\n",
      "Loading datasets QM9_1.4k_dgl...\n",
      "train, test, val sizes : 1000 200 200\n",
      "Time: 0.8732s\n",
      "1000\n",
      "200\n",
      "200\n",
      "([Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})], [tensor([-0.2532]), tensor([1.0897])])\n",
      "(Graph(num_nodes=9, num_edges=18,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([0.5060]))\n",
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-4.4048]))\n"
     ]
    }
   ],
   "source": [
    "# Select dataset\n",
    "dataset_name = 'QM9_1.4k'; data_folder_pytorch = 'dataset/QM9_1.4k_pytorch/'; data_folder_dgl = 'dataset/QM9_1.4k_dgl/'\n",
    "\n",
    "# Load the number of atom and bond types\n",
    "with open(data_folder_pytorch + \"atom_dict.pkl\" ,\"rb\") as f: num_atom_type = len(pickle.load(f))\n",
    "with open(data_folder_pytorch + \"bond_dict.pkl\" ,\"rb\") as f: num_bond_type = len(pickle.load(f))\n",
    "print(num_atom_type)\n",
    "print(num_bond_type)\n",
    "\n",
    "# Load the DGL datasets\n",
    "datasets_dgl = MoleculeDataset(dataset_name, data_folder_dgl)\n",
    "trainset, valset, testset = datasets_dgl.train, datasets_dgl.val, datasets_dgl.test\n",
    "print(len(trainset))\n",
    "print(len(valset))\n",
    "print(len(testset))\n",
    "idx = 0\n",
    "print(trainset[:2])\n",
    "print(valset[idx])\n",
    "print(testset[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTYjHxtUxXcg"
   },
   "source": [
    "# Add positional encoding feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1998,
     "status": "ok",
     "timestamp": 1730637255093,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "TQp3RdpAxXcg",
    "outputId": "19126433-8031-4aa7-d982-1cd2d516a142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Graph(num_nodes=9, num_edges=20,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'pos_enc': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)}), tensor([-0.2532]))\n"
     ]
    }
   ],
   "source": [
    "# Positional encoding as Laplacian eigenvectors\n",
    "def LapEig_positional_encoding(g, pos_enc_dim):\n",
    "    Adj = g.adj().to_dense() # Adjacency matrix\n",
    "    Dn = ( g.in_degrees()** -0.5 ).diag() # Inverse and sqrt of degree matrix\n",
    "    Lap = torch.eye(g.number_of_nodes()) - Dn.matmul(Adj).matmul(Dn) # Laplacian operator\n",
    "    EigVal, EigVec = torch.linalg.eig(Lap) # Compute full EVD\n",
    "    EigVal, EigVec = EigVal.real, EigVec.real # make eig real\n",
    "    EigVec = EigVec[:, EigVal.argsort()] # sort in increasing order of eigenvalues\n",
    "    EigVec = EigVec[:,1:pos_enc_dim+1] # select the first non-trivial \"pos_enc_dim\" eigenvector\n",
    "    return EigVec\n",
    "\n",
    "# Add node positional encoding features to graphs\n",
    "pos_enc_dim = 3 # dimension of PE, QM9\n",
    "def add_node_edge_features(dataset):\n",
    "    for (graph,_) in dataset:\n",
    "        graph.ndata['pos_enc'] = LapEig_positional_encoding(graph, pos_enc_dim) # node positional encoding feature\n",
    "    return dataset\n",
    "\n",
    "# Generate graph datasets\n",
    "trainset = add_node_edge_features(trainset)\n",
    "testset = add_node_edge_features(testset)\n",
    "valset = add_node_edge_features(valset)\n",
    "print(trainset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukrFXYNexXcg"
   },
   "source": [
    "# Visualize positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 863
    },
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1730637255907,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "i77JNkdBxXcg",
    "outputId": "a2e5b1cf-7e62-4fe9-8653-d1fb55bab668"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = trainset[0][0]\n",
    "\n",
    "# Visualize graph\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "Adj = graph.adj().to_dense()\n",
    "A_nx = nx.from_numpy_array(Adj.numpy())\n",
    "C = compute_ncut(Adj.long(), 4)\n",
    "nx.draw(A_nx, ax=ax, node_color=C, cmap='jet', with_labels=True, font_size=10) # visualise node indexes\n",
    "ax.title.set_text('Visualization with networkx')\n",
    "plt.show()\n",
    "\n",
    "# plot 2D coordinates\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "x = graph.ndata['pos_enc']\n",
    "ax.scatter(x[:,0], x[:,1])\n",
    "idx = list(range(graph.number_of_nodes()))\n",
    "ax.scatter(x[:,0], x[:,1], c=C, cmap='jet')\n",
    "for i, txt in enumerate(idx):\n",
    "    ax.annotate(txt, (x[:,0][i], x[:,1][i]), textcoords=\"offset points\", xytext=(1,5))\n",
    "ax.title.set_text('2D embdding of nodes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R14LMFIRxXch"
   },
   "source": [
    "# Define the collate function to prepare a batch of DGL graphs and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 689,
     "status": "ok",
     "timestamp": 1730637256594,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "d7ZXaxwKxXch",
    "outputId": "af206596-e7cd-41ce-9b51-20b2b9e4167e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=85, num_edges=192,\n",
      "      ndata_schemes={'feat': Scheme(shape=(), dtype=torch.int64), 'pos_enc': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'feat': Scheme(shape=(), dtype=torch.int64)})\n",
      "tensor([[-3.0030],\n",
      "        [-0.2742],\n",
      "        [ 0.0930],\n",
      "        [-0.3138],\n",
      "        [-1.7507],\n",
      "        [-0.5748],\n",
      "        [-0.2974],\n",
      "        [-1.4732],\n",
      "        [ 0.8009],\n",
      "        [ 2.6352]])\n",
      "batch_x: torch.Size([85])\n",
      "batch_pe: torch.Size([85, 3])\n",
      "batch_e: torch.Size([192])\n"
     ]
    }
   ],
   "source": [
    "# collate function prepares a batch of graphs, labels and other graph features (if needed)\n",
    "def collate(samples):\n",
    "    # Input sample is a list of pairs (graph, label)\n",
    "    graphs, labels = map(list, zip(*samples))\n",
    "    batch_graphs = dgl.batch(graphs)    # batch of graphs\n",
    "    batch_labels = torch.stack(labels)  # batch of labels (here chemical target)\n",
    "    return batch_graphs, batch_labels\n",
    "\n",
    "\n",
    "# Generate a batch of graphs\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "print(batch_graphs)\n",
    "print(batch_labels)\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "print('batch_x:',batch_x.size())\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "print('batch_pe:',batch_pe.size())\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "print('batch_e:',batch_e.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VBKKjxWxXch"
   },
   "source": [
    "# Exercise 1: Design the class of GraphTransformer networks with edge features\n",
    "\n",
    "Node update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=& h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell}),\\textrm{LN}(e^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h,e)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k,e_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{E\\times d'}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h,e)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T \\textrm{diag}(e_{ij}) k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T \\textrm{diag}(e_{ij'}) k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score w/ edge feature}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, E=e_k W_E\\in \\mathbb{R}^{E\\times d'=d/H}, W_Q, W_K, W_V, W_E\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n",
    "\n",
    "Edge update equation:  \n",
    "\\begin{eqnarray*}\n",
    "\\bar{e}^{\\ell} &=& e^{\\ell} + \\textrm{gMHE} (\\textrm{LN}(e^{\\ell}),\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{E\\times d}\\\\\n",
    "e^{\\ell+1} &=& \\bar{e}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{e}^{\\ell})) \\in \\mathbb{R}^{E\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHE}(e,h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHE}(e_k,h_k) \\right) W_O^e \\in \\mathbb{R}^{E\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, e_k\\in \\mathbb{R}^{E\\times d'}, W_O^e\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\textrm{with } \\textrm{gHE}(e,h)_{ij}=q_i \\odot e_{ij} \\odot k_j/\\sqrt{d'} \\in \\mathbb{R}^{d'} \\textrm{ (point-wise equation)}\\\\\n",
    "e^{\\ell=0} &=& \\textrm{LL}(e_0) \\in \\mathbb{R}^{E\\times d}\\ \\textrm{(input edge feature)}\\\\\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqI7VXd9gchf"
   },
   "source": [
    "### Question 1.1: Implement a Graph Multi-Head Attention (MHA) Layer with edge features\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Step 1 of message-passing with DGL:* Pass node feature and edge features along edges (src/j => dst/i) by:\n",
    "    - *Step 1.1:* Compute bi-linear products with edge feature: $q_i^T * diag(e_{ij}) * k_j$. You may use ```edges.dst[]``` for ```i, edges.src[]``` for ```j, edges.data[]``` form ```ij```\".  \n",
    "\n",
    "    - *Step 1.2* Compute $\\textrm{exp}_{ij} = \\exp( q_i^T * k_j / \\sqrt{d'} )$, ```size=(E,K,1)```.\n",
    "\n",
    "    - *Step 1.3:* Obtain ```V```.\n",
    "\n",
    "    - *Step 1.4:* Compute edge feature: $q_i^T * diag(e_{ij}) * k_j$.\n",
    "\n",
    "    - *Step 1.5:* Update edge feature.\n",
    "\n",
    "- *Step 2 of message-passing with DGL:* Define a reduce function that\n",
    "    - *Step 2.1:* Use ```nodes.mailbox[]``` to collects all messages ```= {vj, eij}``` sent to node dst/i with *Step 1*.\n",
    "    \n",
    "    - *Step 2.2:* Sum/mean over the graph neigbors ```j``` in ```Ni```.\n",
    "\n",
    "- Assign ```Q, K, V, E, F, G```  to graphs by storing them in the ndata dictionary with the keys ```'Q', 'K', 'V', 'E', 'F', 'G'``` for ```g.ndata[]``` and reshape them using ```.view(-1, num_heads, head_hidden_dim)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "XQ-OgcDJ7rC4"
   },
   "outputs": [],
   "source": [
    "# class graph multi head attention layer\n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "\n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x W matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WE = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WF = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WG = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "\n",
    "    # Step 1 of message-passing with DGL:\n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i)\n",
    "    def message_func(self, edges):\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Step 1.1: Compute bi-linear products with edge feature\n",
    "        qi = edges.dst['Q']\n",
    "        kj = edges.src['K']\n",
    "        eij = edges.data['E']\n",
    "        qikj = qi * eij * kj # size=(E,K,1), edges.src/dst/data[].size=(E,K,d')\n",
    "        qikj = qikj.sum(dim=2).unsqueeze(2) # Important for calculation\n",
    "        \n",
    "        # Step 1.2: Compute exp_ij = exp( q_i^T * k_j / sqrt(d') ), size=(E,K,1)\n",
    "        expij =  torch.exp(qikj / torch.sqrt(torch.tensor(self.head_hidden_dim)))\n",
    "\n",
    "        # Step 1.3: Obtain vj\n",
    "        vj = edges.src['V'] # size=(E,K,d')\n",
    "\n",
    "        # Step 1.4: Compute edge feature: e_ij = q_i^T * diag(E_ij) * k_j / sqrt(d'), size=(E,K,d')\n",
    "        eij =  qi * eij * kj  / torch.sqrt(torch.tensor(self.head_hidden_dim))\n",
    "\n",
    "        # Step 1.5: Update edge feature\n",
    "        edges.data['e'] = eij\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return {'expij' : expij, 'vj' : vj}\n",
    "\n",
    "    # Step 2 of message-passing with DGL:\n",
    "    #   Reduce function collects all messages={hj, eij} sent to node dst/i with Step 1\n",
    "    #                   and sum/mean over the graph neigbors j in Ni\n",
    "    def reduce_func(self, nodes):\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Step 2.1: Collects all messages= eij\n",
    "        # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        expij =  nodes.mailbox['expij']\n",
    "\n",
    "        # Step 2.1: Collects all messages= vj\n",
    "        # size=(N,|Nj|,K,d')\n",
    "        vj =  nodes.mailbox['vj']\n",
    "\n",
    "        # Step 2.2: Sum/mean over the graph neigbors j in Ni\n",
    "        # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        numerator =  torch.sum(expij * vj, dim=1)\n",
    "\n",
    "        # sum_j' exp_ij', size=(N,K,1)\n",
    "        denominator =  torch.sum(expij, dim = 1)\n",
    "\n",
    "        # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        h = numerator / denominator\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        return {'h' : h}\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "        E = self.WE(e) # size=(E, d)\n",
    "        F = self.WF(h) # size=(N, d)\n",
    "        G = self.WG(h) # size=(N, d)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        g.ndata['Q'] = Q.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = K.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = V.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.edata['E'] = E.view(-1, self.num_heads, self.head_hidden_dim) # size=(E, K, d'=d/K)\n",
    "        g.ndata['F'] = F.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['G'] = G.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.update_all(self.message_func, self.reduce_func) # compute with DGL the graph MHA\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        gMHE = g.edata['e'] # size=(E, K, d'=d/K)\n",
    "        return gMHA, gMHE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJix5TqbrFhR"
   },
   "source": [
    "### Question 1.2: Implement a Graph Transformer layer (with edge feature)\n",
    "\n",
    "- Implement dropout, layer normalization, and residual connection layers for edge features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Roe4rKU371mH"
   },
   "outputs": [],
   "source": [
    "# class GraphTransformer layer\n",
    "class GraphTransformer_layer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_h_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_h_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim) # layer normalization\n",
    "        self.layer_norm1e = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Dropout layers for edge features\n",
    "        self.dropout_e_mha = nn.Dropout(dropout)\n",
    "        self.dropout_e_mlp = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP layers for edge features\n",
    "        self.WOe = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Layer normalization for edge features\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.layer_norm2e = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # MLP layers for edge features\n",
    "        self.linear1e = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear2e = nn.Linear(hidden_dim, hidden_dim)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "    def forward(self, g, h, e):\n",
    "\n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        e_rc = e\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # layer normalization for edge features, size=(N, d)\n",
    "        e = self.layer_norm1e(e)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        h_MHA, e_MHE = self.gMHA(g, h, e) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_h_mha(h_MHA) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Update for edge features\n",
    "        e_MHE = e_MHE.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        e_MHE = self.dropout_e_mha(e_MHE) \n",
    "        e_MHE = self.WOe(e_MHE)\n",
    "        e = e_rc + e_MHE # residual connection, size=(N, d)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        e_rc = e # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N, d)\n",
    "        e = self.layer_norm2e(e) # layer normalization, size=(N, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(H, d)\n",
    "        e_MLP = self.linear1e(e) # LL, size=(H, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        e_MLP = torch.relu(e_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_h_mlp(h_MLP) # dropout, size=(N, d)\n",
    "        e_MLP = self.dropout_e_mlp(e_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        e_MLP = self.linear2e(e_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "        e = e_rc + e_MLP # residual connection, size=(N, d)\n",
    "\n",
    "        return h, e\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rST8IpB2rYqK"
   },
   "source": [
    "### Question 1.3: Combine all previous defined MLP Layer, GraphTransformer layer to construct the Graph Transformer network (with edge feature)\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- *Adding a input edge embedding layer:* Initialize a linear layer ```nn.Linear()``` to convert input edge features into edge embeddings.\n",
    "\n",
    "- *Graph transformer layer (with edge feature):* Initialize a ModuleList ```nn.ModuleList()``` containing ```L``` instances of ```GraphTransformer_layer()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2DQRl2bYxXch"
   },
   "outputs": [],
   "source": [
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        pos_enc_dim = net_parameters['pos_enc_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Initialize a edge embedding layer\n",
    "        self.embedding_e = nn.Embedding(num_bond_type, hidden_dim)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        self.embedding_pe = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ])\n",
    "        self.ln_h_final = nn.LayerNorm(hidden_dim)\n",
    "        self.linear_h_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, g, h, pe, e):\n",
    "\n",
    "        # input node embedding\n",
    "        h = self.embedding_h(h) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # if PE used\n",
    "        # h = h + self.embedding_pe(pe) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        ###############################################\n",
    "        # YOUR CODE STARTS\n",
    "        ###############################################\n",
    "        # Implement teh edge embedding layer\n",
    "        # size=(num_edges, hidden_dim)\n",
    "        e =  self.embedding_e(e)\n",
    "        ###############################################\n",
    "        # YOUR CODE ENDS\n",
    "        ###############################################\n",
    "\n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h, e = GT_layer(g, h, e) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        mol_token = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)\n",
    "        y = self.ln_h_final(mol_token)\n",
    "        y = self.linear_h_final(y) # size=(num_graphs, num_classes)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1730637256594,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "qzWVgqKW7vSU",
    "outputId": "3703d3bb-d48e-43d6-ed06-8723e11cb904"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphTransformer_net(\n",
      "  (embedding_h): Embedding(9, 128)\n",
      "  (embedding_e): Embedding(4, 128)\n",
      "  (embedding_pe): Linear(in_features=3, out_features=128, bias=True)\n",
      "  (GraphTransformer_layers): ModuleList(\n",
      "    (0-3): 4 x GraphTransformer_layer(\n",
      "      (dropout_h_mha): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_h_mlp): Dropout(p=0.0, inplace=False)\n",
      "      (gMHA): graph_MHA_layer(\n",
      "        (WQ): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WK): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WV): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WE): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WF): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (WG): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "      (WO): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (layer_norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm1e): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (dropout_e_mha): Dropout(p=0.0, inplace=False)\n",
      "      (dropout_e_mlp): Dropout(p=0.0, inplace=False)\n",
      "      (WOe): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (layer_norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2e): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (linear1e): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (linear2e): Linear(in_features=128, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (ln_h_final): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (linear_h_final): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate one network (testing)\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "print(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "batch_e = batch_graphs.edata['feat']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x, batch_pe, batch_e)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0gZzixzxXci"
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291348,
     "status": "ok",
     "timestamp": 1730637547939,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "Hw7LEG5exXci",
    "outputId": "ead6b872-7372-4f0c-cffa-845c8147baff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 799233 (0.80 million)\n",
      "Epoch 0, time 9.6029, train_loss: 1.3173, test_loss: 1.2226\n",
      "Epoch 1, time 18.0250, train_loss: 1.1017, test_loss: 0.9981\n",
      "Epoch 2, time 26.7906, train_loss: 1.0099, test_loss: 0.9891\n",
      "Epoch 3, time 35.4225, train_loss: 0.9724, test_loss: 0.9583\n",
      "Epoch 4, time 43.7715, train_loss: 0.9461, test_loss: 0.9119\n",
      "Epoch 5, time 52.0988, train_loss: 0.9086, test_loss: 0.9044\n",
      "Epoch 6, time 60.8016, train_loss: 0.8893, test_loss: 0.8871\n",
      "Epoch 7, time 69.2134, train_loss: 0.8757, test_loss: 0.8622\n",
      "Epoch 8, time 77.3944, train_loss: 0.8767, test_loss: 0.8899\n",
      "Epoch 9, time 85.5711, train_loss: 0.8893, test_loss: 0.8518\n",
      "Epoch 10, time 93.7565, train_loss: 0.8398, test_loss: 0.8276\n",
      "Epoch 11, time 101.8592, train_loss: 0.8295, test_loss: 0.8268\n",
      "Epoch 12, time 109.9120, train_loss: 0.8723, test_loss: 0.8042\n",
      "Epoch 13, time 117.9949, train_loss: 0.8029, test_loss: 0.8335\n",
      "Epoch 14, time 126.0222, train_loss: 0.8085, test_loss: 0.9937\n",
      "Epoch 15, time 134.1186, train_loss: 0.8144, test_loss: 0.7780\n",
      "Epoch 16, time 142.2447, train_loss: 0.7828, test_loss: 0.8469\n",
      "Epoch 17, time 150.2465, train_loss: 0.7844, test_loss: 0.7630\n",
      "Epoch 18, time 158.3156, train_loss: 0.8122, test_loss: 0.7998\n",
      "Epoch 19, time 166.3354, train_loss: 0.7698, test_loss: 0.7769\n",
      "Epoch 20, time 174.3292, train_loss: 0.7374, test_loss: 0.7863\n",
      "Epoch 21, time 182.3992, train_loss: 0.7871, test_loss: 0.8936\n",
      "Epoch 22, time 190.4208, train_loss: 0.7716, test_loss: 0.7996\n",
      "Epoch 23, time 198.4532, train_loss: 0.7319, test_loss: 0.7698\n",
      "Epoch 24, time 206.5279, train_loss: 0.6909, test_loss: 0.8060\n",
      "Epoch 25, time 214.5750, train_loss: 0.7025, test_loss: 0.7448\n",
      "Epoch 26, time 222.6528, train_loss: 0.7041, test_loss: 0.7899\n",
      "Epoch 27, time 230.7100, train_loss: 0.7093, test_loss: 0.7657\n",
      "Epoch 28, time 238.7045, train_loss: 0.6996, test_loss: 0.7604\n",
      "Epoch 29, time 246.8860, train_loss: 0.6825, test_loss: 0.7498\n",
      "Epoch 30, time 254.9586, train_loss: 0.6768, test_loss: 0.7446\n",
      "Epoch 31, time 263.0941, train_loss: 0.6962, test_loss: 0.7699\n",
      "Epoch 32, time 271.1871, train_loss: 0.6718, test_loss: 0.7635\n",
      "Epoch 33, time 279.1121, train_loss: 0.6912, test_loss: 0.7941\n",
      "Epoch 34, time 287.1623, train_loss: 0.7062, test_loss: 0.7940\n",
      "Epoch 35, time 295.2648, train_loss: 0.6891, test_loss: 0.7665\n",
      "Epoch 36, time 303.3161, train_loss: 0.6663, test_loss: 0.7295\n",
      "Epoch 37, time 311.3751, train_loss: 0.6362, test_loss: 0.7137\n",
      "Epoch 38, time 319.3946, train_loss: 0.6287, test_loss: 0.7515\n",
      "Epoch 39, time 327.5569, train_loss: 0.6389, test_loss: 0.7664\n",
      "Epoch 40, time 335.7781, train_loss: 0.6733, test_loss: 0.7285\n",
      "Epoch 41, time 343.9184, train_loss: 0.6626, test_loss: 0.7361\n",
      "Epoch 42, time 352.0572, train_loss: 0.6091, test_loss: 0.7176\n",
      "Epoch 43, time 360.1536, train_loss: 0.6256, test_loss: 0.7654\n",
      "Epoch 44, time 368.2368, train_loss: 0.6153, test_loss: 0.7293\n",
      "Epoch 45, time 376.3756, train_loss: 0.6088, test_loss: 0.7574\n",
      "Epoch 46, time 384.4620, train_loss: 0.6356, test_loss: 0.7162\n",
      "Epoch 47, time 392.5343, train_loss: 0.6563, test_loss: 0.7369\n",
      "Epoch 48, time 400.5694, train_loss: 0.6152, test_loss: 0.7273\n",
      "Epoch 49, time 408.5009, train_loss: 0.5946, test_loss: 0.7265\n",
      "Epoch 50, time 416.5344, train_loss: 0.5832, test_loss: 0.7368\n",
      "Epoch 51, time 424.5466, train_loss: 0.5807, test_loss: 0.7325\n",
      "Epoch 52, time 432.5712, train_loss: 0.5924, test_loss: 0.7166\n",
      "Epoch 53, time 440.6388, train_loss: 0.5810, test_loss: 0.7377\n",
      "Epoch 54, time 448.7514, train_loss: 0.5908, test_loss: 0.7251\n",
      "Epoch 55, time 456.8252, train_loss: 0.5749, test_loss: 0.7025\n",
      "Epoch 56, time 465.0431, train_loss: 0.5850, test_loss: 0.7333\n",
      "Epoch 57, time 473.1681, train_loss: 0.5552, test_loss: 0.7387\n",
      "Epoch 58, time 481.2005, train_loss: 0.5694, test_loss: 0.7562\n",
      "Epoch 59, time 489.3227, train_loss: 0.5527, test_loss: 0.7211\n",
      "Epoch 60, time 497.3805, train_loss: 0.5522, test_loss: 0.7335\n",
      "Epoch 61, time 505.3620, train_loss: 0.5524, test_loss: 0.7379\n",
      "Epoch 62, time 513.3922, train_loss: 0.5434, test_loss: 0.7042\n",
      "Epoch 63, time 521.4724, train_loss: 0.5384, test_loss: 0.7498\n",
      "Epoch 64, time 529.4801, train_loss: 0.5433, test_loss: 0.7310\n",
      "Epoch 65, time 537.5325, train_loss: 0.5645, test_loss: 0.7068\n",
      "Epoch 66, time 545.5840, train_loss: 0.5743, test_loss: 0.8060\n",
      "Epoch 67, time 553.6919, train_loss: 0.5676, test_loss: 0.7141\n",
      "Epoch 68, time 561.7010, train_loss: 0.5505, test_loss: 0.7369\n",
      "Epoch 69, time 569.7761, train_loss: 0.5305, test_loss: 0.7098\n",
      "Epoch 70, time 577.8961, train_loss: 0.5317, test_loss: 0.7312\n",
      "Epoch 71, time 585.9472, train_loss: 0.5118, test_loss: 0.7113\n",
      "Epoch 72, time 594.0155, train_loss: 0.5139, test_loss: 0.7340\n",
      "Epoch 73, time 602.0817, train_loss: 0.5452, test_loss: 0.7339\n",
      "Epoch 74, time 610.1613, train_loss: 0.5368, test_loss: 0.7141\n",
      "Epoch 75, time 618.2493, train_loss: 0.5294, test_loss: 0.7317\n",
      "Epoch 76, time 626.3958, train_loss: 0.5217, test_loss: 0.7629\n",
      "Epoch 77, time 634.4873, train_loss: 0.5470, test_loss: 0.7685\n",
      "Epoch 78, time 642.5742, train_loss: 0.5328, test_loss: 0.7308\n",
      "Epoch 79, time 650.6148, train_loss: 0.4990, test_loss: 0.7307\n",
      "Epoch 80, time 658.7145, train_loss: 0.5109, test_loss: 0.7285\n",
      "Epoch 81, time 666.8270, train_loss: 0.5068, test_loss: 0.7332\n",
      "Epoch 82, time 674.8961, train_loss: 0.5015, test_loss: 0.6926\n",
      "Epoch 83, time 682.9847, train_loss: 0.4787, test_loss: 0.7134\n",
      "Epoch 84, time 691.0722, train_loss: 0.4777, test_loss: 0.7120\n",
      "Epoch 85, time 699.0432, train_loss: 0.4829, test_loss: 0.7170\n",
      "Epoch 86, time 707.0877, train_loss: 0.4793, test_loss: 0.7414\n",
      "Epoch 87, time 715.1299, train_loss: 0.5028, test_loss: 0.7291\n",
      "Epoch 88, time 723.2272, train_loss: 0.4943, test_loss: 0.7200\n",
      "Epoch 89, time 731.2516, train_loss: 0.4913, test_loss: 0.7306\n",
      "Epoch 90, time 739.2558, train_loss: 0.4564, test_loss: 0.6951\n",
      "Epoch 91, time 747.3231, train_loss: 0.4664, test_loss: 0.7211\n",
      "Epoch 92, time 755.3403, train_loss: 0.4634, test_loss: 0.7124\n",
      "Epoch 93, time 763.4250, train_loss: 0.4478, test_loss: 0.7404\n",
      "Epoch 94, time 771.4385, train_loss: 0.4607, test_loss: 0.7216\n",
      "Epoch 95, time 779.4413, train_loss: 0.4646, test_loss: 0.7543\n",
      "Epoch 96, time 787.5185, train_loss: 0.4738, test_loss: 0.7019\n",
      "Epoch 97, time 795.6436, train_loss: 0.4754, test_loss: 0.7031\n",
      "Epoch 98, time 803.7470, train_loss: 0.4524, test_loss: 0.7145\n",
      "Epoch 99, time 811.8746, train_loss: 0.4598, test_loss: 0.6995\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        bs2 = batch_labels.size(0)\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_e = batch_graphs.edata['feat']\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_pe, batch_e)\n",
    "        lossMAE = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            lossMAE.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += bs2 * lossMAE.detach().item()\n",
    "        nb_data += bs2\n",
    "    epoch_loss /= nb_data\n",
    "    return epoch_loss, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "lossMAE = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    epoch_train_loss, optimizer = run_one_epoch(net, train_loader, True, lossMAE, optimizer)\n",
    "    with torch.no_grad():\n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False, lossMAE)[0]\n",
    "        # epoch_val_loss = run_one_epoch(net, val_loader, False, lossMAE)[0]\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehCjHW1YxXci"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Amo_4YiKxXci"
   },
   "source": [
    "# GT without edge features\n",
    "\n",
    "Node update equation:\n",
    "\\begin{eqnarray*}\n",
    "\\bar{h}^{\\ell} &=& h^{\\ell} + \\textrm{gMHA} (\\textrm{LN}(h^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "h^{\\ell+1} &=& \\bar{h}^{\\ell} + \\textrm{MLP} (\\textrm{LN}(\\bar{h}^{\\ell})) \\in \\mathbb{R}^{N\\times d}\\\\\n",
    "&&\\textrm{with } \\textrm{gMHA}(h)=\\textrm{Concat}_{k=1}^H \\left( \\textrm{gHA}(h_k) \\right) W_O \\in \\mathbb{R}^{N\\times d},\\ h_k\\in \\mathbb{R}^{N\\times d'=d/H}, W_O\\in \\mathbb{R}^{d\\times d} \\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h)=\\textrm{Softmax}\\left( A_G \\odot \\frac{QK^T}{\\sqrt{d'}} \\right) V \\in \\mathbb{R}^{N\\times d'=d/H}, A_G\\in \\mathbb{R}^{N\\times N} \\textrm{ (graph adjacency matrix)}\\\\\n",
    "&&\\quad\\quad\\ \\textrm{gHA}(h)_i= \\sum_{j\\in \\mathcal{N}_i} \\underbrace{\\frac{\\exp(q_i^T k_j/\\sqrt{d'})}{ \\sum_{j'\\in\\mathcal{N}_i} \\exp(q_i^T k_{j'}/\\sqrt{d'}) }}_{\\textrm{graph attention score}_{ij}} v_j\\ \\textrm{ (point-wise equation)}\\\\\n",
    "&&\\quad\\quad\\ Q=h_k W_Q, K=h_k W_K, V=h_k W_V\\in \\mathbb{R}^{N\\times d'=d/H}, W_Q, W_K, W_V\\in \\mathbb{R}^{d'\\times d'}\\\\\n",
    "h^{\\ell=0} &=& \\textrm{LL}_1(h_0)+\\textrm{LL}_2(p_0) \\in \\mathbb{R}^{N\\times d}\\ \\textrm{(input node feature and positional encoding)}\\\\\n",
    "&&\\textrm{with } p_0=\\Phi_{\\{2,..,K+1\\}}\\in \\mathbb{R}^{N\\times K},\\ \\Delta = \\Phi \\Lambda \\Phi^T \\in \\mathbb{R}^{N\\times N}\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BKceFO_W8HBf"
   },
   "outputs": [],
   "source": [
    "# class graph multi head attention layer\n",
    "class graph_MHA_layer(nn.Module): # MHA = Multi Head Attention\n",
    "\n",
    "    def __init__(self, hidden_dim, head_hidden_dim, num_heads): # hidden_dim = d\n",
    "        super().__init__()\n",
    "        self.head_hidden_dim = head_hidden_dim # head_hidden_dim = d' = d/K\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.WQ = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True) # define K x WQ matrix of size=(d',d')\n",
    "        self.WK = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "        self.WV = nn.Linear(hidden_dim, head_hidden_dim * num_heads, bias=True)\n",
    "\n",
    "    # Step 1 of message-passing with DGL:\n",
    "    #   Node feature and edge features are passed along edges (src/j => dst/i)\n",
    "    def message_func(self, edges):\n",
    "        # Compute the dot products q_i^T * k_j\n",
    "        # You may use \"edges.dst[] for i, edges.src[] for j\"\n",
    "        qikj = (edges.dst['Q'] * edges.src['K']).sum(dim=2).unsqueeze(2) # all dot products q_i^T * k_j, size=(E,K,1), edges.src/dst[].size=(E,K,d')\n",
    "        #qikj = ### YOUR CODE HERE, size=(E,K,1), , edges.src/dst[].size=(E,K,d')\n",
    "        expij = torch.exp( qikj / torch.sqrt(torch.tensor(self.head_hidden_dim)) ) # exp_ij = exp( clamp(q_i^T * k_j / sqrt(d')) ), size=(E,K,1)\n",
    "        vj = edges.src['V'] # size=(E,K,d')\n",
    "        return {'expij' : expij, 'vj' : vj}\n",
    "\n",
    "    # Step 2 of message-passing with DGL:\n",
    "    #   Reduce function collects all messages={hj, eij} sent to node dst/i with Step 1\n",
    "    #                   and sum/mean over the graph neigbors j in Ni\n",
    "    def reduce_func(self, nodes):\n",
    "        expij = nodes.mailbox['expij'] # size=(N,|Nj|,K,1), |Nj|=num_neighbors\n",
    "        vj = nodes.mailbox['vj'] # size=(N,|Nj|,K,d')\n",
    "        # Compute h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij'\n",
    "        #numerator = ### YOUR CODE HERE, sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        #denominator = ### YOUR CODE HERE, sum_j' exp_ij', size=(N,K,1)\n",
    "        numerator = torch.sum( expij * vj, dim=1 ) # sum_j exp_ij . v_j, size=(N,K,d')\n",
    "        denominator = torch.sum( expij, dim=1 ) # sum_j' exp_ij', size=(N,K,1)\n",
    "        h = numerator / denominator # h_i = sum_j score_ij . v_j , where score_ij = exp_ij / sum_j' exp_ij', size=(N,K,d')\n",
    "        return {'h' : h}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        Q = self.WQ(h) # size=(N, d)\n",
    "                       # computational trick to compute quickly K linear transformations h_k.WQ of size=(N, d')\n",
    "                       # first compute linear transformation h.WQ of size=(N, d)\n",
    "                       # then reshape h.WQ of size=(N, K, d'=d/K)\n",
    "        K = self.WK(h) # size=(N, d)\n",
    "        V = self.WV(h) # size=(N, d)\n",
    "        g.ndata['Q'] = Q.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['K'] = K.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.ndata['V'] = V.view(-1, self.num_heads, self.head_hidden_dim) # size=(N, K, d'=d/K)\n",
    "        g.update_all(self.message_func, self.reduce_func) # compute with DGL the graph MHA\n",
    "        gMHA = g.ndata['h'] # size=(N, K, d'=d/K)\n",
    "        return gMHA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lWOIc6W_8JY4"
   },
   "outputs": [],
   "source": [
    "# class GraphTransformer layer\n",
    "class GraphTransformer_layer(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim # hidden_dim = d\n",
    "        self.num_heads = num_heads # number of heads = K\n",
    "        self.dropout_mha = nn.Dropout(dropout) # dropout value\n",
    "        self.dropout_mlp = nn.Dropout(dropout) # dropout value\n",
    "        self.gMHA = graph_MHA_layer(hidden_dim, hidden_dim//num_heads, num_heads) # graph MHA layer\n",
    "        self.WO = nn.Linear(hidden_dim, hidden_dim) # LL\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim) # layer normalization\n",
    "        self.layer_norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.linear1 = nn.Linear(hidden_dim, hidden_dim) # LL1 for MLP\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim) # LL2 for MLP\n",
    "\n",
    "    def forward(self, g, h):\n",
    "\n",
    "        # Self-attention layer\n",
    "        h_rc = h # size=(N,d), V=num_nodes, for residual connection\n",
    "        h = self.layer_norm1(h) # layer normalization, size=(N, d)\n",
    "        h_MHA = self.gMHA(g, h) # MHA, size=(N, K, d'=d/K)\n",
    "        h_MHA = h_MHA.view(-1, self.hidden_dim) # size=(N, d)\n",
    "        h_MHA = self.dropout_mha(h_MHA) # dropout, size=(N, d)\n",
    "        h_MHA = self.WO(h_MHA) # LL, size=(N, d)\n",
    "        h = h_rc + h_MHA # residual connection, size=(N, d)\n",
    "\n",
    "        # Fully-connected layer\n",
    "        h_rc = h # for residual connection, size=(N, d)\n",
    "        h = self.layer_norm2(h) # layer normalization, size=(N, d)\n",
    "        h_MLP = self.linear1(h) # LL, size=(H, d)\n",
    "        h_MLP = torch.relu(h_MLP) # size=(N, d)\n",
    "        h_MLP = self.dropout_mlp(h_MLP) # dropout, size=(N, d)\n",
    "        h_MLP = self.linear2(h_MLP) # LL, size=(N, d)\n",
    "        h = h_rc + h_MLP # residual connection, size=(N, d)\n",
    "\n",
    "        return h\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "o_Gaf_Vi8MQt"
   },
   "outputs": [],
   "source": [
    "# class Graph Transformer network\n",
    "class GraphTransformer_net(nn.Module):\n",
    "\n",
    "    def __init__(self, net_parameters):\n",
    "        super(GraphTransformer_net, self).__init__()\n",
    "        input_dim = net_parameters['input_dim']\n",
    "        pos_enc_dim = net_parameters['pos_enc_dim']\n",
    "        hidden_dim = net_parameters['hidden_dim']\n",
    "        num_heads = net_parameters['num_heads']\n",
    "        L = net_parameters['L']\n",
    "        self.embedding_h = nn.Embedding(num_atom_type, hidden_dim)\n",
    "        self.embedding_pe = nn.Linear(pos_enc_dim, hidden_dim)\n",
    "        self.embedding_e = nn.Linear(1, hidden_dim)\n",
    "        self.GraphTransformer_layers = nn.ModuleList([ GraphTransformer_layer(hidden_dim, num_heads) for _ in range(L) ])\n",
    "        self.ln_h_final = nn.LayerNorm(hidden_dim)\n",
    "        self.linear_h_final = nn.Linear(hidden_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, g, h, pe):\n",
    "\n",
    "        # input node embedding = node in-degree feature\n",
    "        h = self.embedding_h(h) # in-degree feature, size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # graph convnet layers\n",
    "        for GT_layer in self.GraphTransformer_layers:\n",
    "            h = GT_layer(g,h) # size=(num_nodes, hidden_dim)\n",
    "\n",
    "        # MLP classifier\n",
    "        g.ndata['h'] = h\n",
    "        mol_token = dgl.mean_nodes(g,'h') # DGL mean function over the neighbors, size=(num_graphs, hidden_dim)\n",
    "        y = self.ln_h_final(mol_token)\n",
    "        y = self.linear_h_final(y) # size=(num_graphs, 1)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1730637547939,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "rPtk6EjKxXci",
    "outputId": "8db1c7ef-bbc3-4f67-df10-3ce74c4fac93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 400641 (0.40 million)\n",
      "torch.Size([10, 1])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "_ = display_num_param(net)\n",
    "\n",
    "batch_size = 10\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "batch_graphs, batch_labels = list(train_loader)[0]\n",
    "batch_x = batch_graphs.ndata['feat']\n",
    "batch_pe = batch_graphs.ndata['pos_enc']\n",
    "batch_labels = batch_labels\n",
    "batch_scores = net(batch_graphs, batch_x, batch_pe)\n",
    "print(batch_scores.size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jt8BzHJxXci"
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 168398,
     "status": "ok",
     "timestamp": 1730637716334,
     "user": {
      "displayName": "Guoji Fu",
      "userId": "16398754709610840055"
     },
     "user_tz": -480
    },
    "id": "xAdlIhxXxXci",
    "outputId": "ec39dc6a-0677-4b9e-93a0-d79426d21d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 400641 (0.40 million)\n",
      "Epoch 0, time 7.6868, train_loss: 1.4041, test_loss: 1.2948\n",
      "Epoch 1, time 15.2500, train_loss: 1.3050, test_loss: 1.2774\n",
      "Epoch 2, time 22.8270, train_loss: 1.2324, test_loss: 1.2039\n",
      "Epoch 3, time 30.6732, train_loss: 1.1488, test_loss: 1.1705\n",
      "Epoch 4, time 38.5291, train_loss: 1.1123, test_loss: 1.1563\n",
      "Epoch 5, time 46.1919, train_loss: 1.0814, test_loss: 1.1007\n",
      "Epoch 6, time 53.7318, train_loss: 1.0603, test_loss: 1.0743\n",
      "Epoch 7, time 61.4788, train_loss: 1.0400, test_loss: 1.0721\n",
      "Epoch 8, time 69.0981, train_loss: 1.0570, test_loss: 1.1523\n",
      "Epoch 9, time 76.5901, train_loss: 1.0419, test_loss: 1.0554\n",
      "Epoch 10, time 84.1103, train_loss: 1.0124, test_loss: 1.0849\n",
      "Epoch 11, time 91.5739, train_loss: 1.0132, test_loss: 0.9977\n",
      "Epoch 12, time 98.9597, train_loss: 0.9741, test_loss: 1.0093\n",
      "Epoch 13, time 106.3863, train_loss: 0.9913, test_loss: 1.0030\n",
      "Epoch 14, time 113.8747, train_loss: 0.9744, test_loss: 1.1644\n",
      "Epoch 15, time 121.2653, train_loss: 1.0575, test_loss: 1.0019\n",
      "Epoch 16, time 128.6523, train_loss: 0.9728, test_loss: 0.9790\n",
      "Epoch 17, time 136.0948, train_loss: 0.9512, test_loss: 0.9825\n",
      "Epoch 18, time 143.5380, train_loss: 0.9304, test_loss: 0.9469\n",
      "Epoch 19, time 150.9439, train_loss: 0.9321, test_loss: 0.9471\n",
      "Epoch 20, time 158.3909, train_loss: 0.9343, test_loss: 0.9756\n",
      "Epoch 21, time 165.8418, train_loss: 0.9291, test_loss: 0.9387\n",
      "Epoch 22, time 173.1638, train_loss: 0.9134, test_loss: 0.9616\n",
      "Epoch 23, time 180.6596, train_loss: 0.9078, test_loss: 0.9287\n",
      "Epoch 24, time 188.0168, train_loss: 0.9122, test_loss: 0.9182\n",
      "Epoch 25, time 195.4396, train_loss: 0.8877, test_loss: 0.9196\n",
      "Epoch 26, time 202.8513, train_loss: 0.8901, test_loss: 0.9824\n",
      "Epoch 27, time 210.3628, train_loss: 0.9047, test_loss: 0.9681\n",
      "Epoch 28, time 217.8377, train_loss: 0.9035, test_loss: 0.9742\n",
      "Epoch 29, time 225.2622, train_loss: 0.8739, test_loss: 0.9532\n",
      "Epoch 30, time 232.7117, train_loss: 0.8994, test_loss: 1.0487\n",
      "Epoch 31, time 240.1471, train_loss: 0.8976, test_loss: 0.9228\n",
      "Epoch 32, time 247.6348, train_loss: 0.8856, test_loss: 0.9947\n",
      "Epoch 33, time 255.0737, train_loss: 0.8769, test_loss: 0.9239\n",
      "Epoch 34, time 262.5508, train_loss: 0.8691, test_loss: 0.8998\n",
      "Epoch 35, time 270.0758, train_loss: 0.8587, test_loss: 0.9080\n",
      "Epoch 36, time 277.5212, train_loss: 0.8527, test_loss: 0.9239\n",
      "Epoch 37, time 284.9238, train_loss: 0.8354, test_loss: 0.9346\n",
      "Epoch 38, time 292.3516, train_loss: 0.8455, test_loss: 0.9425\n",
      "Epoch 39, time 299.8227, train_loss: 0.8573, test_loss: 0.9481\n",
      "Epoch 40, time 307.3307, train_loss: 0.8386, test_loss: 0.9335\n",
      "Epoch 41, time 314.7703, train_loss: 0.8418, test_loss: 0.9022\n",
      "Epoch 42, time 322.2841, train_loss: 0.8048, test_loss: 0.9231\n",
      "Epoch 43, time 329.7000, train_loss: 0.8033, test_loss: 0.9017\n",
      "Epoch 44, time 337.1324, train_loss: 0.8035, test_loss: 0.9609\n",
      "Epoch 45, time 344.6031, train_loss: 0.8174, test_loss: 0.9054\n",
      "Epoch 46, time 352.0858, train_loss: 0.8089, test_loss: 0.9219\n",
      "Epoch 47, time 359.5371, train_loss: 0.7889, test_loss: 0.9121\n",
      "Epoch 48, time 366.9699, train_loss: 0.7963, test_loss: 0.9358\n",
      "Epoch 49, time 374.4557, train_loss: 0.7841, test_loss: 0.9112\n",
      "Epoch 50, time 381.8268, train_loss: 0.8119, test_loss: 0.9300\n",
      "Epoch 51, time 389.3046, train_loss: 0.7877, test_loss: 0.9148\n",
      "Epoch 52, time 396.8504, train_loss: 0.7694, test_loss: 0.9216\n",
      "Epoch 53, time 404.2654, train_loss: 0.7646, test_loss: 0.9072\n",
      "Epoch 54, time 411.7554, train_loss: 0.7633, test_loss: 0.9107\n",
      "Epoch 55, time 419.2691, train_loss: 0.7680, test_loss: 0.9294\n",
      "Epoch 56, time 426.7419, train_loss: 0.7609, test_loss: 0.9201\n",
      "Epoch 57, time 434.2316, train_loss: 0.7265, test_loss: 0.9211\n",
      "Epoch 58, time 441.8298, train_loss: 0.7463, test_loss: 0.9269\n",
      "Epoch 59, time 449.3337, train_loss: 0.7382, test_loss: 0.9242\n",
      "Epoch 60, time 456.8877, train_loss: 0.7425, test_loss: 0.9056\n",
      "Epoch 61, time 465.0855, train_loss: 0.7467, test_loss: 0.9539\n",
      "Epoch 62, time 473.6694, train_loss: 0.7440, test_loss: 0.9677\n",
      "Epoch 63, time 482.4093, train_loss: 0.7260, test_loss: 0.8949\n",
      "Epoch 64, time 490.3084, train_loss: 0.7211, test_loss: 0.9113\n",
      "Epoch 65, time 497.5511, train_loss: 0.7382, test_loss: 0.9328\n",
      "Epoch 66, time 503.4456, train_loss: 0.7328, test_loss: 0.9019\n",
      "Epoch 67, time 509.5734, train_loss: 0.7000, test_loss: 0.9317\n",
      "Epoch 68, time 515.8582, train_loss: 0.7010, test_loss: 0.9151\n",
      "Epoch 69, time 522.8033, train_loss: 0.7006, test_loss: 0.9385\n",
      "Epoch 70, time 529.6716, train_loss: 0.6931, test_loss: 0.9186\n",
      "Epoch 71, time 536.1415, train_loss: 0.7068, test_loss: 0.9272\n",
      "Epoch 72, time 543.3746, train_loss: 0.6794, test_loss: 0.9360\n",
      "Epoch 73, time 549.9978, train_loss: 0.6822, test_loss: 0.9263\n",
      "Epoch 74, time 557.4060, train_loss: 0.7000, test_loss: 0.9355\n",
      "Epoch 75, time 563.9556, train_loss: 0.6783, test_loss: 0.9518\n",
      "Epoch 76, time 572.0455, train_loss: 0.6804, test_loss: 0.9450\n",
      "Epoch 77, time 578.6062, train_loss: 0.6695, test_loss: 0.9442\n",
      "Epoch 78, time 585.4387, train_loss: 0.6443, test_loss: 0.9423\n",
      "Epoch 79, time 591.2760, train_loss: 0.6516, test_loss: 0.9271\n",
      "Epoch 80, time 597.7890, train_loss: 0.6680, test_loss: 0.9383\n",
      "Epoch 81, time 604.3428, train_loss: 0.6713, test_loss: 0.9534\n",
      "Epoch 82, time 612.1246, train_loss: 0.6528, test_loss: 0.9528\n",
      "Epoch 83, time 618.7843, train_loss: 0.6402, test_loss: 0.9388\n",
      "Epoch 84, time 625.3814, train_loss: 0.6484, test_loss: 0.9492\n",
      "Epoch 85, time 632.9699, train_loss: 0.6444, test_loss: 0.9286\n",
      "Epoch 86, time 640.3880, train_loss: 0.6375, test_loss: 0.9737\n",
      "Epoch 87, time 647.7178, train_loss: 0.6375, test_loss: 0.9383\n",
      "Epoch 88, time 655.1589, train_loss: 0.6117, test_loss: 0.9603\n",
      "Epoch 89, time 662.6193, train_loss: 0.6224, test_loss: 0.9628\n",
      "Epoch 90, time 670.0236, train_loss: 0.6100, test_loss: 0.9830\n",
      "Epoch 91, time 677.4205, train_loss: 0.6191, test_loss: 0.9625\n",
      "Epoch 92, time 684.8705, train_loss: 0.6142, test_loss: 0.9628\n",
      "Epoch 93, time 692.3243, train_loss: 0.6074, test_loss: 0.9382\n",
      "Epoch 94, time 699.7543, train_loss: 0.5970, test_loss: 0.9707\n",
      "Epoch 95, time 707.2216, train_loss: 0.6034, test_loss: 0.9561\n",
      "Epoch 96, time 714.6151, train_loss: 0.5844, test_loss: 0.9849\n",
      "Epoch 97, time 722.0377, train_loss: 0.6089, test_loss: 0.9617\n",
      "Epoch 98, time 729.3484, train_loss: 0.5946, test_loss: 0.9988\n",
      "Epoch 99, time 736.7730, train_loss: 0.5845, test_loss: 1.0031\n"
     ]
    }
   ],
   "source": [
    "def run_one_epoch(net, data_loader, train=True, loss_fc=None, optimizer=None):\n",
    "    if train:\n",
    "        net.train() # during training\n",
    "    else:\n",
    "        net.eval()  # during inference/test\n",
    "    epoch_loss = 0\n",
    "    nb_data = 0\n",
    "    for iter, (batch_graphs, batch_labels) in enumerate(data_loader):\n",
    "        batch_x = batch_graphs.ndata['feat']\n",
    "        bs2 = batch_labels.size(0)\n",
    "        batch_pe = batch_graphs.ndata['pos_enc']\n",
    "        batch_pe = batch_pe * ( 2 * torch.randint(low=0, high=2, size=(1,pos_enc_dim)).float() - 1.0 ) # randomly flip sign of eigenvectors\n",
    "        batch_labels = batch_labels\n",
    "        batch_scores = net.forward(batch_graphs, batch_x, batch_pe)\n",
    "        lossMAE = loss_fc(batch_scores, batch_labels)\n",
    "        if train: # during training, run backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            lossMAE.backward()\n",
    "            optimizer.step()\n",
    "        epoch_loss += bs2 * lossMAE.detach().item()\n",
    "        nb_data += bs2\n",
    "    epoch_loss /= nb_data\n",
    "    return epoch_loss, optimizer\n",
    "\n",
    "\n",
    "# dataset loaders\n",
    "batch_size = 50\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, drop_last=False, collate_fn=collate)\n",
    "\n",
    "# Instantiate one network\n",
    "net_parameters = {}\n",
    "net_parameters['input_dim'] = 1\n",
    "net_parameters['pos_enc_dim'] = pos_enc_dim\n",
    "net_parameters['hidden_dim'] = 128\n",
    "net_parameters['num_heads'] = 8\n",
    "net_parameters['L'] = 4\n",
    "del net\n",
    "net = GraphTransformer_net(net_parameters)\n",
    "def display_num_param(net):\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += param.numel()\n",
    "    print('Number of parameters: {} ({:.2f} million)'.format(nb_param, nb_param/1e6))\n",
    "    return nb_param/1e6\n",
    "_ = display_num_param(net)\n",
    "\n",
    "# loss, optimizer\n",
    "lossMAE = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0003)\n",
    "# optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "# training loop\n",
    "start = time.time()\n",
    "for epoch in range(100):\n",
    "    epoch_train_loss, optimizer = run_one_epoch(net, train_loader, True, lossMAE, optimizer)\n",
    "    with torch.no_grad():\n",
    "        epoch_test_loss = run_one_epoch(net, test_loader, False, lossMAE)[0]\n",
    "        # epoch_val_loss = run_one_epoch(net, val_loader, False, lossMAE)[0]\n",
    "    print('Epoch {}, time {:.4f}, train_loss: {:.4f}, test_loss: {:.4f}'.format(epoch, time.time()-start, epoch_train_loss, epoch_test_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n8uBSgqTxXcj"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWLI02VPxXcj"
   },
   "source": [
    "## Compare results\n",
    "\n",
    "| GNN    | train MAE | test MAE |\n",
    "| -------- | ------- | ------- |\n",
    "| GT w/ edge features (bond type)    |  0.4598|  0.6995    |\n",
    "| GT without edge features (only atom type)    | 0.5845    | 1.0031     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d26mlnkvxXcj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j73ydTFQxXcj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "gnn_course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
